{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nano-DyMEAN\n",
    "\n",
    "## Description\n",
    "Personal modifications of the dyMEAN framework tailored for Nanobody analysis.\n",
    "We start with DyMEAN paper implimentation, and modify that scripts to avoid the use of lighchain explicly.\n",
    "the input for dyMEAN is just the json files(train.json, valid.json and test.json) and all_structure folders which has all pdb structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########### Import your packages below ##########\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import datetime\n",
    "import json\n",
    "import argparse\n",
    "from random import random\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from math import cos, pi, log, exp, nan\n",
    "from copy import copy, deepcopy\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter_mean, scatter_std, scatter_softmax, scatter_sum\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "from Bio.PDB import PDBParser, PDBIO\n",
    "from Bio.PDB.Structure import Structure as BStructure\n",
    "from Bio.PDB.Model import Model as BModel\n",
    "from Bio.PDB.Chain import Chain as BChain\n",
    "from Bio.PDB.Residue import Residue as BResidue\n",
    "from Bio.PDB.Atom import Atom as BAtom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEED = 12\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "config = Config(\n",
    "    backbone_only=False,\n",
    "    batch_size=1,\n",
    "    bind_dist_cutoff=6.6,\n",
    "    cdr=['H3'],\n",
    "    embed_dim=64,\n",
    "    final_lr=0.0001,\n",
    "    fix_channel_weights=False,\n",
    "    gpus=[0],\n",
    "    # gpus=[0, 1],\n",
    "    grad_clip=1.0,\n",
    "    hidden_size=128,\n",
    "    iter_round=3,\n",
    "    k_neighbors=9,\n",
    "    local_rank=-1,\n",
    "    lr=0.001,\n",
    "    max_epoch=200,\n",
    "    model_type='dyMEAN',\n",
    "    n_layers=3,\n",
    "    no_memory=False,\n",
    "    no_pred_edge_dist=False,\n",
    "    num_workers=4,\n",
    "    paratope='H3',\n",
    "    patience=1000,\n",
    "    save_dir='./all_data/RAbD/models_single_cdr_design',\n",
    "    save_topk=10,\n",
    "    seq_warmup=0,\n",
    "    shuffle=True,\n",
    "    struct_only=False,\n",
    "    train_set='./all_data/RAbD/train.json',\n",
    "    valid_set='./all_data/RAbD/valid.json',\n",
    "    warmup=0\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVELS = ['TRACE', 'DEBUG', 'INFO', 'WARN', 'ERROR']\n",
    "LEVELS_MAP = None\n",
    "\n",
    "\n",
    "def init_map():\n",
    "    global LEVELS_MAP, LEVELS\n",
    "    LEVELS_MAP = {}\n",
    "    for idx, level in enumerate(LEVELS):\n",
    "        LEVELS_MAP[level] = idx\n",
    "\n",
    "\n",
    "def get_prio(level):\n",
    "    global LEVELS_MAP\n",
    "    if LEVELS_MAP is None:\n",
    "        init_map()\n",
    "    return LEVELS_MAP[level.upper()]\n",
    "\n",
    "\n",
    "def print_log(s, level='INFO', end='\\n', no_prefix=False):\n",
    "    pth_prio = get_prio(os.getenv('LOG', 'INFO'))\n",
    "    prio = get_prio(level)\n",
    "    if prio >= pth_prio:\n",
    "        if not no_prefix:\n",
    "            now = datetime.datetime.now()\n",
    "            prefix = now.strftime(\"%Y-%m-%d %H:%M:%S\") + f'::{level.upper()}::'\n",
    "            print(prefix, end='')\n",
    "        print(s, end=end)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "setup_seed(SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "Four parts:\n",
    "1. basic variables\n",
    "2. benchmark definitions and configs for data processing\n",
    "3. definitions for antibody numbering system\n",
    "4. optional dependencies for pipelines\n",
    "'''\n",
    "\n",
    "# 1. basic variables\n",
    "PROJ_DIR = './'\n",
    "RENUMBER = os.path.join(PROJ_DIR, 'utils', 'renumber.py')\n",
    "# FoldX\n",
    "FOLDX_BIN = './foldx5/foldx_20231231'\n",
    "# DockQ \n",
    "# IMPORTANT: change it to your path to DockQ project)\n",
    "DOCKQ_DIR = './DockQ'\n",
    "# cache directory\n",
    "CACHE_DIR = os.path.join(PROJ_DIR, '__cache__')\n",
    "if not os.path.exists(CACHE_DIR):\n",
    "    os.makedirs(CACHE_DIR)\n",
    "\n",
    "\n",
    "# 2. configs related to data process\n",
    "AG_TYPES = ['protein', 'peptide']\n",
    "RAbD_PDB = ['1a14', '1a2y', '1fe8', '1ic7', '1iqd', '1n8z', '1ncb', '1osp', '1uj3', '1w72', '2adf', '2b2x', '2cmr', '2dd8', '2ghw', '2vxt', '2xqy', '2xwt', '2ypv', '3bn9', '3cx5', '3ffd', '3h3b', '3hi6', '3k2u', '3l95', '3mxw', '3nid', '3o2d', '3rkd', '3s35', '3uzq', '3w9e', '4cmh', '4dtg', '4dvr', '4etq', '4ffv', '4fqj', '4g6j', '4g6m', '4h8w', '4ki5', '4lvn', '4ot1', '4qci', '4xnq', '4ydk', '5b8c', '5bv7', '5d93', '5d96', '5en2', '5f9o', '5ggs', '5hi4', '5j13', '5l6y', '5mes', '5nuz']\n",
    "IGFOLD_TEST_PDB = ['7rdm', '6xsw', '7e9b', '7cj2', '7o33', '7ora', '7mzh', '7mzj', '7ahu', '7s13', '7mzk', '7e72', '7n3c', '7n3f', '7rdl', '7mzg', '7n4j', '7r9d', '7e3o', '7rah', '7or9', '7oo2', '6xsn', '7arn', '7n3e', '7o30', '7kf0', '7lfa', '7nx3', '7keo', '7mzf', '7o31', '7e5o', '7daa', '7s11', '7aj6', '7m2i', '7kf1', '7jkm', '7n4i', '6xm2', '7doh', '7o2z', '7kba', '7s3m', '7mzm', '7rks', '7n3h', '7lyw', '7rco', '7bg1', '7coe', '7n3g', '7kkz', '7kyo', '7s4s', '7rnj', '7bbg', '7l7e', '7n0u', '6xlz', '7mf7', '6xp6', '7lfb', '7kn3', '7rdk', '7s0b', '7kez', '7n3d', '7o4y']\n",
    "SKEMPI_PDB = ['1ahw', '1dvf', '1vfb', '2vis', '2vir', '1kiq', '1kip', '1kir', '2jel', '1nca', '1dqj', '1jrh', '1nmb', '3hfm', '1yy9', '4gxu', '3lzf', '1n8z', '3g6d', '1xgu', '1xgp', '1xgq', '1xgr', '1xgt', '3n85', '4i77', '3l5x', '4jpk', '1bj1', '1cz8', '1mhp', '2b2x', '1mlc', '3bdy', '3be1', '2ny7', '3idx', '2nyy', '2nz9', '3ngb', '2bdn', '3w2d', '4krl', '4kro', '4krp', '4nm8', '4u6h', '4zs6', '5c6t', '5dwu', '3se8', '3se9', '1yqv']\n",
    "CONTACT_DIST = 6.6  # 6.6 A between one pair of atoms means the two residues are interacting\n",
    "\n",
    "\n",
    "# 3. antibody numbering, [start, end] of residue id, both start & end are included\n",
    "# 3.1 IMGT numbering definition\n",
    "class IMGT:\n",
    "    # heavy chain\n",
    "    HFR1 = (1, 26)\n",
    "    HFR2 = (39, 55)\n",
    "    HFR3 = (66, 104)\n",
    "    HFR4 = (118, 129)\n",
    "\n",
    "    H1 = (27, 38)\n",
    "    H2 = (56, 65)\n",
    "    H3 = (105, 117)\n",
    "\n",
    "    # light chain\n",
    "    LFR1 = (1, 26)\n",
    "    LFR2 = (39, 55)\n",
    "    LFR3 = (66, 104)\n",
    "    LFR4 = (118, 129)\n",
    "\n",
    "    L1 = (27, 38)\n",
    "    L2 = (56, 65)\n",
    "    L3 = (105, 117)\n",
    "\n",
    "    Hconserve = {\n",
    "        23: ['CYS'],\n",
    "        41: ['TRP'],\n",
    "        104: ['CYS']\n",
    "    }\n",
    "\n",
    "    Lconserve = {\n",
    "        23: ['CYS'],\n",
    "        41: ['TRP'],\n",
    "        104: ['CYS']\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def renumber(cls, pdb, out_pdb):\n",
    "        code = os.system(f'python {RENUMBER} {pdb} {out_pdb} imgt 0')\n",
    "        return code\n",
    "\n",
    "# 3.2 Chothia numbering definition\n",
    "class Chothia:\n",
    "    # heavy chain\n",
    "    HFR1 = (1, 25)\n",
    "    HFR2 = (33, 51)\n",
    "    HFR3 = (57, 94)\n",
    "    HFR4 = (103, 113)\n",
    "\n",
    "    H1 = (26, 32)\n",
    "    H2 = (52, 56)\n",
    "    H3 = (95, 102)\n",
    "\n",
    "    # light chain\n",
    "    LFR1 = (1, 23)\n",
    "    LFR2 = (35, 49)\n",
    "    LFR3 = (57, 88)\n",
    "    LFR4 = (98, 107)\n",
    "\n",
    "    L1 = (24, 34)\n",
    "    L2 = (50, 56)\n",
    "    L3 = (89, 97)\n",
    "\n",
    "    Hconserve = {\n",
    "        92: ['CYS']\n",
    "    }\n",
    "\n",
    "    Lconserve = {\n",
    "        88: ['CYS']\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def renumber(cls, pdb, out_pdb):\n",
    "        code = os.system(f'python {RENUMBER} {pdb} {out_pdb} chothia 0')\n",
    "        return code\n",
    "\n",
    "\n",
    "# (Optional) 4. dependencies for pipelines\n",
    "# 4.1 structure prediction\n",
    "IGFOLD_DIR = './IgFold'\n",
    "IGFOLD_CKPTS = None  # 'None' means using the default checkpoints\n",
    "# 4.2 docking\n",
    "HDOCK_DIR = './HDOCKlite-v1.1'\n",
    "# 4.3 CDR generation models\n",
    "MEAN_DIR = './MEAN'\n",
    "Rosetta_DIR = './rosetta/rosetta.binary.linux.release-315/main/source/bin'\n",
    "DiffAb_DIR = './diffab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class AminoAcid:\n",
    "    def __init__(self, symbol: str, abrv: str, sidechain: List[str], idx=0):\n",
    "        self.symbol = symbol\n",
    "        self.abrv = abrv\n",
    "        self.idx = idx\n",
    "        self.sidechain = sidechain\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'{self.idx} {self.symbol} {self.abrv} {self.sidechain}'\n",
    "\n",
    "\n",
    "class AminoAcidVocab:\n",
    "\n",
    "    MAX_ATOM_NUMBER = 14   # 4 backbone atoms + up to 10 sidechain atoms\n",
    "\n",
    "    def __init__(self):\n",
    "        self.backbone_atoms = ['N', 'CA', 'C', 'O']\n",
    "        self.PAD, self.MASK = '#', '*'\n",
    "        self.BOA, self.BOH, self.BOL = '&', '+', '-' # begin of antigen, heavy chain, light chain\n",
    "        specials = [# special added\n",
    "                (self.PAD, 'PAD'), (self.MASK, 'MASK'), # mask for masked / unknown residue\n",
    "                (self.BOA, '<X>'), (self.BOH, '<H>'), (self.BOL, '<L>')\n",
    "            ]\n",
    "        aas = [\n",
    "                ('G', 'GLY'), ('A', 'ALA'), ('V', 'VAL'), ('L', 'LEU'),\n",
    "                ('I', 'ILE'), ('F', 'PHE'), ('W', 'TRP'), ('Y', 'TYR'),\n",
    "                ('D', 'ASP'), ('H', 'HIS'), ('N', 'ASN'), ('E', 'GLU'),\n",
    "                ('K', 'LYS'), ('Q', 'GLN'), ('M', 'MET'), ('R', 'ARG'),\n",
    "                ('S', 'SER'), ('T', 'THR'), ('C', 'CYS'), ('P', 'PRO') # 20 aa\n",
    "                # ('U', 'SEC') # 21 aa for eukaryote\n",
    "            ]\n",
    "\n",
    "        # max number of sidechain atoms: 10        \n",
    "        self.atom_pad, self.atom_mask = 'p', 'm'\n",
    "        self.atom_pos_mask, self.atom_pos_bb, self.atom_pos_pad = 'm', 'b', 'p'\n",
    "        sidechain_map = {\n",
    "            'G': [],   # -H\n",
    "            'A': ['CB'],  # -CH3\n",
    "            'V': ['CB', 'CG1', 'CG2'],  # -CH-(CH3)2\n",
    "            'L': ['CB', 'CG', 'CD1', 'CD2'],  # -CH2-CH(CH3)2\n",
    "            'I': ['CB', 'CG1', 'CG2', 'CD1'], # -CH(CH3)-CH2-CH3\n",
    "            'F': ['CB', 'CG', 'CD1', 'CD2', 'CE1', 'CE2', 'CZ'],  # -CH2-C6H5\n",
    "            'W': ['CB', 'CG', 'CD1', 'CD2', 'NE1', 'CE2', 'CE3', 'CZ2', 'CZ3', 'CH2'],  # -CH2-C8NH6\n",
    "            'Y': ['CB', 'CG', 'CD1', 'CD2', 'CE1', 'CE2', 'CZ', 'OH'],  # -CH2-C6H4-OH\n",
    "            'D': ['CB', 'CG', 'OD1', 'OD2'],  # -CH2-COOH\n",
    "            'H': ['CB', 'CG', 'ND1', 'CD2', 'CE1', 'NE2'],  # -CH2-C3H3N2\n",
    "            'N': ['CB', 'CG', 'OD1', 'ND2'],  # -CH2-CONH2\n",
    "            'E': ['CB', 'CG', 'CD', 'OE1', 'OE2'],  # -(CH2)2-COOH\n",
    "            'K': ['CB', 'CG', 'CD', 'CE', 'NZ'],  # -(CH2)4-NH2\n",
    "            'Q': ['CB', 'CG', 'CD', 'OE1', 'NE2'],  # -(CH2)-CONH2\n",
    "            'M': ['CB', 'CG', 'SD', 'CE'],  # -(CH2)2-S-CH3\n",
    "            'R': ['CB', 'CG', 'CD', 'NE', 'CZ', 'NH1', 'NH2'],  # -(CH2)3-NHC(NH)NH2\n",
    "            'S': ['CB', 'OG'],  # -CH2-OH\n",
    "            'T': ['CB', 'OG1', 'CG2'],  # -CH(CH3)-OH\n",
    "            'C': ['CB', 'SG'],  # -CH2-SH\n",
    "            'P': ['CB', 'CG', 'CD'],  # -C3H6\n",
    "        }\n",
    "\n",
    "        self.chi_angles_atoms = {\n",
    "            \"ALA\": [],\n",
    "            # Chi5 in arginine is always 0 +- 5 degrees, so ignore it.\n",
    "            \"ARG\": [\n",
    "                [\"N\", \"CA\", \"CB\", \"CG\"],\n",
    "                [\"CA\", \"CB\", \"CG\", \"CD\"],\n",
    "                [\"CB\", \"CG\", \"CD\", \"NE\"],\n",
    "                [\"CG\", \"CD\", \"NE\", \"CZ\"],\n",
    "            ],\n",
    "            \"ASN\": [[\"N\", \"CA\", \"CB\", \"CG\"], [\"CA\", \"CB\", \"CG\", \"OD1\"]],\n",
    "            \"ASP\": [[\"N\", \"CA\", \"CB\", \"CG\"], [\"CA\", \"CB\", \"CG\", \"OD1\"]],\n",
    "            \"CYS\": [[\"N\", \"CA\", \"CB\", \"SG\"]],\n",
    "            \"GLN\": [\n",
    "                [\"N\", \"CA\", \"CB\", \"CG\"],\n",
    "                [\"CA\", \"CB\", \"CG\", \"CD\"],\n",
    "                [\"CB\", \"CG\", \"CD\", \"OE1\"],\n",
    "            ],\n",
    "            \"GLU\": [\n",
    "                [\"N\", \"CA\", \"CB\", \"CG\"],\n",
    "                [\"CA\", \"CB\", \"CG\", \"CD\"],\n",
    "                [\"CB\", \"CG\", \"CD\", \"OE1\"],\n",
    "            ],\n",
    "            \"GLY\": [],\n",
    "            \"HIS\": [[\"N\", \"CA\", \"CB\", \"CG\"], [\"CA\", \"CB\", \"CG\", \"ND1\"]],\n",
    "            \"ILE\": [[\"N\", \"CA\", \"CB\", \"CG1\"], [\"CA\", \"CB\", \"CG1\", \"CD1\"]],\n",
    "            \"LEU\": [[\"N\", \"CA\", \"CB\", \"CG\"], [\"CA\", \"CB\", \"CG\", \"CD1\"]],\n",
    "            \"LYS\": [\n",
    "                [\"N\", \"CA\", \"CB\", \"CG\"],\n",
    "                [\"CA\", \"CB\", \"CG\", \"CD\"],\n",
    "                [\"CB\", \"CG\", \"CD\", \"CE\"],\n",
    "                [\"CG\", \"CD\", \"CE\", \"NZ\"],\n",
    "            ],\n",
    "            \"MET\": [\n",
    "                [\"N\", \"CA\", \"CB\", \"CG\"],\n",
    "                [\"CA\", \"CB\", \"CG\", \"SD\"],\n",
    "                [\"CB\", \"CG\", \"SD\", \"CE\"],\n",
    "            ],\n",
    "            \"PHE\": [[\"N\", \"CA\", \"CB\", \"CG\"], [\"CA\", \"CB\", \"CG\", \"CD1\"]],\n",
    "            \"PRO\": [[\"N\", \"CA\", \"CB\", \"CG\"], [\"CA\", \"CB\", \"CG\", \"CD\"]],\n",
    "            \"SER\": [[\"N\", \"CA\", \"CB\", \"OG\"]],\n",
    "            \"THR\": [[\"N\", \"CA\", \"CB\", \"OG1\"]],\n",
    "            \"TRP\": [[\"N\", \"CA\", \"CB\", \"CG\"], [\"CA\", \"CB\", \"CG\", \"CD1\"]],\n",
    "            \"TYR\": [[\"N\", \"CA\", \"CB\", \"CG\"], [\"CA\", \"CB\", \"CG\", \"CD1\"]],\n",
    "            \"VAL\": [[\"N\", \"CA\", \"CB\", \"CG1\"]],\n",
    "        }\n",
    "\n",
    "        self.sidechain_bonds = {\n",
    "            \"ALA\": { \"CA\": [\"CB\"] },\n",
    "            \"GLY\": {},\n",
    "            \"VAL\": {\n",
    "                \"CA\": [\"CB\"],\n",
    "                \"CB\": [\"CG1\", \"CG2\"]\n",
    "            },\n",
    "            \"LEU\": {\n",
    "                \"CA\": [\"CB\"],\n",
    "                \"CB\": [\"CG\"],\n",
    "                \"CG\": [\"CD2\", \"CD1\"]\n",
    "            },\n",
    "            \"ILE\": {\n",
    "                \"CA\": [\"CB\"],\n",
    "                \"CB\": [\"CG1\", \"CG2\"],\n",
    "                \"CG1\": [\"CD1\"]\n",
    "            },\n",
    "            \"MET\": {\n",
    "                \"CA\": [\"CB\"],\n",
    "                \"CB\": [\"CG\"],\n",
    "                \"CG\": [\"SD\"],\n",
    "                \"SD\": [\"CE\"],\n",
    "            },\n",
    "            \"PHE\": {\n",
    "                \"CA\": [\"CB\"],\n",
    "                \"CB\": [\"CG\"],\n",
    "                \"CG\": [\"CD1\", \"CD2\"],\n",
    "                \"CD1\": [\"CE1\"],\n",
    "                \"CD2\": [\"CE2\"],\n",
    "                \"CE1\": [\"CZ\"]\n",
    "            },\n",
    "            \"TRP\": {\n",
    "                \"CA\": [\"CB\"],\n",
    "                \"CB\": [\"CG\"],\n",
    "                \"CG\": [\"CD1\", \"CD2\"],\n",
    "                \"CD1\": [\"NE1\"],\n",
    "                \"CD2\": [\"CE2\", \"CE3\"],\n",
    "                \"CE2\": [\"CZ2\"],\n",
    "                \"CZ2\": [\"CH2\"],\n",
    "                \"CE3\": [\"CZ3\"]\n",
    "            },\n",
    "            \"PRO\": {\n",
    "                \"CA\": [\"CB\"],\n",
    "                \"CB\": [\"CG\"],\n",
    "                \"CG\": [\"CD\"]\n",
    "            },\n",
    "            \"SER\": {\n",
    "                \"CA\": [\"CB\"],\n",
    "                \"CB\": [\"OG\"]\n",
    "            },\n",
    "            \"THR\": {\n",
    "                \"CA\": [\"CB\"],\n",
    "                \"CB\": [\"OG1\", \"CG2\"]\n",
    "            },\n",
    "            \"CYS\": {\n",
    "                \"CA\": [\"CB\"],\n",
    "                \"CB\": [\"SG\"]\n",
    "            },\n",
    "            \"TYR\": {\n",
    "                \"CA\": [\"CB\"],\n",
    "                \"CB\": [\"CG\"],\n",
    "                \"CG\": [\"CD1\", \"CD2\"],\n",
    "                \"CD1\": [\"CE1\"],\n",
    "                \"CD2\": [\"CE2\"],\n",
    "                \"CE1\": [\"CZ\"],\n",
    "                \"CZ\": [\"OH\"]\n",
    "            },\n",
    "            \"ASN\": {\n",
    "                \"CA\": [\"CB\"],\n",
    "                \"CB\": [\"CG\"],\n",
    "                \"CG\": [\"OD1\", \"ND2\"]\n",
    "            },\n",
    "            \"GLN\": {\n",
    "                \"CA\": [\"CB\"],\n",
    "                \"CB\": [\"CG\"],\n",
    "                \"CG\": [\"CD\"],\n",
    "                \"CD\": [\"OE1\", \"NE2\"]\n",
    "            },\n",
    "            \"ASP\": {\n",
    "                \"CA\": [\"CB\"],\n",
    "                \"CB\": [\"CG\"],\n",
    "                \"CG\": [\"OD1\", \"OD2\"]\n",
    "            },\n",
    "            \"GLU\": {\n",
    "                \"CA\": [\"CB\"],\n",
    "                \"CB\": [\"CG\"],\n",
    "                \"CG\": [\"CD\"],\n",
    "                \"CD\": [\"OE1\", \"OE2\"]\n",
    "            },\n",
    "            \"LYS\": {\n",
    "                \"CA\": [\"CB\"],\n",
    "                \"CB\": [\"CG\"],\n",
    "                \"CG\": [\"CD\"],\n",
    "                \"CD\": [\"CE\"],\n",
    "                \"CE\": [\"NZ\"]\n",
    "            },\n",
    "            \"ARG\": {\n",
    "                \"CA\": [\"CB\"],\n",
    "                \"CB\": [\"CG\"],\n",
    "                \"CG\": [\"CD\"],\n",
    "                \"CD\": [\"NE\"],\n",
    "                \"NE\": [\"CZ\"],\n",
    "                \"CZ\": [\"NH1\", \"NH2\"]\n",
    "            },\n",
    "            \"HIS\": {\n",
    "                \"CA\": [\"CB\"],\n",
    "                \"CB\": [\"CG\"],\n",
    "                \"CG\": [\"ND1\", \"CD2\"],\n",
    "                \"ND1\": [\"CE1\"],\n",
    "                \"CD2\": [\"NE2\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "\n",
    "        _all = aas + specials\n",
    "        self.amino_acids = [AminoAcid(symbol, abrv, sidechain_map.get(symbol, [])) for symbol, abrv in _all]\n",
    "        self.symbol2idx, self.abrv2idx = {}, {}\n",
    "        for i, aa in enumerate(self.amino_acids):\n",
    "            self.symbol2idx[aa.symbol] = i\n",
    "            self.abrv2idx[aa.abrv] = i\n",
    "            aa.idx = i\n",
    "        self.special_mask = [0 for _ in aas] + [1 for _ in specials]\n",
    "\n",
    "        # atom level vocab\n",
    "        self.idx2atom = [self.atom_pad, self.atom_mask, 'C', 'N', 'O', 'S']\n",
    "        self.idx2atom_pos = [self.atom_pos_pad, self.atom_pos_mask, self.atom_pos_bb, 'B', 'G', 'D', 'E', 'Z', 'H']\n",
    "        self.atom2idx, self.atom_pos2idx = {}, {}\n",
    "        for i, atom in enumerate(self.idx2atom):\n",
    "            self.atom2idx[atom] = i\n",
    "        for i, atom_pos in enumerate(self.idx2atom_pos):\n",
    "            self.atom_pos2idx[atom_pos] = i\n",
    "    \n",
    "    def abrv_to_symbol(self, abrv):\n",
    "        idx = self.abrv_to_idx(abrv)\n",
    "        return None if idx is None else self.amino_acids[idx].symbol\n",
    "\n",
    "    def symbol_to_abrv(self, symbol):\n",
    "        idx = self.symbol_to_idx(symbol)\n",
    "        return None if idx is None else self.amino_acids[idx].abrv\n",
    "\n",
    "    def abrv_to_idx(self, abrv):\n",
    "        abrv = abrv.upper()\n",
    "        return self.abrv2idx.get(abrv, None)\n",
    "\n",
    "    def symbol_to_idx(self, symbol):\n",
    "        symbol = symbol.upper()\n",
    "        return self.symbol2idx.get(symbol, None)\n",
    "    \n",
    "    def idx_to_symbol(self, idx):\n",
    "        return self.amino_acids[idx].symbol\n",
    "\n",
    "    def idx_to_abrv(self, idx):\n",
    "        return self.amino_acids[idx].abrv\n",
    "\n",
    "    def get_pad_idx(self):\n",
    "        return self.symbol_to_idx(self.PAD)\n",
    "\n",
    "    def get_mask_idx(self):\n",
    "        return self.symbol_to_idx(self.MASK)\n",
    "    \n",
    "    def get_special_mask(self):\n",
    "        return copy(self.special_mask)\n",
    "\n",
    "    def get_atom_type_mat(self):\n",
    "        atom_pad = self.get_atom_pad_idx()\n",
    "        mat = []\n",
    "        for i, aa in enumerate(self.amino_acids):\n",
    "            atoms = [atom_pad for _ in range(self.MAX_ATOM_NUMBER)]\n",
    "            if aa.symbol == self.PAD:\n",
    "                pass\n",
    "            elif self.special_mask[i] == 1:  # specials\n",
    "                atom_mask = self.get_atom_mask_idx()\n",
    "                atoms = [atom_mask for _ in range(self.MAX_ATOM_NUMBER)]\n",
    "            else:\n",
    "                for aidx, atom in enumerate(self.backbone_atoms + aa.sidechain):\n",
    "                    atoms[aidx] = self.atom_to_idx(atom[0])\n",
    "            mat.append(atoms)\n",
    "        return mat\n",
    "\n",
    "    def get_atom_pos_mat(self):\n",
    "        atom_pos_pad = self.get_atom_pos_pad_idx()\n",
    "        mat = []\n",
    "        for i, aa in enumerate(self.amino_acids):\n",
    "            aps = [atom_pos_pad for _ in range(self.MAX_ATOM_NUMBER)]\n",
    "            if aa.symbol == self.PAD:\n",
    "                pass\n",
    "            elif self.special_mask[i] == 1:\n",
    "                atom_pos_mask = self.get_atom_pos_mask_idx()\n",
    "                aps = [atom_pos_mask for _ in range(self.MAX_ATOM_NUMBER)]\n",
    "            else:\n",
    "                aidx = 0\n",
    "                for _ in self.backbone_atoms:\n",
    "                    aps[aidx] = self.atom_pos_to_idx(self.atom_pos_bb)\n",
    "                    aidx += 1\n",
    "                for atom in aa.sidechain:\n",
    "                    aps[aidx] = self.atom_pos_to_idx(atom[1])\n",
    "                    aidx += 1\n",
    "            mat.append(aps)\n",
    "        return mat\n",
    "\n",
    "    def get_sidechain_info(self, symbol):\n",
    "        idx = self.symbol_to_idx(symbol)\n",
    "        return copy(self.amino_acids[idx].sidechain)\n",
    "    \n",
    "    def get_sidechain_geometry(self, symbol):\n",
    "        abrv = self.symbol_to_abrv(symbol)\n",
    "        chi_angles_atoms = copy(self.chi_angles_atoms[abrv])\n",
    "        sidechain_bonds = self.sidechain_bonds[abrv]\n",
    "        return (chi_angles_atoms, sidechain_bonds)\n",
    "    \n",
    "    def get_atom_pad_idx(self):\n",
    "        return self.atom2idx[self.atom_pad]\n",
    "    \n",
    "    def get_atom_mask_idx(self):\n",
    "        return self.atom2idx[self.atom_mask]\n",
    "    \n",
    "    def get_atom_pos_pad_idx(self):\n",
    "        return self.atom_pos2idx[self.atom_pos_pad]\n",
    "\n",
    "    def get_atom_pos_mask_idx(self):\n",
    "        return self.atom_pos2idx[self.atom_pos_mask]\n",
    "    \n",
    "    def idx_to_atom(self, idx):\n",
    "        return self.idx2atom[idx]\n",
    "\n",
    "    def atom_to_idx(self, atom):\n",
    "        return self.atom2idx[atom]\n",
    "\n",
    "    def idx_to_atom_pos(self, idx):\n",
    "        return self.idx2atom_pos[idx]\n",
    "    \n",
    "    def atom_pos_to_idx(self, atom_pos):\n",
    "        return self.atom_pos2idx[atom_pos]\n",
    "\n",
    "    def get_num_atom_type(self):\n",
    "        return len(self.idx2atom)\n",
    "    \n",
    "    def get_num_atom_pos(self):\n",
    "        return len(self.idx2atom_pos)\n",
    "\n",
    "    def get_num_amino_acid_type(self):\n",
    "        return len(self.special_mask) - sum(self.special_mask)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.symbol2idx)\n",
    "\n",
    "\n",
    "VOCAB = AminoAcidVocab()\n",
    "\n",
    "\n",
    "def format_aa_abrv(abrv):  # special cases\n",
    "    if abrv == 'MSE':\n",
    "        return 'MET' # substitue MSE with MET\n",
    "    return abrv\n",
    "\n",
    "\n",
    "class Residue:\n",
    "    def __init__(self, symbol: str, coordinate: Dict, _id: Tuple):\n",
    "        self.symbol = symbol\n",
    "        self.coordinate = coordinate\n",
    "        self.sidechain = VOCAB.get_sidechain_info(symbol)\n",
    "        self.id = _id  # (residue_number, insert_code)\n",
    "\n",
    "    def get_symbol(self):\n",
    "        return self.symbol\n",
    "\n",
    "    def get_coord(self, atom_name):\n",
    "        return copy(self.coordinate[atom_name])\n",
    "\n",
    "    def get_coord_map(self) -> Dict[str, List]:\n",
    "        return deepcopy(self.coordinate)\n",
    "\n",
    "    def get_backbone_coord_map(self) -> Dict[str, List]:\n",
    "        coord = { atom: self.coordinate[atom] for atom in self.coordinate if atom in VOCAB.backbone_atoms }\n",
    "        return coord\n",
    "\n",
    "    def get_sidechain_coord_map(self) -> Dict[str, List]:\n",
    "        coord = {}\n",
    "        for atom in self.sidechain:\n",
    "            if atom in self.coordinate:\n",
    "                coord[atom] = self.coordinate[atom]\n",
    "        return coord\n",
    "\n",
    "    def get_atom_names(self):\n",
    "        return list(self.coordinate.keys())\n",
    "\n",
    "    def get_id(self):\n",
    "        return self.id\n",
    "\n",
    "    def set_symbol(self, symbol):\n",
    "        assert VOCAB.symbol_to_abrv(symbol) is not None, f'{symbol} is not an amino acid'\n",
    "        self.symbol = symbol\n",
    "\n",
    "    def set_coord(self, coord):\n",
    "        self.coordinate = deepcopy(coord)\n",
    "\n",
    "    def dist_to(self, residue):  # measured by nearest atoms\n",
    "        xa = np.array(list(self.get_coord_map().values()))\n",
    "        xb = np.array(list(residue.get_coord_map().values()))\n",
    "        if len(xa) == 0 or len(xb) == 0:\n",
    "            return nan\n",
    "        dist = np.linalg.norm(xa[:, None, :] - xb[None, :, :], axis=-1)\n",
    "        return np.min(dist)\n",
    "\n",
    "    def to_bio(self):\n",
    "        _id = (' ', self.id[0], self.id[1])\n",
    "        residue = BResidue(_id, VOCAB.symbol_to_abrv(self.symbol), '    ')\n",
    "        atom_map = self.coordinate\n",
    "        for i, atom in enumerate(atom_map):\n",
    "            fullname = ' ' + atom\n",
    "            while len(fullname) < 4:\n",
    "                fullname += ' '\n",
    "            bio_atom = BAtom(\n",
    "                name=atom,\n",
    "                coord=np.array(atom_map[atom], dtype=np.float32),\n",
    "                bfactor=0,\n",
    "                occupancy=1.0,\n",
    "                altloc=' ',\n",
    "                fullname=fullname,\n",
    "                serial_number=i,\n",
    "                element=atom[0]  # not considering symbols with 2 chars (e.g. FE, MG)\n",
    "            )\n",
    "            residue.add(bio_atom)\n",
    "        return residue\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter([(atom_name, self.coordinate[atom_name]) for atom_name in self.coordinate])\n",
    "\n",
    "\n",
    "class Peptide:\n",
    "    def __init__(self, _id, residues: List[Residue]):\n",
    "        self.residues = residues\n",
    "        self.seq = ''\n",
    "        self.id = _id\n",
    "        for residue in residues:\n",
    "            self.seq += residue.get_symbol()\n",
    "\n",
    "    def set_id(self, _id):\n",
    "        self.id = _id\n",
    "\n",
    "    def get_id(self):\n",
    "        return self.id\n",
    "\n",
    "    def get_seq(self):\n",
    "        return self.seq\n",
    "\n",
    "    def get_span(self, i, j):  # [i, j)\n",
    "        i, j = max(i, 0), min(j, len(self.seq))\n",
    "        if j <= i:\n",
    "            return None\n",
    "        else:\n",
    "            residues = deepcopy(self.residues[i:j])\n",
    "            return Peptide(self.id, residues)\n",
    "\n",
    "    def get_residue(self, i):\n",
    "        return deepcopy(self.residues[i])\n",
    "    \n",
    "    def get_ca_pos(self, i):\n",
    "        return copy(self.residues[i].get_coord('CA'))\n",
    "\n",
    "    def get_cb_pos(self, i):\n",
    "        return copy(self.residues[i].get_coord('CB'))\n",
    "\n",
    "    def set_residue_coord(self, i, coord):\n",
    "        self.residues[i].set_coord(coord)\n",
    "\n",
    "    def set_residue_translation(self, i, vec):\n",
    "        coord = self.residues[i].get_coord_map()\n",
    "        for atom in coord:\n",
    "            ori_vec = coord[atom]\n",
    "            coord[atom] = [a + b for a, b in zip(ori_vec, vec)]\n",
    "        self.set_residue_coord(i, coord)\n",
    "\n",
    "    def set_residue_symbol(self, i, symbol):\n",
    "        self.residues[i].set_symbol(symbol)\n",
    "        self.seq = self.seq[:i] + symbol + self.seq[i+1:]\n",
    "\n",
    "    def set_residue(self, i, symbol, coord):\n",
    "        self.set_residue_symbol(i, symbol)\n",
    "        self.set_residue_coord(i, coord)\n",
    "\n",
    "    def to_bio(self):\n",
    "        chain = BChain(id=self.id)\n",
    "        for residue in self.residues:\n",
    "            chain.add(residue.to_bio())\n",
    "        return chain\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.residues)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.seq\n",
    "\n",
    "\n",
    "class Protein:\n",
    "    def __init__(self, pdb_id, peptides):\n",
    "        self.pdb_id = pdb_id\n",
    "        self.peptides = peptides\n",
    "\n",
    "    @classmethod\n",
    "    def from_pdb(cls, pdb_path):\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        structure = parser.get_structure('anonym', pdb_path)\n",
    "        pdb_id = structure.header['idcode'].upper().strip()\n",
    "        if pdb_id == '':\n",
    "            # deduce from file name\n",
    "            pdb_id = os.path.split(pdb_path)[1].split('.')[0] + '(filename)'\n",
    "\n",
    "        peptides = {}\n",
    "        for chain in structure.get_chains():\n",
    "            _id = chain.get_id()\n",
    "            residues = []\n",
    "            has_non_residue = False\n",
    "            for residue in chain:\n",
    "                abrv = residue.get_resname()\n",
    "                hetero_flag, res_number, insert_code = residue.get_id()\n",
    "                if hetero_flag != ' ':\n",
    "                    continue   # residue from glucose or water\n",
    "                symbol = VOCAB.abrv_to_symbol(abrv)\n",
    "                if symbol is None:\n",
    "                    has_non_residue = True\n",
    "                    # print(f'has non residue: {abrv}')\n",
    "                    break\n",
    "                # filter Hs because not all data include them\n",
    "                atoms = { atom.get_id(): atom.get_coord() for atom in residue if atom.element != 'H' }\n",
    "                residues.append(Residue(\n",
    "                    symbol, atoms, (res_number, insert_code)\n",
    "                ))\n",
    "            if has_non_residue or len(residues) == 0:  # not a peptide\n",
    "                continue\n",
    "            peptides[_id] = Peptide(_id, residues)\n",
    "        return cls(pdb_id, peptides)\n",
    "\n",
    "    def get_id(self):\n",
    "        return self.pdb_id\n",
    "\n",
    "    def num_chains(self):\n",
    "        return len(self.peptides)\n",
    "\n",
    "    def get_chain(self, name):\n",
    "        if name in self.peptides:\n",
    "            return deepcopy(self.peptides[name])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_chain_names(self):\n",
    "        return list(self.peptides.keys())\n",
    "\n",
    "    def to_bio(self):\n",
    "        structure = BStructure(id=self.pdb_id)\n",
    "        model = BModel(id=0)\n",
    "        for name in self.peptides:\n",
    "            model.add(self.peptides[name].to_bio())\n",
    "        structure.add(model)\n",
    "        return structure\n",
    "\n",
    "    def to_pdb(self, path, atoms=None):\n",
    "        if atoms is None:\n",
    "            bio_structure = self.to_bio()\n",
    "        else:\n",
    "            prot = deepcopy(self)\n",
    "            for _, chain in prot:\n",
    "                for residue in chain:\n",
    "                    coordinate = {}\n",
    "                    for atom in atoms:\n",
    "                        if atom in residue.coordinate:\n",
    "                            coordinate[atom] = residue.coordinate[atom]\n",
    "                    residue.coordinate = coordinate\n",
    "            bio_structure = prot.to_bio()\n",
    "        io = PDBIO()\n",
    "        io.set_structure(bio_structure)\n",
    "        io.save(path)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter([(c, self.peptides[c]) for c in self.peptides])\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Protein):\n",
    "            raise TypeError('Cannot compare other type to Protein')\n",
    "        for key in self.peptides:\n",
    "            if key in other.peptides and self.peptides[key].seq == other.peptides[key].seq:\n",
    "                continue\n",
    "            else:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def __str__(self):\n",
    "        res = self.pdb_id + '\\n'\n",
    "        for seq_name in self.peptides:\n",
    "            res += f'\\t{seq_name}: {self.peptides[seq_name]}\\n'\n",
    "        return res\n",
    "\n",
    "\n",
    "class AgAbComplex:\n",
    "\n",
    "    num_interface_residues = 48  # from PNAS (view as epitope)\n",
    "\n",
    "    def __init__(self, antigen: Protein, antibody: Protein, heavy_chain: str, light_chain: str,\n",
    "                 numbering: str='imgt', skip_epitope_cal=False, skip_validity_check=False) -> None:\n",
    "        self.heavy_chain = heavy_chain\n",
    "        self.light_chain = light_chain\n",
    "        self.numbering = numbering\n",
    "\n",
    "        self.antigen = antigen\n",
    "        if skip_validity_check:\n",
    "            self.antibody, self.cdr_pos = antibody, None\n",
    "        else:\n",
    "            self.antibody, self.cdr_pos = self._extract_antibody_info(antibody, numbering)\n",
    "        self.pdb_id = antigen.get_id()\n",
    "\n",
    "        if skip_epitope_cal:\n",
    "            self.epitope = None\n",
    "        else:\n",
    "            self.epitope = self._cal_epitope()\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pdb(cls, pdb_path: str, heavy_chain: str, light_chain: str, antigen_chains: List[str],\n",
    "                 numbering: str='imgt', skip_epitope_cal=False, skip_validity_check=False):\n",
    "        protein = Protein.from_pdb(pdb_path)\n",
    "        pdb_id = protein.get_id()\n",
    "        # print('skipping light chain',protein.get_chain(light_chain))\n",
    "        ab_peptides = {\n",
    "            heavy_chain: protein.get_chain(heavy_chain)\n",
    "            # ,light_chain: protein.get_chain(light_chain)\n",
    "        }\n",
    "        ag_peptides = { chain: protein.get_chain(chain) for chain in antigen_chains if protein.get_chain(chain) is not None }\n",
    "        for chain in antigen_chains:\n",
    "            assert chain in ag_peptides, f'Antigen chain {chain} has something wrong!'\n",
    "\n",
    "        antigen = Protein(pdb_id, ag_peptides)\n",
    "        antibody = Protein(pdb_id, ab_peptides)\n",
    "\n",
    "\n",
    "        # Print the contents of the dictionaries\n",
    "        # not necessary\n",
    "        # print(\"Antibody Peptides:\")\n",
    "        # for chain_name, peptide in ab_peptides.items():\n",
    "        #     print(f\"Chain: {chain_name}\")\n",
    "        #     print(f\"Peptide: {peptide}\")\n",
    "\n",
    "        # print(\"Antigen Peptides:\")\n",
    "        # for chain_name, peptide in ag_peptides.items():\n",
    "        #     print(f\"Chain: {chain_name}\")\n",
    "        #     print(f\"Peptide: {peptide}\")\n",
    "\n",
    "        return cls(antigen, antibody, heavy_chain, light_chain, numbering, skip_epitope_cal, skip_validity_check)\n",
    "\n",
    "    def _extract_antibody_info(self, antibody: Protein, numbering: str):\n",
    "        # calculating cdr pos according to number scheme (type_mapping and conserved residues)\n",
    "        numbering = numbering.lower()\n",
    "        if numbering == 'imgt':\n",
    "            _scheme = IMGT\n",
    "        elif numbering.lower() == 'chothia':\n",
    "            _scheme = Chothia\n",
    "            # for i in list(range(1, 27)) + list(range(39, 56)) + list(range(66, 105)) + list(range(118, 130)):\n",
    "            #     type_mapping[i] = '0'\n",
    "            # for i in range(27, 39):     # cdr1\n",
    "            #     type_mapping[i] = '1'\n",
    "            # for i in range(56, 66):     # cdr2\n",
    "            #     type_mapping[i] = '2'\n",
    "            # for i in range(105, 118):   # cdr3\n",
    "            #     type_mapping[i] = '3'\n",
    "            # conserved = {\n",
    "            #     23: ['CYS'],\n",
    "            #     41: ['TRP'],\n",
    "            #     104: ['CYS'],\n",
    "            #     # 118: ['PHE', 'TRP']\n",
    "            # }\n",
    "        else:\n",
    "            raise NotImplementedError(f'Numbering scheme {numbering} not implemented')\n",
    "\n",
    "        # get cdr/frame denotes\n",
    "        h_type_mapping, l_type_mapping = {}, {}  # - for non-Fv region, 0 for framework, 1/2/3 for cdr1/2/3\n",
    "\n",
    "        for lo, hi in [_scheme.HFR1, _scheme.HFR2, _scheme.HFR3, _scheme.HFR4]:\n",
    "            for i in range(lo, hi + 1):\n",
    "                h_type_mapping[i] = '0'\n",
    "        for cdr, (lo, hi) in zip(['1', '2', '3'], [_scheme.H1, _scheme.H2, _scheme.H3]):\n",
    "            for i in range(lo, hi + 1):\n",
    "                h_type_mapping[i] = cdr\n",
    "        h_conserved = _scheme.Hconserve\n",
    "\n",
    "        for lo, hi in [_scheme.LFR1, _scheme.LFR2, _scheme.LFR3, _scheme.LFR4]:\n",
    "            for i in range(lo, hi + 1):\n",
    "                l_type_mapping[i] = '0'\n",
    "        for cdr, (lo, hi) in zip(['1', '2', '3'], [_scheme.L1, _scheme.L2, _scheme.L3]):\n",
    "            for i in range(lo, hi + 1):\n",
    "                l_type_mapping[i] = cdr\n",
    "        l_conserved = _scheme.Lconserve\n",
    "\n",
    "        # get variable domain and cdr positions\n",
    "        selected_peptides, cdr_pos = {}, {}\n",
    "        # for c, chain_name in zip(['H', 'L'], [self.heavy_chain, self.light_chain]):\n",
    "        for c, chain_name in zip(['H'], [self.heavy_chain]):\n",
    "            chain = antibody.get_chain(chain_name)\n",
    "            # Note: possbly two chains are different segments of a same chain\n",
    "            assert chain is not None, f'Chain {chain_name} not found in the antibody'\n",
    "            type_mapping = h_type_mapping if c == 'H' else l_type_mapping\n",
    "            conserved = h_conserved if c == 'H' else l_conserved\n",
    "            res_type = ''\n",
    "            for i in range(len(chain)):\n",
    "                residue = chain.get_residue(i)\n",
    "                residue_number = residue.get_id()[0]\n",
    "                if residue_number in type_mapping:\n",
    "                    res_type += type_mapping[residue_number]\n",
    "                    if residue_number in conserved:\n",
    "                        hit, symbol = False, residue.get_symbol()\n",
    "                        for conserved_residue in conserved[residue_number]:\n",
    "                            if symbol == VOCAB.abrv_to_symbol(conserved_residue):\n",
    "                                hit = True\n",
    "                                break\n",
    "                        assert hit, f'Not {conserved[residue_number]} at {residue_number}'\n",
    "                else:\n",
    "                    res_type += '-'\n",
    "            if '0' not in res_type:\n",
    "                print(self.heavy_chain, self.light_chain, antibody.pdb_id, res_type)\n",
    "            start, end = res_type.index('0'), res_type.rindex('0')\n",
    "            for cdr in ['1', '2', '3']:\n",
    "                cdr_start, cdr_end = res_type.find(cdr), res_type.rfind(cdr)\n",
    "                assert cdr_start != -1, f'cdr {c}{cdr} not found, residue type: {res_type}'\n",
    "                start, end = min(start, cdr_start), max(end, cdr_end)\n",
    "                cdr_pos[f'CDR-{c}{cdr}'] = (cdr_start, cdr_end)\n",
    "            for cdr in ['1', '2', '3']:\n",
    "                cdr = f'CDR-{c}{cdr}'\n",
    "                cdr_start, cdr_end = cdr_pos[cdr]\n",
    "                cdr_pos[cdr] = (cdr_start - start, cdr_end - start)\n",
    "            chain = chain.get_span(start, end + 1)  # the length may exceed 130 because of inserted amino acids\n",
    "            chain.set_id(chain_name)\n",
    "            selected_peptides[chain_name] = chain\n",
    "\n",
    "        antibody = Protein(antibody.get_id(), selected_peptides)\n",
    "\n",
    "        return antibody, cdr_pos\n",
    "\n",
    "    def _cal_epitope(self):\n",
    "        ag_rids, ag_xs, ab_xs = [], [], []\n",
    "        ag_mask, ab_mask = [], []\n",
    "        cdrh3 = self.get_cdr('H3')\n",
    "        for _type, protein in zip(['ag', 'ab'], [self.antigen, [('A', cdrh3)]]):\n",
    "            is_ag = _type == 'ag'\n",
    "            rids = []\n",
    "            if is_ag: \n",
    "                xs, masks = ag_xs, ag_mask\n",
    "            else:\n",
    "                xs, masks = ab_xs, ab_mask\n",
    "            for chain_name, chain in protein:\n",
    "                for i, residue in enumerate(chain):\n",
    "                    bb_coord = residue.get_backbone_coord_map()\n",
    "                    sc_coord = residue.get_sidechain_coord_map()\n",
    "                    coord = {}\n",
    "                    coord.update(bb_coord)\n",
    "                    coord.update(sc_coord)\n",
    "                    num_pad = VOCAB.MAX_ATOM_NUMBER - len(coord)\n",
    "                    x = [coord[key] for key in coord] + [[0, 0, 0] for _ in range(num_pad)]\n",
    "                    mask = [1 for _ in coord] + [0 for _ in range(num_pad)]\n",
    "                    rids.append((chain_name, i))\n",
    "                    xs.append(x)\n",
    "                    masks.append(mask)\n",
    "            if is_ag:\n",
    "                ag_rids = rids\n",
    "        assert len(ag_xs) != 0, 'No antigen structure!'\n",
    "        # calculate distance\n",
    "        ag_xs, ab_xs = np.array(ag_xs), np.array(ab_xs)  # [Nag/ab, M, 3], M == MAX_ATOM_NUM\n",
    "        ag_mask, ab_mask = np.array(ag_mask).astype('bool'), np.array(ab_mask).astype('bool')  # [Nag/ab, M]\n",
    "        dist = np.linalg.norm(ag_xs[:, None] - ab_xs[None, :], axis=-1)  # [Nag, Nab, M]\n",
    "        dist = dist + np.logical_not(ag_mask[:, None] * ab_mask[None, :]) * 1e6  # [Nag, Nab, M]\n",
    "        min_dists = np.min(np.min(dist, axis=-1), axis=-1)  # [ag_len]\n",
    "        topk = min(len(min_dists), self.num_interface_residues)\n",
    "        ind = np.argpartition(-min_dists, -topk)[-topk:]\n",
    "        epitope = []\n",
    "        for idx in ind:\n",
    "            chain_name, i = ag_rids[idx]\n",
    "            residue = self.antigen.peptides[chain_name].get_residue(i)\n",
    "            epitope.append((residue, chain_name, i))\n",
    "        return epitope\n",
    "\n",
    "    def get_id(self) -> str:\n",
    "        return self.antibody.pdb_id\n",
    "\n",
    "    def get_antigen(self) -> Protein:\n",
    "        return deepcopy(self.antigen)\n",
    "\n",
    "    def get_epitope(self, cdrh3_pos=None) -> List[Tuple[Residue, str, int]]:\n",
    "        if cdrh3_pos is not None:\n",
    "            backup = self.cdr_pos\n",
    "            self.cdr_pos = {'CDR-H3': [cdrh3_pos[0], cdrh3_pos[1]]}\n",
    "            epitope = self._cal_epitope()\n",
    "            self.cdr_pos = backup\n",
    "            return deepcopy(epitope)\n",
    "        if self.epitope is None:\n",
    "            self.epitope = self._cal_epitope()\n",
    "        return deepcopy(self.epitope)\n",
    "\n",
    "    def get_interacting_residues(self, dist_cutoff=5) -> Tuple[List[int], List[int]]:\n",
    "        ag_rids, ag_xs, ab_xs = [], [], []\n",
    "        for chain_name in self.antigen.get_chain_names():\n",
    "            chain = self.antigen.get_chain(chain_name)\n",
    "            for i in range(len(chain)):\n",
    "                try:\n",
    "                    x = chain.get_ca_pos(i)\n",
    "                except KeyError:  # CA position is missing\n",
    "                    continue\n",
    "                ag_rids.append((chain_name, i))\n",
    "                ag_xs.append(x)\n",
    "        for chain_name in self.antibody.get_chain_names():\n",
    "            chain = self.antibody.get_chain(chain_name)\n",
    "            for i in range(len(chain)):\n",
    "                try:\n",
    "                    x = chain.get_ca_pos(i)\n",
    "                except KeyError:\n",
    "                    continue\n",
    "                ab_xs.append(x)\n",
    "        assert len(ag_xs) != 0, 'No antigen structure!'\n",
    "        # calculate distance\n",
    "        ag_xs, ab_xs = np.array(ag_xs), np.array(ab_xs)\n",
    "        dist = np.linalg.norm(ag_xs[:, None, :] - ab_xs[None, :, :], axis=-1)\n",
    "        min_dists = np.min(dist, axis=1)  # [ag_len]\n",
    "        topk = min(len(min_dists), self.num_interface_residues)\n",
    "        ind = np.argpartition(-min_dists, -topk)[-topk:]\n",
    "        epitope = []\n",
    "        for idx in ind:\n",
    "            chain_name, i = ag_rids[idx]\n",
    "            residue = self.antigen.peptides[chain_name].get_residue(i)\n",
    "            epitope.append((residue, chain_name, i))\n",
    "        return\n",
    "\n",
    "    def get_heavy_chain(self) -> Peptide:\n",
    "        return self.antibody.get_chain(self.heavy_chain)\n",
    "\n",
    "    def get_light_chain(self) -> Peptide:\n",
    "        return self.antibody.get_chain(self.light_chain)\n",
    "\n",
    "    def get_framework(self, fr):  # H/L + FR + 1/2/3/4\n",
    "        seg_id = int(fr[-1])\n",
    "        chain = self.get_heavy_chain() if fr[0] == 'H' else self.get_light_chain()\n",
    "        begin, end = -1, -1\n",
    "        if seg_id == 1:\n",
    "            begin, end = 0, self.get_cdr_pos(fr[0] + str(seg_id))[0]\n",
    "        elif seg_id == 4:\n",
    "            begin, end = self.get_cdr_pos(fr[0] + '3')[-1] + 1, len(chain)\n",
    "        else:\n",
    "            begin = self.get_cdr_pos(fr[0] + str(seg_id - 1))[-1] + 1\n",
    "            end = self.get_cdr_pos(fr[0] + str(seg_id))[0]\n",
    "        return chain.get_span(begin, end)\n",
    "\n",
    "    def get_cdr_pos(self, cdr='H3'):  # H/L + 1/2/3, return [begin, end] position\n",
    "        cdr = f'CDR-{cdr}'.upper()\n",
    "        if cdr in self.cdr_pos:\n",
    "            return self.cdr_pos[cdr]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_cdr(self, cdr='H3'):\n",
    "        cdr = cdr.upper()\n",
    "        pos = self.get_cdr_pos(cdr)\n",
    "        if pos is None:\n",
    "            return None\n",
    "        chain = self.get_heavy_chain() if 'H' in cdr else self.get_light_chain()\n",
    "        return chain.get_span(pos[0], pos[1] + 1)\n",
    "\n",
    "    def to_pdb(self, path, atoms=None):\n",
    "        peptides = {}\n",
    "        for name in self.antigen.get_chain_names():\n",
    "            peptides[name] = self.antigen.get_chain(name)\n",
    "        for name in self.antibody.get_chain_names():\n",
    "            peptides[name] = self.antibody.get_chain(name)\n",
    "        protein = Protein(self.get_id(), peptides)\n",
    "        protein.to_pdb(path, atoms)\n",
    "    \n",
    "    def __str__(self):\n",
    "        pdb_info = f'PDB ID: {self.pdb_id}'\n",
    "        antibody_info = f'Antibody H-{self.heavy_chain} ({len(self.get_heavy_chain())}), ' + \\\n",
    "                        f'L-{self.light_chain} ({len(self.get_light_chain())})'\n",
    "        antigen_info = f'Antigen Chains: {[(ag, len(self.antigen.get_chain(ag))) for ag in self.antigen.get_chain_names()]}'\n",
    "        cdr_info = f'CDRs: \\n'\n",
    "        for name in self.cdr_pos:\n",
    "            chain = self.get_heavy_chain() if 'H' in name else self.get_light_chain()\n",
    "            start, end = self.cdr_pos[name]\n",
    "            cdr_info += f'\\t{name}: [{start}, {end}], {chain.seq[start:end + 1]}\\n'\n",
    "        epitope_info = f'Epitope: \\n'\n",
    "        residue_map = {}\n",
    "        for _, chain_name, i in self.get_epitope():\n",
    "            if chain_name not in residue_map:\n",
    "                residue_map[chain_name] = []\n",
    "            residue_map[chain_name].append(i)\n",
    "        for chain_name in residue_map:\n",
    "            epitope_info += f'\\t{chain_name}: {sorted(residue_map[chain_name])}\\n'\n",
    "\n",
    "        sep = '\\n' + '=' * 20 + '\\n'\n",
    "        return sep + pdb_info + '\\n' + antibody_info + '\\n' + cdr_info + '\\n' + antigen_info + '\\n' + epitope_info + sep\n",
    "\n",
    "\n",
    "def merge_to_one_chain(protein: Protein):\n",
    "    residues = []\n",
    "    chain_order = sorted(protein.get_chain_names())\n",
    "    for chain_name in chain_order:\n",
    "        chain = protein.get_chain(chain_name)\n",
    "        for _, residue in enumerate(chain.residues):\n",
    "            residue.id = (len(residues), ' ')\n",
    "            residues.append(residue)\n",
    "    return Protein(protein.get_id(), {'A': Peptide('A', residues)})\n",
    "\n",
    "\n",
    "def fetch_from_pdb(identifier):\n",
    "    # example identifier: 1FBI\n",
    "    url = 'https://data.rcsb.org/rest/v1/core/entry/' + identifier\n",
    "    res = requests.get(url)\n",
    "    if res.status_code != 200:\n",
    "        return None\n",
    "    url = f'https://files.rcsb.org/download/{identifier}.pdb'\n",
    "    text = requests.get(url)\n",
    "    data = res.json()\n",
    "    data['pdb'] = text.text\n",
    "    return data\n",
    "\n",
    "\n",
    "VOCAB = AminoAcidVocab()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# from https://github.com/charnley/rmsd/blob/master/rmsd/calculate_rmsd.py\n",
    "def kabsch_rotation(P, Q):\n",
    "    \"\"\"\n",
    "    Using the Kabsch algorithm with two sets of paired point P and Q, centered\n",
    "    around the centroid. Each vector set is represented as an NxD\n",
    "    matrix, where D is the the dimension of the space.\n",
    "    The algorithm works in three steps:\n",
    "    - a centroid translation of P and Q (assumed done before this function\n",
    "      call)\n",
    "    - the computation of a covariance matrix C\n",
    "    - computation of the optimal rotation matrix U\n",
    "    For more info see http://en.wikipedia.org/wiki/Kabsch_algorithm\n",
    "    Parameters\n",
    "    ----------\n",
    "    P : array\n",
    "        (N,D) matrix, where N is points and D is dimension.\n",
    "    Q : array\n",
    "        (N,D) matrix, where N is points and D is dimension.\n",
    "    Returns\n",
    "    -------\n",
    "    U : matrix\n",
    "        Rotation matrix (D,D)\n",
    "    \"\"\"\n",
    "\n",
    "    # Computation of the covariance matrix\n",
    "    C = np.dot(np.transpose(P), Q)\n",
    "\n",
    "    # Computation of the optimal rotation matrix\n",
    "    # This can be done using singular value decomposition (SVD)\n",
    "    # Getting the sign of the det(V)*(W) to decide\n",
    "    # whether we need to correct our rotation matrix to ensure a\n",
    "    # right-handed coordinate system.\n",
    "    # And finally calculating the optimal rotation matrix U\n",
    "    # see http://en.wikipedia.org/wiki/Kabsch_algorithm\n",
    "    V, S, W = np.linalg.svd(C)\n",
    "    d = (np.linalg.det(V) * np.linalg.det(W)) < 0.0\n",
    "\n",
    "    if d:\n",
    "        S[-1] = -S[-1]\n",
    "        V[:, -1] = -V[:, -1]\n",
    "\n",
    "    # Create Rotation matrix U\n",
    "    U = np.dot(V, W)\n",
    "\n",
    "    return U\n",
    "\n",
    "\n",
    "# have been validated with kabsch from RefineGNN\n",
    "def kabsch(a, b):\n",
    "    # find optimal rotation matrix to transform a into b\n",
    "    # a, b are both [N, 3]\n",
    "    # a_aligned = aR + t\n",
    "    a, b = np.array(a), np.array(b)\n",
    "    a_mean = np.mean(a, axis=0)\n",
    "    b_mean = np.mean(b, axis=0)\n",
    "    a_c = a - a_mean\n",
    "    b_c = b - b_mean\n",
    "\n",
    "    rotation = kabsch_rotation(a_c, b_c)\n",
    "    # a_aligned = np.dot(a_c, rotation)\n",
    "    # t = b_mean - np.mean(a_aligned, axis=0)\n",
    "    # a_aligned += t\n",
    "    t = b_mean - np.dot(a_mean, rotation)\n",
    "    a_aligned = np.dot(a, rotation) + t\n",
    "\n",
    "    return a_aligned, rotation, t\n",
    "    \n",
    "\n",
    "# a: [N, 3], b: [N, 3]\n",
    "def compute_rmsd(a, b, aligned=False):  # amino acids level rmsd\n",
    "    if aligned:\n",
    "        a_aligned = a\n",
    "    else:\n",
    "        a_aligned, _, _ = kabsch(a, b)\n",
    "    dist = np.sum((a_aligned - b) ** 2, axis=-1)\n",
    "    rmsd = np.sqrt(dist.sum() / a.shape[0])\n",
    "    return float(rmsd)\n",
    "\n",
    "\n",
    "def kabsch_torch(A, B, requires_grad=False):\n",
    "    \"\"\"\n",
    "    See: https://en.wikipedia.org/wiki/Kabsch_algorithm\n",
    "    2-D or 3-D registration with known correspondences.\n",
    "    Registration occurs in the zero centered coordinate system, and then\n",
    "    must be transported back.\n",
    "        Args:\n",
    "        -    A: Torch tensor of shape (N,D) -- Point Cloud to Align (source)\n",
    "        -    B: Torch tensor of shape (N,D) -- Reference Point Cloud (target)\n",
    "        Returns:\n",
    "        -    R: optimal rotation\n",
    "        -    t: optimal translation\n",
    "    Test on rotation + translation and on rotation + translation + reflection\n",
    "        >>> A = torch.tensor([[1., 1.], [2., 2.], [1.5, 3.]], dtype=torch.float)\n",
    "        >>> R0 = torch.tensor([[np.cos(60), -np.sin(60)], [np.sin(60), np.cos(60)]], dtype=torch.float)\n",
    "        >>> B = (R0.mm(A.T)).T\n",
    "        >>> t0 = torch.tensor([3., 3.])\n",
    "        >>> B += t0\n",
    "        >>> R, t = find_rigid_alignment(A, B)\n",
    "        >>> A_aligned = (R.mm(A.T)).T + t\n",
    "        >>> rmsd = torch.sqrt(((A_aligned - B)**2).sum(axis=1).mean())\n",
    "        >>> rmsd\n",
    "        tensor(3.7064e-07)\n",
    "        >>> B *= torch.tensor([-1., 1.])\n",
    "        >>> R, t = find_rigid_alignment(A, B)\n",
    "        >>> A_aligned = (R.mm(A.T)).T + t\n",
    "        >>> rmsd = torch.sqrt(((A_aligned - B)**2).sum(axis=1).mean())\n",
    "        >>> rmsd\n",
    "        tensor(3.7064e-07)\n",
    "    \"\"\"\n",
    "    a_mean = A.mean(axis=0)\n",
    "    b_mean = B.mean(axis=0)\n",
    "    A_c = A - a_mean\n",
    "    B_c = B - b_mean\n",
    "    # Covariance matrix\n",
    "    H = A_c.T.mm(B_c)\n",
    "    # U, S, V = torch.svd(H)\n",
    "    if requires_grad:  # try more times to find a stable solution\n",
    "        assert not torch.isnan(H).any()\n",
    "        U, S, Vt = torch.linalg.svd(H)\n",
    "        num_it = 0\n",
    "        while torch.min(S) < 1e-3 or torch.min(torch.abs((S**2).view(1,3) - (S**2).view(3,1) + torch.eye(3).to(S.device))) < 1e-2:\n",
    "            H = H + torch.rand(3,3).to(H.device) * torch.eye(3).to(H.device)\n",
    "            U, S, Vt = torch.linalg.svd(H)\n",
    "            num_it += 1\n",
    "\n",
    "            if num_it > 10:\n",
    "                raise RuntimeError('SVD consistently numerically unstable! Exitting ... ')\n",
    "    else:\n",
    "        U, S, Vt = torch.linalg.svd(H)\n",
    "    V = Vt.T\n",
    "    # rms\n",
    "    d = (torch.linalg.det(U) * torch.linalg.det(V)) < 0.0\n",
    "    if d:\n",
    "        SS = torch.diag(torch.tensor([1. for _ in range(len(U) - 1)] + [-1.], device=U.device, dtype=U.dtype))\n",
    "        U = U @ SS\n",
    "        # U[:, -1] = -U[:, -1]\n",
    "    # Rotation matrix\n",
    "    R = V.mm(U.T)\n",
    "    # Translation vector\n",
    "    t = b_mean[None, :] - R.mm(a_mean[None, :].T).T\n",
    "    t = (t.T).squeeze()\n",
    "    return R.mm(A.T).T + t, R, t\n",
    "\n",
    "\n",
    "def batch_kabsch_torch(A, B):\n",
    "    '''\n",
    "    A: [B, N, 3]\n",
    "    B: [B, N, 3]\n",
    "    '''\n",
    "    a_mean = A.mean(dim=1, keepdims=True)\n",
    "    b_mean = B.mean(dim=1, keepdims=True)\n",
    "    A_c = A - a_mean\n",
    "    B_c = B - b_mean\n",
    "    # Covariance matrix\n",
    "    H = torch.bmm(A_c.transpose(1,2), B_c)  # [B, 3, 3]\n",
    "    U, S, Vt = torch.linalg.svd(H)  # [B, 3, 3]\n",
    "    V = Vt.transpose(1, 2)\n",
    "    # rms\n",
    "    d = ((torch.linalg.det(U) * torch.linalg.det(V)) < 0.0).long()  # [B]\n",
    "    nSS = torch.diag(torch.tensor([1. for _ in range(len(U))], device=U.device, dtype=U.dtype))\n",
    "    SS = torch.diag(torch.tensor([1. for _ in range(len(U) - 1)] + [-1.], device=U.device, dtype=U.dtype))\n",
    "    bSS = torch.stack([nSS, SS], dim=0)[d]  # [B, 3, 3]\n",
    "    U = torch.bmm(U, bSS)\n",
    "    # Rotation matrix\n",
    "    R = torch.bmm(V, U.transpose(1,2))  # [B, 3, 3]\n",
    "    # Translation vector\n",
    "    t = b_mean - torch.bmm(R, a_mean.transpose(1,2)).transpose(1,2)\n",
    "    A_aligned = torch.bmm(R, A.transpose(1,2)).transpose(1,2) + t\n",
    "    return A_aligned, R, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def singleton(cls):\n",
    "    _instance = {}\n",
    "\n",
    "    def inner(*args, **kwargs):\n",
    "        if cls not in _instance:\n",
    "            _instance[cls] = cls(*args, **kwargs)\n",
    "        return _instance[cls]\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@singleton\n",
    "class ConserveTemplateGenerator:\n",
    "    def __init__(self, json_path=None):\n",
    "        if json_path is None:\n",
    "\n",
    "            folder = 'data/'\n",
    "            json_path = os.path.join(folder, 'template.json')\n",
    "            # print(json_path)\n",
    "        with open(json_path, 'r') as fin:\n",
    "            self.template_map = json.load(fin)\n",
    "    \n",
    "    def _chain_template(self, cplx: AgAbComplex, poses, n_channel, heavy=True):\n",
    "        chain = cplx.get_heavy_chain() if heavy else cplx.get_light_chain()\n",
    "        chain_name = 'H' if heavy else 'L'\n",
    "        hit_map = { pos: False for pos in poses }\n",
    "        X, hit_index = [], []\n",
    "        for i, residue in enumerate(chain):\n",
    "            pos, _ = residue.get_id()\n",
    "            pos = str(pos)\n",
    "            if pos in hit_map:\n",
    "                coord = self.template_map[chain_name][pos]  # N, CA, C, O\n",
    "                ca, num_sc = coord[1], n_channel - len(coord)\n",
    "                coord.extend([ca for _ in range(num_sc)])\n",
    "                hit_index.append(i)\n",
    "                coord = np.array(coord)\n",
    "            else:\n",
    "                coord = [[0, 0, 0] for _ in range(n_channel)]\n",
    "            X.append(coord)\n",
    "        # uniform distribution between residues and extension at two ends\n",
    "        for left_i, right_i in zip(hit_index[:-1], hit_index[1:]):\n",
    "            left, right = X[left_i], X[right_i]\n",
    "            span, index_span = right - left, right_i - left_i\n",
    "            span = span / index_span\n",
    "            for i in range(left_i + 1, right_i):\n",
    "                X[i] = X[i - 1] + span\n",
    "        # start and end\n",
    "        if hit_index[0] != 0:\n",
    "            left_i = hit_index[0]\n",
    "            span = X[left_i] - X[left_i + 1]\n",
    "            for i in reversed(range(0, left_i)):\n",
    "                X[i] = X[i + 1] + span\n",
    "        if hit_index[-1] != len(X) - 1:\n",
    "            right_i = hit_index[-1]\n",
    "            span = X[right_i] - X[right_i - 1]\n",
    "            for i in range(right_i + 1, len(X)):\n",
    "                X[i] = X[i - 1] + span\n",
    "        return X, hit_index\n",
    "\n",
    "    def construct_template(self, cplx: AgAbComplex, n_channel=VOCAB.MAX_ATOM_NUMBER, align=True):\n",
    "        hc, hc_hit = self._chain_template(cplx, self.template_map['H'], n_channel, heavy=True)\n",
    "        # lc, lc_hit = self._chain_template(cplx, self.template_map['L'], n_channel, heavy=False)\n",
    "        template = np.array(hc)  # [N, n_channel, 3]\n",
    "        if align:\n",
    "            # align (will be dropped in the future)\n",
    "            true_X_bb, temp_X_bb = [], []\n",
    "            chains = [cplx.get_heavy_chain(), cplx.get_light_chain()]\n",
    "            temps, hits = [hc], [hc_hit]\n",
    "            for chain, temp, hit in zip(chains, temps, hits):\n",
    "                for i, residue_temp in zip(hit, temp):\n",
    "                    residue = chain.get_residue(i)\n",
    "                    bb = residue.get_backbone_coord_map()\n",
    "                    for ai, atom in enumerate(VOCAB.backbone_atoms):\n",
    "                        if atom not in bb:\n",
    "                            continue\n",
    "                        true_X_bb.append(bb[atom])\n",
    "                        temp_X_bb.append(residue_temp[ai])\n",
    "            true_X_bb, temp_X_bb = np.array(true_X_bb), np.array(temp_X_bb)\n",
    "            _, Q, t = kabsch(temp_X_bb, true_X_bb)\n",
    "            template = np.dot(template, Q) + t\n",
    "        return template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def _generate_chain_data(residues, start):\n",
    "    backbone_atoms = VOCAB.backbone_atoms\n",
    "    # Coords, Sequence, residue positions, mask for loss calculation (exclude missing coordinates)\n",
    "    X, S, res_pos, xloss_mask = [], [], [], []\n",
    "    # global node\n",
    "    # coordinates will be set to the center of the chain\n",
    "    X.append([[0, 0, 0] for _ in range(VOCAB.MAX_ATOM_NUMBER)])  \n",
    "    S.append(VOCAB.symbol_to_idx(start))\n",
    "    res_pos.append(0)\n",
    "    xloss_mask.append([0 for _ in range(VOCAB.MAX_ATOM_NUMBER)])\n",
    "    # other nodes\n",
    "    for residue in residues:\n",
    "        residue_xloss_mask = [0 for _ in range(VOCAB.MAX_ATOM_NUMBER)]\n",
    "        bb_atom_coord = residue.get_backbone_coord_map()\n",
    "        sc_atom_coord = residue.get_sidechain_coord_map()\n",
    "        if 'CA' not in bb_atom_coord:\n",
    "            for atom in bb_atom_coord:\n",
    "                ca_x = bb_atom_coord[atom]\n",
    "                print_log(f'no ca, use {atom}', level='DEBUG')\n",
    "                break\n",
    "        else:\n",
    "            ca_x = bb_atom_coord['CA']\n",
    "        x = [ca_x for _ in range(VOCAB.MAX_ATOM_NUMBER)]\n",
    "        \n",
    "        i = 0\n",
    "        for atom in backbone_atoms:\n",
    "            if atom in bb_atom_coord:\n",
    "                x[i] = bb_atom_coord[atom]\n",
    "                residue_xloss_mask[i] = 1\n",
    "            i += 1\n",
    "        for atom in residue.sidechain:\n",
    "            if atom in sc_atom_coord:\n",
    "                x[i] = sc_atom_coord[atom]\n",
    "                residue_xloss_mask[i] = 1\n",
    "            i += 1\n",
    "\n",
    "        X.append(x)\n",
    "        S.append(VOCAB.symbol_to_idx(residue.get_symbol()))\n",
    "        res_pos.append(residue.get_id()[0])\n",
    "        xloss_mask.append(residue_xloss_mask)\n",
    "    X = np.array(X)\n",
    "    center = np.mean(X[1:].reshape(-1, 3), axis=0)\n",
    "    X[0] = center  # set center\n",
    "    if start == VOCAB.BOA:  # epitope does not have position encoding\n",
    "        res_pos = [0 for _ in res_pos]\n",
    "    data = {'X': X, 'S': S, 'residue_pos': res_pos, 'xloss_mask': xloss_mask}\n",
    "    return data\n",
    "\n",
    "\n",
    "# use this class to splice the dataset and maintain only one part of it in RAM\n",
    "# Antibody-Antigen Complex dataset\n",
    "class E2EDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path, save_dir=None, cdr=None, paratope='H3', full_antigen=False, num_entry_per_file=-1, random=False):\n",
    "        '''\n",
    "        file_path: path to the dataset\n",
    "        save_dir: directory to save the processed data\n",
    "        cdr: which cdr to generate (L1/2/3, H1/2/3) (can be list), None for all including framework\n",
    "        paratope: which cdr to use as paratope (L1/2/3, H1/2/3) (can be list)\n",
    "        full_antigen: whether to use the full antigen information\n",
    "        num_entry_per_file: number of entries in a single file. -1 to save all data into one file \n",
    "                            (In-memory dataset)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.cdr = cdr\n",
    "        self.paratope = paratope\n",
    "        self.full_antigen = full_antigen\n",
    "        if save_dir is None:\n",
    "            if not os.path.isdir(file_path):\n",
    "                save_dir = os.path.split(file_path)[0]\n",
    "            else:\n",
    "                save_dir = file_path\n",
    "            prefix = os.path.split(file_path)[1]\n",
    "            if '.' in prefix:\n",
    "                prefix = prefix.split('.')[0]\n",
    "            save_dir = os.path.join(save_dir, f'{prefix}_processed')\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        metainfo_file = os.path.join(save_dir, '_metainfo')\n",
    "        self.data: List[AgAbComplex] = []  # list of ABComplex\n",
    "\n",
    "        # try loading preprocessed files\n",
    "        need_process = False\n",
    "        try:\n",
    "            with open(metainfo_file, 'r') as fin:\n",
    "                metainfo = json.load(fin)\n",
    "                self.num_entry = metainfo['num_entry']\n",
    "                self.file_names = metainfo['file_names']\n",
    "                self.file_num_entries = metainfo['file_num_entries']\n",
    "        except FileNotFoundError:\n",
    "            print_log('No meta-info file found, start processing', level='INFO')\n",
    "            need_process = True\n",
    "        except Exception as e:\n",
    "            print_log(f'Faild to load file {metainfo_file}, error: {e}', level='WARN')\n",
    "            need_process = True\n",
    "\n",
    "        if need_process:\n",
    "            # preprocess\n",
    "            self.file_names, self.file_num_entries = [], []\n",
    "            self.preprocess(file_path, save_dir, num_entry_per_file)\n",
    "            self.num_entry = sum(self.file_num_entries)\n",
    "\n",
    "            metainfo = {\n",
    "                'num_entry': self.num_entry,\n",
    "                'file_names': self.file_names,\n",
    "                'file_num_entries': self.file_num_entries\n",
    "            }\n",
    "            with open(metainfo_file, 'w') as fout:\n",
    "                json.dump(metainfo, fout)\n",
    "\n",
    "        self.random = random\n",
    "        self.cur_file_idx, self.cur_idx_range = 0, (0, self.file_num_entries[0])  # left close, right open\n",
    "        self._load_part()\n",
    "\n",
    "        # user defined variables\n",
    "        self.idx_mapping = [i for i in range(self.num_entry)]\n",
    "        self.mode = '101'  # H/L/Antigen, 1 for include, 0 for exclude\n",
    "\n",
    "    def _save_part(self, save_dir, num_entry):\n",
    "        file_name = os.path.join(save_dir, f'part_{len(self.file_names)}.pkl')\n",
    "        print_log(f'Saving {file_name} ...')\n",
    "        file_name = os.path.abspath(file_name)\n",
    "        if num_entry == -1:\n",
    "            end = len(self.data)\n",
    "        else:\n",
    "            end = min(num_entry, len(self.data))\n",
    "        with open(file_name, 'wb') as fout:\n",
    "            pickle.dump(self.data[:end], fout)\n",
    "        self.file_names.append(file_name)\n",
    "        self.file_num_entries.append(end)\n",
    "        self.data = self.data[end:]\n",
    "\n",
    "    def _load_part(self):\n",
    "        f = self.file_names[self.cur_file_idx]\n",
    "        print_log(f'Loading preprocessed file {f}, {self.cur_file_idx + 1}/{len(self.file_names)}')\n",
    "        with open(f, 'rb') as fin:\n",
    "            del self.data\n",
    "            self.data = pickle.load(fin)\n",
    "        self.access_idx = [i for i in range(len(self.data))]\n",
    "        if self.random:\n",
    "            np.random.shuffle(self.access_idx)\n",
    "\n",
    "    def _check_load_part(self, idx):\n",
    "        if idx < self.cur_idx_range[0]:\n",
    "            while idx < self.cur_idx_range[0]:\n",
    "                end = self.cur_idx_range[0]\n",
    "                self.cur_file_idx -= 1\n",
    "                start = end - self.file_num_entries[self.cur_file_idx]\n",
    "                self.cur_idx_range = (start, end)\n",
    "            self._load_part()\n",
    "        elif idx >= self.cur_idx_range[1]:\n",
    "            while idx >= self.cur_idx_range[1]:\n",
    "                start = self.cur_idx_range[1]\n",
    "                self.cur_file_idx += 1\n",
    "                end = start + self.file_num_entries[self.cur_file_idx]\n",
    "                self.cur_idx_range = (start, end)\n",
    "            self._load_part()\n",
    "        idx = self.access_idx[idx - self.cur_idx_range[0]]\n",
    "        return idx\n",
    "     \n",
    "    def __len__(self):\n",
    "        return self.num_entry\n",
    "\n",
    "    ########### load data from file_path and add to self.data ##########\n",
    "    def preprocess(self, file_path, save_dir, num_entry_per_file):\n",
    "        '''\n",
    "        Load data from file_path and add processed data entries to self.data.\n",
    "        Remember to call self._save_data(num_entry_per_file) to control the number\n",
    "        of items in self.data (this function will save the first num_entry_per_file\n",
    "        data and release them from self.data) e.g. call it when len(self.data) reaches\n",
    "        num_entry_per_file.\n",
    "        '''\n",
    "        with open(file_path, 'r') as fin:\n",
    "            lines = fin.read().strip().split('\\n')\n",
    "        # line_id = 0\n",
    "        for line in tqdm(lines):\n",
    "            # if line_id < 206:\n",
    "            #     line_id += 1\n",
    "            #     continue\n",
    "            item = json.loads(line)\n",
    "            try:\n",
    "                # print('making AgABComplex')\n",
    "                cplx = AgAbComplex.from_pdb(\n",
    "                    item['pdb_data_path'], item['heavy_chain'], item['light_chain'],\n",
    "                    item['antigen_chains'])\n",
    "            except AssertionError as e:\n",
    "                print_log(e, level='ERROR')\n",
    "                print_log(f'parse {item[\"pdb\"]} pdb failed, skip', level='ERROR')\n",
    "                continue\n",
    "\n",
    "            self.data.append(cplx)\n",
    "            if num_entry_per_file > 0 and len(self.data) >= num_entry_per_file:\n",
    "                self._save_part(save_dir, num_entry_per_file)\n",
    "        if len(self.data):\n",
    "            self._save_part(save_dir, num_entry_per_file)\n",
    "\n",
    "    ########## override get item ##########\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        an example of the returned data\n",
    "        {\n",
    "            'X': [n, n_channel, 3],\n",
    "            'S': [n],\n",
    "            'cmask': [n],\n",
    "            'smask': [n],\n",
    "            'paratope_mask': [n],\n",
    "            'xloss_mask': [n, n_channel],\n",
    "            'template': [n, n_channel, 3]\n",
    "        }\n",
    "        '''\n",
    "        idx = self.idx_mapping[idx]\n",
    "        # print('idx is',idx)\n",
    "\n",
    "        idx = self._check_load_part(idx)\n",
    "        # print('load_part idx ',idx)\n",
    "\n",
    "        item = self.data[idx]\n",
    "        # print('item is ',item)\n",
    "\n",
    "        # antigen\n",
    "        ag_residues = []\n",
    "\n",
    "        if self.full_antigen:\n",
    "            # get antigen residues\n",
    "            ag = item.get_antigen()\n",
    "            for chain in ag.get_chain_names():\n",
    "                chain = ag.get_chain(chain)\n",
    "                for i in range(len(chain)):\n",
    "                    residue = chain.get_residue(i)\n",
    "                    ag_residues.append(residue)\n",
    "        else:\n",
    "            # get antigen residues (epitope only)\n",
    "            for residue, chain, i in item.get_epitope():\n",
    "                # print(residue, chain, i)\n",
    "                ag_residues.append(residue)\n",
    "        # print('ag resiues are',ag_residues)\n",
    "        # generate antigen data\n",
    "        ag_data = _generate_chain_data(ag_residues, VOCAB.BOA)\n",
    "        # print('ag_data is ',ag_data)\n",
    "        hc, lc = item.get_heavy_chain(), item.get_light_chain()\n",
    "        hc_residues, lc_residues = [], []\n",
    "\n",
    "        # generate heavy chain data\n",
    "        for i in range(len(hc)):\n",
    "            hc_residues.append(hc.get_residue(i))\n",
    "        hc_data = _generate_chain_data(hc_residues, VOCAB.BOH)\n",
    "        # print('hc data is',hc_data)\n",
    "        # generate light chain data\n",
    "        # for i in range(len(lc)):\n",
    "        #     lc_residues.append(lc.get_residue(i))\n",
    "        # lc_data = _generate_chain_data(lc_residues, VOCAB.BOL)\n",
    "        # print('lc data is',lc_data)\n",
    "\n",
    "        data = { key: np.concatenate([ag_data[key], hc_data[key]], axis=0) \\\n",
    "                 for key in hc_data}\n",
    "        # print('data is ',data)\n",
    "\n",
    "        # smask (sequence) and cmask (coordinates): 0 for fixed, 1 for generate\n",
    "        # not generate coordinates of global node and antigen \n",
    "        cmask = [0 for _ in ag_data['S']] + [0] + [1 for _ in hc_data['S'][1:]]\n",
    "        # according to the setting of cdr\n",
    "        if self.cdr is None:\n",
    "            smask = cmask\n",
    "        else:\n",
    "            smask = [0 for _ in range(len(ag_data['S']) + len(hc_data['S']) )]\n",
    "            cdrs = [self.cdr] if type(self.cdr) == str else self.cdr\n",
    "            for cdr in cdrs:\n",
    "                cdr_range = item.get_cdr_pos(cdr)\n",
    "                offset = len(ag_data['S']) + 1 + (0 if cdr[0] == 'H' else len(hc_data['S']))\n",
    "                for idx in range(offset + cdr_range[0], offset + cdr_range[1] + 1):\n",
    "                    smask[idx] = 1\n",
    "\n",
    "        data['cmask'], data['smask'] = cmask, smask\n",
    "        # print('masks are ',data['cmask'], data['smask'])\n",
    "        paratope_mask = [0 for _ in range(len(ag_data['S']) + len(hc_data['S']) )]\n",
    "        paratope = [self.paratope] if type(self.paratope) == str else self.paratope\n",
    "        for cdr in paratope:\n",
    "            cdr_range = item.get_cdr_pos(cdr)\n",
    "            offset = len(ag_data['S']) + 1 + (0 if cdr[0] == 'H' else len(hc_data['S']))\n",
    "            for idx in range(offset + cdr_range[0], offset + cdr_range[1] + 1):\n",
    "                paratope_mask[idx] = 1\n",
    "        data['paratope_mask'] = paratope_mask\n",
    "        # print('paratope masks are ',data['cmask'], data['smask'])\n",
    "\n",
    "\n",
    "        template = ConserveTemplateGenerator().construct_template(item, align=False)\n",
    "        data['template'] = template\n",
    "\n",
    "        return data\n",
    "\n",
    "    @classmethod\n",
    "    def collate_fn(cls, batch):\n",
    "        keys = ['X', 'S', 'smask', 'cmask', 'paratope_mask', 'residue_pos', 'template', 'xloss_mask']\n",
    "        types = [torch.float, torch.long, torch.bool, torch.bool, torch.bool, torch.long, torch.float, torch.bool]\n",
    "        res = {}\n",
    "        for key, _type in zip(keys, types):\n",
    "            val = []\n",
    "            for item in batch:\n",
    "                val.append(torch.tensor(item[key], dtype=_type))\n",
    "            res[key] = torch.cat(val, dim=0)\n",
    "        lengths = [len(item['S']) for item in batch]\n",
    "        res['lengths'] = torch.tensor(lengths, dtype=torch.long)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrainConfig:\n",
    "    def __init__(self, save_dir, lr, max_epoch, warmup=0,\n",
    "                 metric_min_better=True, patience=3,\n",
    "                 grad_clip=None, save_topk=-1,  # -1 for save all\n",
    "                 **kwargs):\n",
    "        self.save_dir = save_dir\n",
    "        self.lr = lr\n",
    "        self.max_epoch = max_epoch\n",
    "        self.warmup = warmup\n",
    "        self.metric_min_better = metric_min_better\n",
    "        self.patience = patience\n",
    "        self.grad_clip = grad_clip\n",
    "        self.save_topk = save_topk\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def add_parameter(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.__class__) + ': ' + str(self.__dict__)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, valid_loader, config):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.optimizer = self.get_optimizer()\n",
    "        sched_config = self.get_scheduler(self.optimizer)\n",
    "        if sched_config is None:\n",
    "            sched_config = {\n",
    "                'scheduler': None,\n",
    "                'frequency': None\n",
    "            }\n",
    "        self.scheduler = sched_config['scheduler']\n",
    "        self.sched_freq = sched_config['frequency']\n",
    "        self.train_loader = train_loader\n",
    "        self.valid_loader = valid_loader\n",
    "\n",
    "        # distributed training\n",
    "        self.local_rank = -1\n",
    "\n",
    "        # log\n",
    "        self.version = self._get_version()\n",
    "        self.config.save_dir = os.path.join(self.config.save_dir, f'version_{self.version}')\n",
    "        self.model_dir = os.path.join(self.config.save_dir, 'checkpoint')\n",
    "        self.writer = None  # initialize right before training\n",
    "        self.writer_buffer = {}\n",
    "\n",
    "        # training process recording\n",
    "        self.global_step = 0\n",
    "        self.valid_global_step = 0\n",
    "        self.epoch = 0\n",
    "        self.last_valid_metric = None\n",
    "        self.topk_ckpt_map = []  # smaller index means better ckpt\n",
    "        self.patience = self.config.patience\n",
    "\n",
    "    @classmethod\n",
    "    def to_device(cls, data, device):\n",
    "        if isinstance(data, dict):\n",
    "            for key in data:\n",
    "                data[key] = cls.to_device(data[key], device)\n",
    "        elif isinstance(data, list) or isinstance(data, tuple):\n",
    "            res = [cls.to_device(item, device) for item in data]\n",
    "            data = type(data)(res)\n",
    "        elif hasattr(data, 'to'):\n",
    "            data = data.to(device)\n",
    "        return data\n",
    "\n",
    "    def _is_main_proc(self):\n",
    "        return self.local_rank == 0 or self.local_rank == -1\n",
    "\n",
    "    def _get_version(self):\n",
    "        version, pattern = -1, r'version_(\\d+)'\n",
    "        if os.path.exists(self.config.save_dir):\n",
    "            for fname in os.listdir(self.config.save_dir):\n",
    "                ver = re.findall(pattern, fname)\n",
    "                if len(ver):\n",
    "                    version = max(int(ver[0]), version)\n",
    "        return version + 1\n",
    "\n",
    "    def _train_epoch(self, device):\n",
    "        if self.train_loader.sampler is not None and self.local_rank != -1:  # distributed\n",
    "            self.train_loader.sampler.set_epoch(self.epoch)\n",
    "        t_iter = tqdm(self.train_loader) if self._is_main_proc() else self.train_loader\n",
    "        for batch in t_iter:\n",
    "            # print(batch,device)\n",
    "            batch = self.to_device(batch, device)\n",
    "            # print(batch,device,self.global_step)\n",
    "            loss = self.train_step(batch, self.global_step)\n",
    "            # print(loss)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if self.config.grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.grad_clip)\n",
    "            self.optimizer.step()\n",
    "            if hasattr(t_iter, 'set_postfix'):\n",
    "                t_iter.set_postfix(loss=loss.item(), version=self.version)\n",
    "            self.global_step += 1\n",
    "            if self.sched_freq == 'batch':\n",
    "                self.scheduler.step()\n",
    "        if self.sched_freq == 'epoch':\n",
    "            self.scheduler.step()\n",
    "    \n",
    "    def _valid_epoch(self, device):\n",
    "        metric_arr = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            t_iter = tqdm(self.valid_loader) if self._is_main_proc() else self.valid_loader\n",
    "            for batch in t_iter:\n",
    "                batch = self.to_device(batch, device)\n",
    "                metric = self.valid_step(batch, self.valid_global_step)\n",
    "                metric_arr.append(metric.cpu().item())\n",
    "                self.valid_global_step += 1\n",
    "        self.model.train()\n",
    "        # judge\n",
    "        valid_metric = np.mean(metric_arr)\n",
    "        if self._metric_better(valid_metric):\n",
    "            self.patience = self.config.patience\n",
    "            if self._is_main_proc():\n",
    "                save_path = os.path.join(self.model_dir, f'epoch{self.epoch}_step{self.global_step}.ckpt')\n",
    "                module_to_save = self.model.module if self.local_rank == 0 else self.model\n",
    "                torch.save(module_to_save, save_path)\n",
    "                self._maintain_topk_checkpoint(valid_metric, save_path)\n",
    "        else:\n",
    "            self.patience -= 1\n",
    "        self.last_valid_metric = valid_metric\n",
    "        # write valid_metric\n",
    "        for name in self.writer_buffer:\n",
    "            value = np.mean(self.writer_buffer[name])\n",
    "            self.log(name, value, self.epoch)\n",
    "        self.writer_buffer = {}\n",
    "    \n",
    "    def _metric_better(self, new):\n",
    "        old = self.last_valid_metric\n",
    "        if old is None:\n",
    "            return True\n",
    "        if self.config.metric_min_better:\n",
    "            return new < old\n",
    "        else:\n",
    "            return old < new\n",
    "\n",
    "    def _maintain_topk_checkpoint(self, valid_metric, ckpt_path):\n",
    "        topk = self.config.save_topk\n",
    "        if self.config.metric_min_better:\n",
    "            better = lambda a, b: a < b\n",
    "        else:\n",
    "            better = lambda a, b: a > b\n",
    "        insert_pos = len(self.topk_ckpt_map)\n",
    "        for i, (metric, _) in enumerate(self.topk_ckpt_map):\n",
    "            if better(valid_metric, metric):\n",
    "                insert_pos = i\n",
    "                break\n",
    "        self.topk_ckpt_map.insert(insert_pos, (valid_metric, ckpt_path))\n",
    "\n",
    "        # maintain topk\n",
    "        if topk > 0:\n",
    "            while len(self.topk_ckpt_map) > topk:\n",
    "                last_ckpt_path = self.topk_ckpt_map[-1][1]\n",
    "                os.remove(last_ckpt_path)\n",
    "                self.topk_ckpt_map.pop()\n",
    "\n",
    "        # save map\n",
    "        topk_map_path = os.path.join(self.model_dir, 'topk_map.txt')\n",
    "        with open(topk_map_path, 'w') as fout:\n",
    "            for metric, path in self.topk_ckpt_map:\n",
    "                fout.write(f'{metric}: {path}\\n')\n",
    "\n",
    "    def train(self, device_ids, local_rank):\n",
    "        # set local rank\n",
    "        self.local_rank = local_rank\n",
    "        # init writer\n",
    "        if self._is_main_proc():\n",
    "            self.writer = SummaryWriter(self.config.save_dir)\n",
    "            if not os.path.exists(self.model_dir):\n",
    "                os.makedirs(self.model_dir)\n",
    "            with open(os.path.join(self.config.save_dir, 'namespace.json'), 'w') as fout:\n",
    "                json.dump(self.config.__dict__, fout, indent=2)\n",
    "        # main device\n",
    "        main_device_id = local_rank if local_rank != -1 else device_ids[0]\n",
    "        device = torch.device('cpu' if main_device_id == -1 else f'cuda:{main_device_id}')\n",
    "        self.model.to(device)\n",
    "        if local_rank != -1:\n",
    "            print_log(f'Using data parallel, local rank {local_rank}, all {device_ids}')\n",
    "            self.model = torch.nn.parallel.DistributedDataParallel(\n",
    "                self.model, device_ids=[local_rank], output_device=local_rank\n",
    "            )\n",
    "        else:\n",
    "            print_log(f'training on {device_ids}')\n",
    "        for _ in range(self.config.max_epoch):\n",
    "            print_log(f'epoch{self.epoch} starts') if self._is_main_proc() else 1\n",
    "            self._train_epoch(device)\n",
    "            print_log(f'validating ...') if self._is_main_proc() else 1\n",
    "            self._valid_epoch(device)\n",
    "            self.epoch += 1\n",
    "            if self.patience <= 0:\n",
    "                break\n",
    "\n",
    "    def log(self, name, value, step, val=False):\n",
    "        if self._is_main_proc():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                value = value.cpu().item()\n",
    "            if val:\n",
    "                if name not in self.writer_buffer:\n",
    "                    self.writer_buffer[name] = []\n",
    "                self.writer_buffer[name].append(value)\n",
    "            else:\n",
    "                self.writer.add_scalar(name, value, step)\n",
    "\n",
    "    ########## Overload these functions below ##########\n",
    "    # define optimizer\n",
    "    def get_optimizer(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.lr)\n",
    "        return optimizer\n",
    "\n",
    "    # scheduler example: linear. Return None if no scheduler is needed.\n",
    "    def get_scheduler(self, optimizer):\n",
    "        lam = lambda epoch: 1 / (epoch + 1)\n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lam)\n",
    "        return {\n",
    "            'scheduler': scheduler,\n",
    "            'frequency': 'epoch' # or batch\n",
    "        }\n",
    "\n",
    "    # train step, note that batch should be dict/list/tuple/instance. Objects with .to(device) attribute will be automatically moved to the same device as the model\n",
    "    def train_step(self, batch, batch_idx):\n",
    "        print(batch, batch_idx)\n",
    "        loss = self.model(batch)\n",
    "        self.log('Loss/train', loss, batch_idx)\n",
    "        return loss\n",
    "\n",
    "    # validation step\n",
    "    def valid_step(self, batch, batch_idx):\n",
    "        loss = self.model(batch)\n",
    "        self.log('Loss/validation', loss, batch_idx, val=True)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AMEGNN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_node_nf, hidden_nf, out_node_nf, n_channel, channel_nf,\n",
    "                 radial_nf, in_edge_nf=0, act_fn=nn.SiLU(), n_layers=4,\n",
    "                 residual=True, dropout=0.1, dense=False):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        :param in_node_nf: Number of features for 'h' at the input\n",
    "        :param hidden_nf: Number of hidden features\n",
    "        :param out_node_nf: Number of features for 'h' at the output\n",
    "        :param n_channel: Number of channels of coordinates\n",
    "        :param in_edge_nf: Number of features for the edge features\n",
    "        :param act_fn: Non-linearity\n",
    "        :param n_layers: Number of layer for the EGNN\n",
    "        :param residual: Use residual connections, we recommend not changing this one\n",
    "        :param dropout: probability of dropout\n",
    "        :param dense: if dense, then context states will be concatenated for all layers,\n",
    "                      coordination will be averaged\n",
    "        '''\n",
    "        self.hidden_nf = hidden_nf\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear_in = nn.Linear(in_node_nf, self.hidden_nf)\n",
    "\n",
    "        self.dense = dense\n",
    "        if dense:\n",
    "            self.linear_out = nn.Linear(self.hidden_nf * (n_layers + 1), out_node_nf)\n",
    "        else:\n",
    "            self.linear_out = nn.Linear(self.hidden_nf, out_node_nf)\n",
    "\n",
    "        for i in range(0, n_layers):\n",
    "            self.add_module(f'gcl_{i}', AM_E_GCL(\n",
    "                self.hidden_nf, self.hidden_nf, self.hidden_nf, n_channel, channel_nf, radial_nf,\n",
    "                edges_in_d=in_edge_nf, act_fn=act_fn, residual=residual, dropout=dropout\n",
    "            ))\n",
    "        self.out_layer = AM_E_GCL(\n",
    "            self.hidden_nf, self.hidden_nf, self.hidden_nf, n_channel, channel_nf,\n",
    "            radial_nf, edges_in_d=in_edge_nf, act_fn=act_fn, residual=residual\n",
    "        )\n",
    "    \n",
    "    def forward(self, h, x, edges, channel_attr, channel_weights, ctx_edge_attr=None):\n",
    "        h = self.linear_in(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        ctx_states, ctx_coords = [], []\n",
    "        for i in range(0, self.n_layers):\n",
    "            h, x = self._modules[f'gcl_{i}'](\n",
    "                h, edges, x, channel_attr, channel_weights,\n",
    "                edge_attr=ctx_edge_attr)\n",
    "            ctx_states.append(h)\n",
    "            ctx_coords.append(x)\n",
    "\n",
    "        h, x = self.out_layer(\n",
    "            h, edges, x, channel_attr, channel_weights,\n",
    "            edge_attr=ctx_edge_attr)\n",
    "        ctx_states.append(h)\n",
    "        ctx_coords.append(x)\n",
    "        if self.dense:\n",
    "            h = torch.cat(ctx_states, dim=-1)\n",
    "            x = torch.mean(torch.stack(ctx_coords), dim=0)\n",
    "        h = self.dropout(h)\n",
    "        h = self.linear_out(h)\n",
    "        return h, x\n",
    "\n",
    "'''\n",
    "Below are the implementation of the adaptive multi-channel message passing mechanism\n",
    "'''\n",
    "\n",
    "@singleton\n",
    "class RollerPooling(nn.Module):\n",
    "    '''\n",
    "    Adaptive average pooling for the adaptive scaler\n",
    "    '''\n",
    "    def __init__(self, n_channel) -> None:\n",
    "        super().__init__()\n",
    "        self.n_channel = n_channel\n",
    "        with torch.no_grad():\n",
    "            pool_matrix = []\n",
    "            ones = torch.ones((n_channel, n_channel), dtype=torch.float)\n",
    "            for i in range(n_channel):\n",
    "                # i start from 0 instead of 1 !!! (less readable but higher implemetation efficiency)\n",
    "                window_size = n_channel - i\n",
    "                mat = torch.triu(ones) - torch.triu(ones, diagonal=window_size)\n",
    "                pool_matrix.append(mat / window_size)\n",
    "            self.pool_matrix = torch.stack(pool_matrix)\n",
    "    \n",
    "    def forward(self, hidden, target_size):\n",
    "        '''\n",
    "        :param hidden: [n_edges, n_channel]\n",
    "        :param target_size: [n_edges]\n",
    "        '''\n",
    "        pool_mat = self.pool_matrix.to(hidden.device).type(hidden.dtype)\n",
    "        pool_mat = pool_mat[target_size - 1]  # [n_edges, n_channel, n_channel]\n",
    "        hidden = hidden.unsqueeze(-1)  # [n_edges, n_channel, 1]\n",
    "        return torch.bmm(pool_mat, hidden)  # [n_edges, n_channel, 1]\n",
    "\n",
    "\n",
    "class AM_E_GCL(nn.Module):\n",
    "    '''\n",
    "    Adaptive Multi-Channel E(n) Equivariant Convolutional Layer\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_nf, output_nf, hidden_nf, n_channel, channel_nf, radial_nf,\n",
    "                 edges_in_d=0, node_attr_d=0, act_fn=nn.SiLU(), residual=True, attention=False,\n",
    "                 normalize=False, coords_agg='mean', tanh=False, dropout=0.1):\n",
    "        super(AM_E_GCL, self).__init__()\n",
    "\n",
    "        input_edge = input_nf * 2\n",
    "        self.residual = residual\n",
    "        self.attention = attention\n",
    "        self.normalize = normalize\n",
    "        self.coords_agg = coords_agg\n",
    "        self.tanh = tanh\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        input_edge = input_nf * 2\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(input_edge + radial_nf + edges_in_d, hidden_nf),\n",
    "            act_fn,\n",
    "            nn.Linear(hidden_nf, hidden_nf),\n",
    "            act_fn)\n",
    "        self.radial_linear = nn.Linear(channel_nf ** 2, radial_nf)\n",
    "\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_nf + input_nf + node_attr_d, hidden_nf),\n",
    "            act_fn,\n",
    "            nn.Linear(hidden_nf, output_nf))\n",
    "\n",
    "        layer = nn.Linear(hidden_nf, n_channel, bias=False)\n",
    "        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n",
    "\n",
    "        coord_mlp = []\n",
    "        coord_mlp.append(nn.Linear(hidden_nf, hidden_nf))\n",
    "        coord_mlp.append(act_fn)\n",
    "        coord_mlp.append(layer)\n",
    "        if self.tanh:\n",
    "            coord_mlp.append(nn.Tanh())\n",
    "        self.coord_mlp = nn.Sequential(*coord_mlp)\n",
    "\n",
    "        if self.attention:\n",
    "            self.att_mlp = nn.Sequential(\n",
    "                nn.Linear(hidden_nf, 1),\n",
    "                nn.Sigmoid())\n",
    "\n",
    "    def edge_model(self, source, target, radial, edge_attr):\n",
    "        '''\n",
    "        :param source: [n_edge, input_size]\n",
    "        :param target: [n_edge, input_size]\n",
    "        :param radial: [n_edge, d, d]\n",
    "        :param edge_attr: [n_edge, edge_dim]\n",
    "        '''\n",
    "        radial = radial.reshape(radial.shape[0], -1)  # [n_edge, d ^ 2]\n",
    "\n",
    "        if edge_attr is None:  # Unused.\n",
    "            out = torch.cat([source, target, radial], dim=1)\n",
    "        else:\n",
    "            out = torch.cat([source, target, radial, edge_attr], dim=1)\n",
    "        out = self.edge_mlp(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        if self.attention:\n",
    "            att_val = self.att_mlp(out)\n",
    "            out = out * att_val\n",
    "        return out\n",
    "\n",
    "    def node_model(self, x, edge_index, edge_attr, node_attr):\n",
    "        '''\n",
    "        :param x: [bs * n_node, input_size]\n",
    "        :param edge_index: list of [n_edge], [n_edge]\n",
    "        :param edge_attr: [n_edge, hidden_size], refers to message from i to j\n",
    "        :param node_attr: [bs * n_node, node_dim]\n",
    "        '''\n",
    "        row, col = edge_index\n",
    "        agg = unsorted_segment_sum(edge_attr, row, num_segments=x.size(0))  # [bs * n_node, hidden_size]\n",
    "        # print_log(f'agg1, {torch.isnan(agg).sum()}', level='DEBUG')\n",
    "        if node_attr is not None:\n",
    "            agg = torch.cat([x, agg, node_attr], dim=1)\n",
    "        else:\n",
    "            agg = torch.cat([x, agg], dim=1)  # [bs * n_node, input_size + hidden_size]\n",
    "        # print_log(f'agg, {torch.isnan(agg).sum()}', level='DEBUG')\n",
    "        out = self.node_mlp(agg)  # [bs * n_node, output_size]\n",
    "        # print_log(f'out, {torch.isnan(out).sum()}', level='DEBUG')\n",
    "        out = self.dropout(out)\n",
    "        if self.residual:\n",
    "            out = x + out\n",
    "        return out, agg\n",
    "\n",
    "    def coord_model(self, coord, edge_index, coord_diff, edge_feat, channel_weights):\n",
    "        '''\n",
    "        coord: [bs * n_node, n_channel, d]\n",
    "        edge_index: list of [n_edge], [n_edge]\n",
    "        coord_diff: [n_edge, n_channel, d]\n",
    "        edge_feat: [n_edge, hidden_size]\n",
    "        channel_weights: [N, n_channel]\n",
    "        '''\n",
    "        row, col = edge_index\n",
    "\n",
    "        # first pooling, then element-wise multiply\n",
    "        n_channel = channel_weights.shape[-1]\n",
    "        edge_feat = self.coord_mlp(edge_feat)  # [n_edge, n_channel]\n",
    "        channel_sum = (channel_weights != 0).long().sum(-1)  # [N]\n",
    "        pooled_edge_feat = RollerPooling(n_channel)(edge_feat, channel_sum[row])  # [n_edge, n_channel, 1]\n",
    "        trans = coord_diff * pooled_edge_feat  # [n_edge, n_channel, d]\n",
    "\n",
    "        # aggregate\n",
    "        if self.coords_agg == 'sum':\n",
    "            agg = unsorted_segment_sum(trans, row, num_segments=coord.size(0))\n",
    "        elif self.coords_agg == 'mean':\n",
    "            agg = unsorted_segment_mean(trans, row, num_segments=coord.size(0))  # [bs * n_node, n_channel, d]\n",
    "        else:\n",
    "            raise Exception('Wrong coords_agg parameter' % self.coords_agg)\n",
    "        coord = coord + agg\n",
    "        return coord\n",
    "\n",
    "    def forward(self, h, edge_index, coord, channel_attr, channel_weights,\n",
    "                edge_attr=None, node_attr=None):\n",
    "        '''\n",
    "        h: [bs * n_node, hidden_size]\n",
    "        edge_index: list of [n_row] and [n_col] where n_row == n_col (with no cutoff, n_row == bs * n_node * (n_node - 1))\n",
    "        coord: [bs * n_node, n_channel, d]\n",
    "        channel_attr: [bs * n_node, n_channel, channel_nf]\n",
    "        channel_weights: [bs * n_node, n_channel]\n",
    "        '''\n",
    "        row, col = edge_index\n",
    "\n",
    "        radial, coord_diff = coord2radial(edge_index, coord, channel_attr, channel_weights, self.radial_linear)\n",
    "\n",
    "        edge_feat = self.edge_model(h[row], h[col], radial, edge_attr)  # [n_edge, hidden_size]\n",
    "        coord = self.coord_model(coord, edge_index, coord_diff, edge_feat, channel_weights)    # [bs * n_node, n_channel, d]\n",
    "        h, agg = self.node_model(h, edge_index, edge_feat, node_attr)\n",
    "\n",
    "        return h, coord\n",
    "\n",
    "\n",
    "def unsorted_segment_sum(data, segment_ids, num_segments):\n",
    "    '''\n",
    "    :param data: [n_edge, *dimensions]\n",
    "    :param segment_ids: [n_edge]\n",
    "    :param num_segments: [bs * n_node]\n",
    "    '''\n",
    "    expand_dims = tuple(data.shape[1:])\n",
    "    result_shape = (num_segments, ) + expand_dims\n",
    "    for _ in expand_dims:\n",
    "        segment_ids = segment_ids.unsqueeze(-1)\n",
    "    segment_ids = segment_ids.expand(-1, *expand_dims)\n",
    "    result = data.new_full(result_shape, 0)  # Init empty result tensor.\n",
    "    result.scatter_add_(0, segment_ids, data)\n",
    "    return result\n",
    "\n",
    "\n",
    "def unsorted_segment_mean(data, segment_ids, num_segments):\n",
    "    '''\n",
    "    :param data: [n_edge, *dimensions]\n",
    "    :param segment_ids: [n_edge]\n",
    "    :param num_segments: [bs * n_node]\n",
    "    '''\n",
    "    expand_dims = tuple(data.shape[1:])\n",
    "    result_shape = (num_segments, ) + expand_dims\n",
    "    for _ in expand_dims:\n",
    "        segment_ids = segment_ids.unsqueeze(-1)\n",
    "    segment_ids = segment_ids.expand(-1, *expand_dims)\n",
    "    result = data.new_full(result_shape, 0)  # Init empty result tensor.\n",
    "    count = data.new_full(result_shape, 0)\n",
    "    result.scatter_add_(0, segment_ids, data)\n",
    "    count.scatter_add_(0, segment_ids, torch.ones_like(data))\n",
    "    return result / count.clamp(min=1)\n",
    "\n",
    "\n",
    "CONSTANT = 1\n",
    "NUM_SEG = 1  # if you do not have enough memory or you have large attr_size, increase this parameter\n",
    "\n",
    "def coord2radial(edge_index, coord, attr, channel_weights, linear_map):\n",
    "    '''\n",
    "    :param edge_index: tuple([n_edge], [n_edge]) which is tuple of (row, col)\n",
    "    :param coord: [N, n_channel, d]\n",
    "    :param attr: [N, n_channel, attr_size], attribute embedding of each channel\n",
    "    :param channel_weights: [N, n_channel], weights of different channels\n",
    "    :param linear_map: nn.Linear, map features to d_out\n",
    "    :param num_seg: split row/col into segments to reduce memory cost\n",
    "    '''\n",
    "    row, col = edge_index\n",
    "    \n",
    "    radials = []\n",
    "\n",
    "    seg_size = (len(row) + NUM_SEG - 1) // NUM_SEG\n",
    "\n",
    "    for i in range(NUM_SEG):\n",
    "        start = i * seg_size\n",
    "        end = min(start + seg_size, len(row))\n",
    "        if end <= start:\n",
    "            break\n",
    "        seg_row, seg_col = row[start:end], col[start:end]\n",
    "\n",
    "        coord_msg = torch.norm(\n",
    "            coord[seg_row].unsqueeze(2) - coord[seg_col].unsqueeze(1),  # [n_edge, n_channel, n_channel, d]\n",
    "            dim=-1, keepdim=False)  # [n_edge, n_channel, n_channel]\n",
    "        \n",
    "        coord_msg = coord_msg * torch.bmm(\n",
    "            channel_weights[seg_row].unsqueeze(2),\n",
    "            channel_weights[seg_col].unsqueeze(1)\n",
    "            )  # [n_edge, n_channel, n_channel]\n",
    "        \n",
    "        radial = torch.bmm(\n",
    "            attr[seg_row].transpose(-1, -2),  # [n_edge, attr_size, n_channel]\n",
    "            coord_msg)  # [n_edge, attr_size, n_channel]\n",
    "        radial = torch.bmm(radial, attr[seg_col])  # [n_edge, attr_size, attr_size]\n",
    "        radial = radial.reshape(radial.shape[0], -1)  # [n_edge, attr_size * attr_size]\n",
    "        radial_norm = torch.norm(radial, dim=-1, keepdim=True) + CONSTANT  # post norm\n",
    "        radial = linear_map(radial) / radial_norm # [n_edge, d_out]\n",
    "\n",
    "        radials.append(radial)\n",
    "    \n",
    "    radials = torch.cat(radials, dim=0)  # [N_edge, d_out]\n",
    "\n",
    "    # generate coord_diff by first mean src then minused by dst\n",
    "    # message passed from col to row\n",
    "    channel_mask = (channel_weights != 0).long()  # [N, n_channel]\n",
    "    channel_sum = channel_mask.sum(-1)  # [N]\n",
    "    pooled_col_coord = (coord[col] * channel_mask[col].unsqueeze(-1)).sum(1)  # [n_edge, d]\n",
    "    pooled_col_coord = pooled_col_coord / channel_sum[col].unsqueeze(-1)  # [n_edge, d], denominator cannot be 0 since no pad node exists\n",
    "    coord_diff = coord[row] - pooled_col_coord.unsqueeze(1)  # [n_edge, n_channel, d]\n",
    "\n",
    "    return radials, coord_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AMEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, in_node_nf, hidden_nf, out_node_nf, n_channel, channel_nf,\n",
    "                 radial_nf, in_edge_nf=0, act_fn=nn.SiLU(), n_layers=4,\n",
    "                 residual=True, dropout=0.1, dense=False):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        :param in_node_nf: Number of features for 'h' at the input\n",
    "        :param hidden_nf: Number of hidden features\n",
    "        :param out_node_nf: Number of features for 'h' at the output\n",
    "        :param n_channel: Number of channels of coordinates\n",
    "        :param in_edge_nf: Number of features for the edge features\n",
    "        :param act_fn: Non-linearity\n",
    "        :param n_layers: Number of layer for the EGNN\n",
    "        :param residual: Use residual connections, we recommend not changing this one\n",
    "        :param dropout: probability of dropout\n",
    "        :param dense: if dense, then context states will be concatenated for all layers,\n",
    "                      coordination will be averaged\n",
    "        '''\n",
    "        self.hidden_nf = hidden_nf\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear_in = nn.Linear(in_node_nf, self.hidden_nf)\n",
    "\n",
    "        self.dense = dense\n",
    "        if dense:\n",
    "            self.linear_out = nn.Linear(self.hidden_nf * (n_layers + 1), out_node_nf)\n",
    "        else:\n",
    "            self.linear_out = nn.Linear(self.hidden_nf, out_node_nf)\n",
    "\n",
    "        for i in range(0, n_layers):\n",
    "            self.add_module(f'ctx_gcl_{i}', AM_E_GCL(\n",
    "                self.hidden_nf, self.hidden_nf, self.hidden_nf, n_channel, channel_nf, radial_nf,\n",
    "                edges_in_d=in_edge_nf, act_fn=act_fn, residual=residual, dropout=dropout\n",
    "            ))\n",
    "            self.add_module(f'inter_gcl_{i}', AM_E_GCL(\n",
    "                self.hidden_nf, self.hidden_nf, self.hidden_nf, n_channel, channel_nf, radial_nf,\n",
    "                edges_in_d=in_edge_nf, act_fn=act_fn, residual=residual, dropout=dropout\n",
    "            ))\n",
    "        self.out_layer = AM_E_GCL(\n",
    "            self.hidden_nf, self.hidden_nf, self.hidden_nf, n_channel, channel_nf,\n",
    "            radial_nf, edges_in_d=in_edge_nf, act_fn=act_fn, residual=residual\n",
    "        )\n",
    "    \n",
    "    def forward(self, h, x, ctx_edges, inter_mask, inter_x, inter_edges, update_mask, inter_update_mask, channel_attr, channel_weights,\n",
    "                ctx_edge_attr=None):\n",
    "        h = self.linear_in(h)\n",
    "        h = self.dropout(h)\n",
    "        inter_h = h[inter_mask]\n",
    "        inter_channel_attr = channel_attr[inter_mask]\n",
    "        inter_channel_weights = channel_weights[inter_mask]\n",
    "\n",
    "        ctx_states, ctx_coords, inter_coords = [], [], []\n",
    "        for i in range(0, self.n_layers):\n",
    "            h, x = self._modules[f'ctx_gcl_{i}'](\n",
    "                h, ctx_edges, x, channel_attr, channel_weights,\n",
    "                edge_attr=ctx_edge_attr)\n",
    "            # synchronization of the shadow paratope (native -> shadow)\n",
    "            inter_h = inter_h.clone()\n",
    "            inter_h[inter_update_mask] = h[update_mask]\n",
    "            inter_h, inter_x = self._modules[f'inter_gcl_{i}'](\n",
    "                inter_h, inter_edges, inter_x, inter_channel_attr, inter_channel_weights\n",
    "            )\n",
    "            # synchronization of the shadow paratope (shadow -> native)\n",
    "            h = h.clone()\n",
    "            h[inter_mask] = inter_h\n",
    "            ctx_states.append(h)\n",
    "            ctx_coords.append(x)\n",
    "            inter_coords.append(inter_x)\n",
    "\n",
    "        h, x = self.out_layer(\n",
    "            h, ctx_edges, x, channel_attr, channel_weights,\n",
    "            edge_attr=ctx_edge_attr)\n",
    "        ctx_states.append(h)\n",
    "        ctx_coords.append(x)\n",
    "        if self.dense:\n",
    "            h = torch.cat(ctx_states, dim=-1)\n",
    "            x = torch.mean(torch.stack(ctx_coords), dim=0)\n",
    "            inter_x = torch.mean(torch.stack(inter_coords), dim=0)\n",
    "        h = self.dropout(h)\n",
    "        h = self.linear_out(h)\n",
    "        return h, x, inter_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sequential_and(*tensors):\n",
    "    res = tensors[0]\n",
    "    for mat in tensors[1:]:\n",
    "        res = torch.logical_and(res, mat)\n",
    "    return res\n",
    "\n",
    "\n",
    "def sequential_or(*tensors):\n",
    "    res = tensors[0]\n",
    "    for mat in tensors[1:]:\n",
    "        res = torch.logical_or(res, mat)\n",
    "    return res\n",
    "\n",
    "\n",
    "def graph_to_batch(tensor, batch_id, padding_value=0, mask_is_pad=True):\n",
    "    '''\n",
    "    :param tensor: [N, D1, D2, ...]\n",
    "    :param batch_id: [N]\n",
    "    :param mask_is_pad: 1 in the mask indicates padding if set to True\n",
    "    '''\n",
    "    lengths = scatter_sum(torch.ones_like(batch_id), batch_id)  # [bs]\n",
    "    bs, max_n = lengths.shape[0], torch.max(lengths)\n",
    "    batch = torch.ones((bs, max_n, *tensor.shape[1:]), dtype=tensor.dtype, device=tensor.device) * padding_value\n",
    "    # generate pad mask: 1 for pad and 0 for data\n",
    "    pad_mask = torch.zeros((bs, max_n + 1), dtype=torch.long, device=tensor.device)\n",
    "    pad_mask[(torch.arange(bs, device=tensor.device), lengths)] = 1\n",
    "    pad_mask = (torch.cumsum(pad_mask, dim=-1)[:, :-1]).bool()\n",
    "    data_mask = torch.logical_not(pad_mask)\n",
    "    # fill data\n",
    "    batch[data_mask] = tensor\n",
    "    mask = pad_mask if mask_is_pad else data_mask\n",
    "    return batch, mask\n",
    "\n",
    "\n",
    "def _knn_edges(X, AP, src_dst, atom_pos_pad_idx, k_neighbors, batch_info, given_dist=None):\n",
    "    '''\n",
    "    :param X: [N, n_channel, 3], coordinates\n",
    "    :param AP: [N, n_channel], atom position with pad type need to be ignored\n",
    "    :param src_dst: [Ef, 2], full possible edges represented in (src, dst)\n",
    "    :param given_dist: [Ef], given distance of edges\n",
    "    '''\n",
    "    offsets, batch_id, max_n, gni2lni = batch_info\n",
    "\n",
    "    BIGINT = 1e10  # assign a large distance to invalid edges\n",
    "    N = X.shape[0]\n",
    "    if given_dist is None:\n",
    "        dist = X[src_dst]  # [Ef, 2, n_channel, 3]\n",
    "        dist = dist[:, 0].unsqueeze(2) - dist[:, 1].unsqueeze(1)  # [Ef, n_channel, n_channel, 3]\n",
    "        dist = torch.norm(dist, dim=-1)  # [Ef, n_channel, n_channel]\n",
    "        pos_pad = AP[src_dst] == atom_pos_pad_idx # [Ef, 2, n_channel]\n",
    "        pos_pad = torch.logical_or(pos_pad[:, 0].unsqueeze(2), pos_pad[:, 1].unsqueeze(1))  # [Ef, n_channel, n_channel]\n",
    "        dist = dist + pos_pad * BIGINT  # [Ef, n_channel, n_channel]\n",
    "        del pos_pad  # release memory\n",
    "        dist = torch.min(dist.reshape(dist.shape[0], -1), dim=1)[0]  # [Ef]\n",
    "    else:\n",
    "        dist = given_dist\n",
    "    src_dst = src_dst.transpose(0, 1)  # [2, Ef]\n",
    "\n",
    "    dist_mat = torch.ones(N, max_n, device=dist.device, dtype=dist.dtype) * BIGINT  # [N, max_n]\n",
    "    dist_mat[(src_dst[0], gni2lni[src_dst[1]])] = dist\n",
    "    del dist\n",
    "    dist_neighbors, dst = torch.topk(dist_mat, k_neighbors, dim=-1, largest=False)  # [N, topk]\n",
    "\n",
    "    src = torch.arange(0, N, device=dst.device).unsqueeze(-1).repeat(1, k_neighbors)\n",
    "    src, dst = src.flatten(), dst.flatten()\n",
    "    dist_neighbors = dist_neighbors.flatten()\n",
    "    is_valid = dist_neighbors < BIGINT\n",
    "    src = src.masked_select(is_valid)\n",
    "    dst = dst.masked_select(is_valid)\n",
    "\n",
    "    dst = dst + offsets[batch_id[src]]  # mapping from local to global node index\n",
    "\n",
    "    edges = torch.stack([src, dst])  # message passed from dst to src\n",
    "    return edges  # [2, E]\n",
    "\n",
    "\n",
    "class EdgeConstructor:\n",
    "    def __init__(self, boa_idx, boh_idx, bol_idx, atom_pos_pad_idx, ag_seg_id) -> None:\n",
    "        self.boa_idx, self.boh_idx, self.bol_idx = boa_idx, boh_idx, bol_idx\n",
    "        self.atom_pos_pad_idx = atom_pos_pad_idx\n",
    "        self.ag_seg_id = ag_seg_id\n",
    "\n",
    "        # buffer\n",
    "        self._reset_buffer()\n",
    "\n",
    "    def _reset_buffer(self):\n",
    "        self.row = None\n",
    "        self.col = None\n",
    "        self.row_global = None\n",
    "        self.col_global = None\n",
    "        self.row_seg = None\n",
    "        self.col_seg = None\n",
    "        self.offsets = None\n",
    "        self.max_n = None\n",
    "        self.gni2lni = None\n",
    "        self.not_global_edges = None\n",
    "\n",
    "    def get_batch_edges(self, batch_id):\n",
    "        # construct tensors to map between global / local node index\n",
    "        lengths = scatter_sum(torch.ones_like(batch_id), batch_id)  # [bs]\n",
    "        N, max_n = batch_id.shape[0], torch.max(lengths)\n",
    "        offsets = F.pad(torch.cumsum(lengths, dim=0)[:-1], pad=(1, 0), value=0)  # [bs]\n",
    "        # global node index to local index. lni2gni can be implemented as lni + offsets[batch_id]\n",
    "        gni = torch.arange(N, device=batch_id.device)\n",
    "        gni2lni = gni - offsets[batch_id]  # [N]\n",
    "\n",
    "        # all possible edges (within the same graph)\n",
    "        # same bid (get rid of self-loop and none edges)\n",
    "        same_bid = torch.zeros(N, max_n, device=batch_id.device)\n",
    "        same_bid[(gni, lengths[batch_id] - 1)] = 1\n",
    "        same_bid = 1 - torch.cumsum(same_bid, dim=-1)\n",
    "        # shift right and pad 1 to the left\n",
    "        same_bid = F.pad(same_bid[:, :-1], pad=(1, 0), value=1)\n",
    "        same_bid[(gni, gni2lni)] = 0  # delete self loop\n",
    "        row, col = torch.nonzero(same_bid).T  # [2, n_edge_all]\n",
    "        col = col + offsets[batch_id[row]]  # mapping from local to global node index\n",
    "        return (row, col), (offsets, max_n, gni2lni)\n",
    "\n",
    "    def _prepare(self, S, batch_id, segment_ids) -> None:\n",
    "        (row, col), (offsets, max_n, gni2lni) = self.get_batch_edges(batch_id)\n",
    "\n",
    "        # not global edges\n",
    "        is_global = sequential_or(S == self.boa_idx, S == self.boh_idx, S == self.bol_idx) # [N]\n",
    "        row_global, col_global = is_global[row], is_global[col]\n",
    "        not_global_edges = torch.logical_not(torch.logical_or(row_global, col_global))\n",
    "        \n",
    "        # segment ids\n",
    "        row_seg, col_seg = segment_ids[row], segment_ids[col]\n",
    "\n",
    "        # add to buffer\n",
    "        self.row, self.col = row, col\n",
    "        self.offsets, self.max_n, self.gni2lni = offsets, max_n, gni2lni\n",
    "        self.row_global, self.col_global = row_global, col_global\n",
    "        self.not_global_edges = not_global_edges\n",
    "        self.row_seg, self.col_seg = row_seg, col_seg\n",
    "\n",
    "    def _construct_inner_edges(self, X, batch_id, k_neighbors, atom_pos):\n",
    "        row, col = self.row, self.col\n",
    "        # all possible ctx edges: same seg, not global\n",
    "        select_edges = torch.logical_and(self.row_seg == self.col_seg, self.not_global_edges)\n",
    "        ctx_all_row, ctx_all_col = row[select_edges], col[select_edges]\n",
    "        # ctx edges\n",
    "        inner_edges = _knn_edges(\n",
    "            X, atom_pos, torch.stack([ctx_all_row, ctx_all_col]).T,\n",
    "            self.atom_pos_pad_idx, k_neighbors,\n",
    "            (self.offsets, batch_id, self.max_n, self.gni2lni))\n",
    "        return inner_edges\n",
    "\n",
    "    def _construct_outer_edges(self, X, batch_id, k_neighbors, atom_pos):\n",
    "        row, col = self.row, self.col\n",
    "        # all possible inter edges: not same seg, not global\n",
    "        select_edges = torch.logical_and(self.row_seg != self.col_seg, self.not_global_edges)\n",
    "        inter_all_row, inter_all_col = row[select_edges], col[select_edges]\n",
    "        outer_edges = _knn_edges(\n",
    "            X, atom_pos, torch.stack([inter_all_row, inter_all_col]).T,\n",
    "            self.atom_pos_pad_idx, k_neighbors,\n",
    "            (self.offsets, batch_id, self.max_n, self.gni2lni))\n",
    "        return outer_edges\n",
    "\n",
    "    def _construct_global_edges(self):\n",
    "        row, col = self.row, self.col\n",
    "        # edges between global and normal nodes\n",
    "        select_edges = torch.logical_and(self.row_seg == self.col_seg, torch.logical_not(self.not_global_edges))\n",
    "        global_normal = torch.stack([row[select_edges], col[select_edges]])  # [2, nE]\n",
    "        # edges between global and global nodes\n",
    "        select_edges = torch.logical_and(self.row_global, self.col_global) # self-loop has been deleted\n",
    "        global_global = torch.stack([row[select_edges], col[select_edges]])  # [2, nE]\n",
    "        return global_normal, global_global\n",
    "\n",
    "    def _construct_seq_edges(self):\n",
    "        row, col = self.row, self.col\n",
    "        # add additional edge to neighbors in 1D sequence (except epitope)\n",
    "        select_edges = sequential_and(\n",
    "            torch.logical_or((row - col) == 1, (row - col) == -1),  # adjacent in the graph\n",
    "            self.not_global_edges,  # not global edges (also ensure the edges are in the same segment)\n",
    "            self.row_seg != self.ag_seg_id  # not epitope\n",
    "        )\n",
    "        seq_adj = torch.stack([row[select_edges], col[select_edges]])  # [2, nE]\n",
    "        return seq_adj\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def construct_edges(self, X, S, batch_id, k_neighbors, atom_pos, segment_ids):\n",
    "        '''\n",
    "        Memory efficient with complexity of O(Nn) where n is the largest number of nodes in the batch\n",
    "        '''\n",
    "        # prepare inputs\n",
    "        self._prepare(S, batch_id, segment_ids)\n",
    "\n",
    "        ctx_edges, inter_edges = [], []\n",
    "\n",
    "        # edges within chains\n",
    "        inner_edges = self._construct_inner_edges(X, batch_id, k_neighbors, atom_pos)\n",
    "        # edges between global nodes and normal/global nodes\n",
    "        global_normal, global_global = self._construct_global_edges()\n",
    "        # edges on the 1D sequence\n",
    "        seq_edges = self._construct_seq_edges()\n",
    "\n",
    "        # construct context edges\n",
    "        ctx_edges = torch.cat([inner_edges, global_normal, global_global, seq_edges], dim=1)  # [2, E]\n",
    "\n",
    "        # construct interaction edges\n",
    "        inter_edges = self._construct_outer_edges(X, batch_id, k_neighbors, atom_pos)\n",
    "\n",
    "        self._reset_buffer()\n",
    "        return ctx_edges, inter_edges\n",
    "\n",
    "\n",
    "class GMEdgeConstructor(EdgeConstructor):\n",
    "    '''\n",
    "    Edge constructor for graph matching (kNN internel edges and all bipartite edges)\n",
    "    '''\n",
    "    def _construct_inner_edges(self, X, batch_id, k_neighbors, atom_pos):\n",
    "        row, col = self.row, self.col\n",
    "        # all possible ctx edges: both in ag or ab, not global\n",
    "        row_is_ag = self.row_seg == self.ag_seg_id\n",
    "        col_is_ag = self.col_seg == self.ag_seg_id\n",
    "        select_edges = torch.logical_and(row_is_ag == col_is_ag, self.not_global_edges)\n",
    "        ctx_all_row, ctx_all_col = row[select_edges], col[select_edges]\n",
    "        # ctx edges\n",
    "        inner_edges = _knn_edges(\n",
    "            X, atom_pos, torch.stack([ctx_all_row, ctx_all_col]).T,\n",
    "            self.atom_pos_pad_idx, k_neighbors,\n",
    "            (self.offsets, batch_id, self.max_n, self.gni2lni))\n",
    "        return inner_edges\n",
    "\n",
    "    def _construct_global_edges(self):\n",
    "        row, col = self.row, self.col\n",
    "        # edges between global and normal nodes\n",
    "        select_edges = torch.logical_and(self.row_seg == self.col_seg, torch.logical_not(self.not_global_edges))\n",
    "        global_normal = torch.stack([row[select_edges], col[select_edges]])  # [2, nE]\n",
    "        # edges between global and global nodes\n",
    "        row_is_ag = self.row_seg == self.ag_seg_id\n",
    "        col_is_ag = self.col_seg == self.ag_seg_id\n",
    "        select_edges = sequential_and(\n",
    "            self.row_global, self.col_global, # self-loop has been deleted\n",
    "            row_is_ag == col_is_ag)  # only inter-ag or inter-ab globals\n",
    "        global_global = torch.stack([row[select_edges], col[select_edges]])  # [2, nE]\n",
    "        return global_normal, global_global\n",
    "\n",
    "    def _construct_outer_edges(self, X, batch_id, k_neighbors, atom_pos):\n",
    "        row, col = self.row, self.col\n",
    "        # all possible inter edges: one in ag and one in ab, not global\n",
    "        row_is_ag = self.row_seg == self.ag_seg_id\n",
    "        col_is_ag = self.col_seg == self.ag_seg_id\n",
    "        select_edges = torch.logical_and(row_is_ag != col_is_ag, self.not_global_edges)\n",
    "        inter_all_row, inter_all_col = row[select_edges], col[select_edges]\n",
    "        return torch.stack([inter_all_row, inter_all_col])  # [2, E]\n",
    "\n",
    "\n",
    "class SinusoidalPositionEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sin-Cos Positional Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim):\n",
    "        super(SinusoidalPositionEmbedding, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, position_ids):\n",
    "        device = position_ids.device\n",
    "        position_ids = position_ids[None] # [1, N]\n",
    "        indices = torch.arange(self.output_dim // 2, device=device, dtype=torch.float)\n",
    "        indices = torch.pow(10000.0, -2 * indices / self.output_dim)\n",
    "        embeddings = torch.einsum('bn,d->bnd', position_ids, indices)\n",
    "        embeddings = torch.stack([torch.sin(embeddings), torch.cos(embeddings)], dim=-1)\n",
    "        embeddings = embeddings.reshape(-1, self.output_dim)\n",
    "        return embeddings\n",
    "\n",
    "# embedding of amino acids. (default: concat residue embedding and atom embedding to one vector)\n",
    "class AminoAcidEmbedding(nn.Module):\n",
    "    '''\n",
    "    [residue embedding + position embedding, mean(atom embeddings + atom position embeddings)]\n",
    "    '''\n",
    "    def __init__(self, num_res_type, num_atom_type, num_atom_pos, res_embed_size, atom_embed_size,\n",
    "                 atom_pad_id=VOCAB.get_atom_pad_idx(), relative_position=True, max_position=192):  # max position (with IMGT numbering)\n",
    "        super().__init__()\n",
    "        self.residue_embedding = nn.Embedding(num_res_type, res_embed_size)\n",
    "        if relative_position:\n",
    "            self.res_pos_embedding = SinusoidalPositionEmbedding(res_embed_size)  # relative positional encoding\n",
    "        else:\n",
    "            self.res_pos_embedding = nn.Embedding(max_position, res_embed_size)  # absolute position encoding\n",
    "        self.atom_embedding = nn.Embedding(num_atom_type, atom_embed_size)\n",
    "        self.atom_pos_embedding = nn.Embedding(num_atom_pos, atom_embed_size)\n",
    "        self.atom_pad_id = atom_pad_id\n",
    "        self.eps = 1e-10  # for mean of atom embedding (some residues have no atom at all)\n",
    "    \n",
    "    def forward(self, S, RP, A, AP):\n",
    "        '''\n",
    "        :param S: [N], residue types\n",
    "        :param RP: [N], residue positions\n",
    "        :param A: [N, n_channel], atom types\n",
    "        :param AP: [N, n_channel], atom positions\n",
    "        '''\n",
    "        res_embed = self.residue_embedding(S) + self.res_pos_embedding(RP)  # [N, res_embed_size]\n",
    "        atom_embed = self.atom_embedding(A) + self.atom_pos_embedding(AP)   # [N, n_channel, atom_embed_size]\n",
    "        atom_not_pad = (AP != self.atom_pad_id)  # [N, n_channel]\n",
    "        denom = torch.sum(atom_not_pad, dim=-1, keepdim=True) + self.eps\n",
    "        atom_embed = torch.sum(atom_embed * atom_not_pad.unsqueeze(-1), dim=1) / denom  # [N, atom_embed_size]\n",
    "        return torch.cat([res_embed, atom_embed], dim=-1)  # [N, res_embed_size + atom_embed_size]\n",
    "\n",
    "\n",
    "class AminoAcidFeature(nn.Module):\n",
    "    def __init__(self, embed_size, relative_position=True, edge_constructor=EdgeConstructor, backbone_only=False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone_only = backbone_only\n",
    "\n",
    "        # number of classes\n",
    "        self.num_aa_type = len(VOCAB)\n",
    "        self.num_atom_type = VOCAB.get_num_atom_type()\n",
    "        self.num_atom_pos = VOCAB.get_num_atom_pos()\n",
    "\n",
    "        # atom-level special tokens\n",
    "        self.atom_mask_idx = VOCAB.get_atom_mask_idx()\n",
    "        self.atom_pad_idx = VOCAB.get_atom_pad_idx()\n",
    "        self.atom_pos_mask_idx = VOCAB.get_atom_pos_mask_idx()\n",
    "        self.atom_pos_pad_idx = VOCAB.get_atom_pos_pad_idx()\n",
    "        \n",
    "        # embedding\n",
    "        self.aa_embedding = AminoAcidEmbedding(\n",
    "            self.num_aa_type, self.num_atom_type, self.num_atom_pos,\n",
    "            embed_size, embed_size, self.atom_pad_idx, relative_position)\n",
    "\n",
    "        # global nodes and mask nodes\n",
    "        self.boa_idx = VOCAB.symbol_to_idx(VOCAB.BOA)\n",
    "        self.boh_idx = VOCAB.symbol_to_idx(VOCAB.BOH)\n",
    "        self.bol_idx = VOCAB.symbol_to_idx(VOCAB.BOL)\n",
    "        self.mask_idx = VOCAB.get_mask_idx()\n",
    "\n",
    "        # segment ids\n",
    "        self.ag_seg_id, self.hc_seg_id, self.lc_seg_id = 1, 2, 3\n",
    "\n",
    "        # atoms encoding\n",
    "        residue_atom_type, residue_atom_pos = [], []\n",
    "        backbone = [VOCAB.atom_to_idx(atom[0]) for atom in VOCAB.backbone_atoms]\n",
    "        n_channel = VOCAB.MAX_ATOM_NUMBER if not backbone_only else 4\n",
    "        special_mask = VOCAB.get_special_mask()\n",
    "        for i in range(len(VOCAB)):\n",
    "            if i == self.boa_idx or i == self.boh_idx or i == self.bol_idx or i == self.mask_idx:\n",
    "                # global nodes\n",
    "                residue_atom_type.append([self.atom_mask_idx for _ in range(n_channel)])\n",
    "                residue_atom_pos.append([self.atom_pos_mask_idx for _ in range(n_channel)])\n",
    "            elif special_mask[i] == 1:\n",
    "                # other special token (pad)\n",
    "                residue_atom_type.append([self.atom_pad_idx for _ in range(n_channel)])\n",
    "                residue_atom_pos.append([self.atom_pos_pad_idx for _ in range(n_channel)])\n",
    "            else:\n",
    "                # normal amino acids\n",
    "                sidechain_atoms = VOCAB.get_sidechain_info(VOCAB.idx_to_symbol(i))\n",
    "                atom_type = backbone\n",
    "                atom_pos = [VOCAB.atom_pos_to_idx(VOCAB.atom_pos_bb) for _ in backbone]\n",
    "                if not backbone_only:\n",
    "                    sidechain_atoms = VOCAB.get_sidechain_info(VOCAB.idx_to_symbol(i))\n",
    "                    atom_type = atom_type + [VOCAB.atom_to_idx(atom[0]) for atom in sidechain_atoms]\n",
    "                    atom_pos = atom_pos + [VOCAB.atom_pos_to_idx(atom[1]) for atom in sidechain_atoms]\n",
    "                num_pad = n_channel - len(atom_type)\n",
    "                residue_atom_type.append(atom_type + [self.atom_pad_idx for _ in range(num_pad)])\n",
    "                residue_atom_pos.append(atom_pos + [self.atom_pos_pad_idx for _ in range(num_pad)])\n",
    "        \n",
    "        # mapping from residue to atom types and positions\n",
    "        self.residue_atom_type = nn.parameter.Parameter(\n",
    "            torch.tensor(residue_atom_type, dtype=torch.long),\n",
    "            requires_grad=False)\n",
    "        self.residue_atom_pos = nn.parameter.Parameter(\n",
    "            torch.tensor(residue_atom_pos, dtype=torch.long),\n",
    "            requires_grad=False)\n",
    "\n",
    "        # sidechain geometry\n",
    "        if not backbone_only:\n",
    "            sc_bonds, sc_bonds_mask = [], []\n",
    "            sc_chi_atoms, sc_chi_atoms_mask = [], []\n",
    "            for i in range(len(VOCAB)):\n",
    "                if special_mask[i] == 1:\n",
    "                    sc_bonds.append([])\n",
    "                    sc_chi_atoms.append([])\n",
    "                else:\n",
    "                    symbol = VOCAB.idx_to_symbol(i)\n",
    "                    atom_type = VOCAB.backbone_atoms + VOCAB.get_sidechain_info(symbol)\n",
    "                    atom2channel = { atom: i for i, atom in enumerate(atom_type) }\n",
    "                    chi_atoms, bond_atoms = VOCAB.get_sidechain_geometry(symbol)\n",
    "                    sc_chi_atoms.append(\n",
    "                        [[atom2channel[atom] for atom in atoms] for atoms in chi_atoms]\n",
    "                    )\n",
    "                    bonds = []\n",
    "                    for src_atom in bond_atoms:\n",
    "                        for dst_atom in bond_atoms[src_atom]:\n",
    "                            bonds.append((atom2channel[src_atom], atom2channel[dst_atom]))\n",
    "                    sc_bonds.append(bonds)\n",
    "            max_num_chis = max([len(chis) for chis in sc_chi_atoms])\n",
    "            max_num_bonds = max([len(bonds) for bonds in sc_bonds])\n",
    "            for i in range(len(VOCAB)):\n",
    "                num_chis, num_bonds = len(sc_chi_atoms[i]), len(sc_bonds[i])\n",
    "                num_pad_chis, num_pad_bonds = max_num_chis - num_chis, max_num_bonds - num_bonds\n",
    "                sc_chi_atoms_mask.append(\n",
    "                    [1 for _ in range(num_chis)] + [0 for _ in range(num_pad_chis)]\n",
    "                )\n",
    "                sc_bonds_mask.append(\n",
    "                    [1 for _ in range(num_bonds)] + [0 for _ in range(num_pad_bonds)]\n",
    "                )\n",
    "                sc_chi_atoms[i].extend([[-1, -1, -1, -1] for _ in range(num_pad_chis)])\n",
    "                sc_bonds[i].extend([(-1, -1) for _ in range(num_pad_bonds)])\n",
    "\n",
    "            # mapping residues to their sidechain chi angle atoms and bonds\n",
    "            self.sidechain_chi_angle_atoms = nn.parameter.Parameter(\n",
    "                torch.tensor(sc_chi_atoms, dtype=torch.long),\n",
    "                requires_grad=False)\n",
    "            self.sidechain_chi_mask = nn.parameter.Parameter(\n",
    "                torch.tensor(sc_chi_atoms_mask, dtype=torch.bool),\n",
    "                requires_grad=False\n",
    "            )\n",
    "            self.sidechain_bonds = nn.parameter.Parameter(\n",
    "                torch.tensor(sc_bonds, dtype=torch.long),\n",
    "                requires_grad=False\n",
    "            )\n",
    "            self.sidechain_bonds_mask = nn.parameter.Parameter(\n",
    "                torch.tensor(sc_bonds_mask, dtype=torch.bool),\n",
    "                requires_grad=False\n",
    "            )\n",
    "\n",
    "        # edge constructor\n",
    "        self.edge_constructor = edge_constructor(self.boa_idx, self.boh_idx, self.bol_idx, self.atom_pos_pad_idx, self.ag_seg_id)\n",
    "\n",
    "    def _is_global(self, S):\n",
    "        return sequential_or(S == self.boa_idx, S == self.boh_idx, S == self.bol_idx)  # [N]\n",
    "\n",
    "    def _construct_residue_pos(self, S):\n",
    "        # construct residue position. global node is 1, the first residue is 2, ... (0 for padding)\n",
    "        glbl_node_mask = self._is_global(S)\n",
    "        glbl_node_idx = torch.nonzero(glbl_node_mask).flatten()  # [batch_size * 3] (boa, boh, bol)\n",
    "        shift = F.pad(glbl_node_idx[:-1] - glbl_node_idx[1:] + 1, (1, 0), value=1) # [batch_size * 3]\n",
    "        residue_pos = torch.ones_like(S)\n",
    "        residue_pos[glbl_node_mask] = shift\n",
    "        residue_pos = torch.cumsum(residue_pos, dim=0)\n",
    "        return residue_pos\n",
    "\n",
    "    def _construct_segment_ids(self, S):\n",
    "        # construct segment ids. 1/2/3 for antigen/heavy chain/light chain\n",
    "        glbl_node_mask = self._is_global(S)\n",
    "        glbl_nodes = S[glbl_node_mask]\n",
    "        boa_mask, boh_mask, bol_mask = (glbl_nodes == self.boa_idx), (glbl_nodes == self.boh_idx), (glbl_nodes == self.bol_idx)\n",
    "        glbl_nodes[boa_mask], glbl_nodes[boh_mask], glbl_nodes[bol_mask] = self.ag_seg_id, self.hc_seg_id, self.lc_seg_id\n",
    "        segment_ids = torch.zeros_like(S)\n",
    "        segment_ids[glbl_node_mask] = glbl_nodes - F.pad(glbl_nodes[:-1], (1, 0), value=0)\n",
    "        segment_ids = torch.cumsum(segment_ids, dim=0)\n",
    "        return segment_ids\n",
    "\n",
    "    def _construct_atom_type(self, S):\n",
    "        # construct atom types\n",
    "        return self.residue_atom_type[S]\n",
    "    \n",
    "    def _construct_atom_pos(self, S):\n",
    "        # construct atom positions\n",
    "        return self.residue_atom_pos[S]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_sidechain_chi_angles_atoms(self, S):\n",
    "        chi_angles_atoms = self.sidechain_chi_angle_atoms[S]  # [N, max_num_chis, 4]\n",
    "        chi_mask = self.sidechain_chi_mask[S]  # [N, max_num_chis]\n",
    "        return chi_angles_atoms, chi_mask\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_sidechain_bonds(self, S):\n",
    "        bonds = self.sidechain_bonds[S]  # [N, max_num_bond, 2]\n",
    "        bond_mask = self.sidechain_bonds_mask[S]\n",
    "        return bonds, bond_mask\n",
    "\n",
    "    def update_globel_coordinates(self, X, S, atom_pos=None):\n",
    "        X = X.clone()\n",
    "\n",
    "        if atom_pos is None:  # [N, n_channel]\n",
    "            atom_pos = self._construct_atom_pos(S)\n",
    "\n",
    "        glbl_node_mask = self._is_global(S)\n",
    "        chain_id = glbl_node_mask.long()\n",
    "        chain_id = torch.cumsum(chain_id, dim=0)  # [N]\n",
    "        chain_id[glbl_node_mask] = 0    # set global nodes to 0\n",
    "        chain_id = chain_id.unsqueeze(-1).repeat(1, atom_pos.shape[-1])  # [N, n_channel]\n",
    "        \n",
    "        not_global = torch.logical_not(glbl_node_mask)\n",
    "        not_pad = (atom_pos != self.atom_pos_pad_idx)[not_global]\n",
    "        flatten_coord = X[not_global][not_pad]  # [N_atom, 3]\n",
    "        flatten_chain_id = chain_id[not_global][not_pad]\n",
    "\n",
    "        global_x = scatter_mean(\n",
    "            src=flatten_coord, index=flatten_chain_id,\n",
    "            dim=0, dim_size=glbl_node_mask.sum() + 1)  # because index start from 1\n",
    "        X[glbl_node_mask] = global_x[1:].unsqueeze(1)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def embedding(self, S, residue_pos=None, atom_type=None, atom_pos=None):\n",
    "        '''\n",
    "        :param S: [N], residue types\n",
    "        '''\n",
    "        if residue_pos is None:  # Residue positions in the chain\n",
    "            residue_pos = self._construct_residue_pos(S)  # [N]\n",
    "\n",
    "        if atom_type is None:  # Atom types in each residue\n",
    "            atom_type = self.residue_atom_type[S]  # [N, n_channel]\n",
    "\n",
    "        if atom_pos is None:   # Atom position in each residue\n",
    "            atom_pos = self.residue_atom_pos[S]     # [N, n_channel]\n",
    "\n",
    "        H = self.aa_embedding(S, residue_pos, atom_type, atom_pos)\n",
    "        return H, (residue_pos, atom_type, atom_pos)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def construct_edges(self, X, S, batch_id, k_neighbors, atom_pos=None, segment_ids=None):\n",
    "\n",
    "        # prepare inputs\n",
    "        if atom_pos is None:  # Atom position in each residue (pad need to be ignored)\n",
    "            atom_pos = self.residue_atom_pos[S]\n",
    "        \n",
    "        if segment_ids is None:\n",
    "            segment_ids = self._construct_segment_ids(S)\n",
    "\n",
    "        ctx_edges, inter_edges = self.edge_constructor.construct_edges(\n",
    "            X, S, batch_id, k_neighbors, atom_pos, segment_ids)\n",
    "\n",
    "        return ctx_edges, inter_edges\n",
    "\n",
    "    def forward(self, X, S, batch_id, k_neighbors):\n",
    "        H, (_, _, atom_pos) = self.embedding(S)\n",
    "        ctx_edges, inter_edges = self.construct_edges(\n",
    "            X, S, batch_id, k_neighbors, atom_pos=atom_pos)\n",
    "        return H, (ctx_edges, inter_edges)\n",
    "\n",
    "\n",
    "class SeparatedAminoAcidFeature(AminoAcidFeature):\n",
    "    '''\n",
    "    Separate embeddings of atoms and residues\n",
    "    '''\n",
    "    def __init__(self, embed_size, atom_embed_size, relative_position=True, edge_constructor=EdgeConstructor, fix_atom_weights=False, backbone_only=False) -> None:\n",
    "        super().__init__(embed_size, relative_position=relative_position, edge_constructor=edge_constructor, backbone_only=backbone_only)\n",
    "        atom_weights_mask = self.residue_atom_type == self.atom_pad_idx\n",
    "        self.register_buffer('atom_weights_mask', atom_weights_mask)\n",
    "        self.fix_atom_weights = fix_atom_weights\n",
    "        if fix_atom_weights:\n",
    "            atom_weights = torch.ones_like(self.residue_atom_type, dtype=torch.float)\n",
    "        else:\n",
    "            atom_weights = torch.randn_like(self.residue_atom_type, dtype=torch.float)\n",
    "        atom_weights[atom_weights_mask] = 0\n",
    "        self.atom_weight = nn.parameter.Parameter(atom_weights, requires_grad=not fix_atom_weights)\n",
    "        self.zero_atom_weight = nn.parameter.Parameter(torch.zeros_like(atom_weights), requires_grad=False)\n",
    "        \n",
    "        # override\n",
    "        self.aa_embedding = AminoAcidEmbedding(\n",
    "            self.num_aa_type, self.num_atom_type, self.num_atom_pos,\n",
    "            embed_size, atom_embed_size, self.atom_pad_idx, relative_position)\n",
    "    \n",
    "    def get_atom_weights(self, residue_types):\n",
    "        weights = torch.where(\n",
    "            self.atom_weights_mask,\n",
    "            self.zero_atom_weight,\n",
    "            self.atom_weight\n",
    "        )  # [num_aa_classes, max_atom_number(n_channel)]\n",
    "        if not self.fix_atom_weights:\n",
    "            weights = F.normalize(weights, dim=-1)\n",
    "        return weights[residue_types]\n",
    "\n",
    "    def forward(self, X, S, batch_id, k_neighbors, residue_pos=None, smooth_prob=None, smooth_mask=None):\n",
    "        if residue_pos is None:\n",
    "            residue_pos = self._construct_residue_pos(S)  # [N]\n",
    "        atom_type = self.residue_atom_type[S]  # [N, n_channel]\n",
    "        atom_pos = self.residue_atom_pos[S]     # [N, n_channel]\n",
    "\n",
    "        # residue embedding\n",
    "        pos_embedding = self.aa_embedding.res_pos_embedding(residue_pos)\n",
    "        H = self.aa_embedding.residue_embedding(S)\n",
    "        if smooth_prob is not None:\n",
    "            res_embeddings = self.aa_embedding.residue_embedding(\n",
    "                torch.arange(smooth_prob.shape[-1], device=S.device, dtype=S.dtype)\n",
    "            )  # [num_aa_type, embed_size]\n",
    "            H[smooth_mask] = smooth_prob.mm(res_embeddings)\n",
    "        H = H + pos_embedding\n",
    "\n",
    "        # atom embedding\n",
    "        atom_embedding = self.aa_embedding.atom_embedding(atom_type) +\\\n",
    "                         self.aa_embedding.atom_pos_embedding(atom_pos)\n",
    "        atom_weights = self.get_atom_weights(S)\n",
    "        \n",
    "        ctx_edges, inter_edges = self.construct_edges(\n",
    "            X, S, batch_id, k_neighbors, atom_pos=atom_pos)\n",
    "        return H, (ctx_edges, inter_edges), (atom_embedding, atom_weights)\n",
    "\n",
    "\n",
    "class ProteinFeature:\n",
    "    def __init__(self, backbone_only=False):\n",
    "        self.backbone_only = backbone_only\n",
    "\n",
    "    def _cal_sidechain_bond_lengths(self, S, X, aa_feature: AminoAcidFeature):\n",
    "        bonds, bonds_mask = aa_feature.get_sidechain_bonds(S)\n",
    "        n = torch.nonzero(bonds_mask)[:, 0]  # [Nbonds]\n",
    "        src, dst = bonds[bonds_mask].T\n",
    "        src_X, dst_X = X[(n, src)], X[(n, dst)]  # [Nbonds, 3]\n",
    "        bond_lengths = torch.norm(dst_X - src_X, dim=-1)\n",
    "        return bond_lengths\n",
    "\n",
    "    def _cal_sidechain_chis(self, S, X, aa_feature: AminoAcidFeature):\n",
    "        chi_atoms, chi_mask = aa_feature.get_sidechain_chi_angles_atoms(S)\n",
    "        n = torch.nonzero(chi_mask)[:, 0]  # [Nchis]\n",
    "        a0, a1, a2, a3 = chi_atoms[chi_mask].T  # [Nchis]\n",
    "        x0, x1, x2, x3 = X[(n, a0)], X[(n, a1)], X[(n, a2)], X[(n, a3)]  # [Nchis, 3]\n",
    "        u_0, u_1, u_2 = (x1 - x0), (x2 - x1), (x3 - x2)  # [Nchis, 3]\n",
    "        # normals of the two planes\n",
    "        n_1 = F.normalize(torch.cross(u_0, u_1), dim=-1)  # [Nchis, 3]\n",
    "        n_2 = F.normalize(torch.cross(u_1, u_2), dim=-1)  # [Nchis, 3]\n",
    "        cosChi = (n_1 * n_2).sum(-1)  # [Nchis]\n",
    "        eps = 1e-7\n",
    "        cosChi = torch.clamp(cosChi, -1 + eps, 1 - eps)\n",
    "        return cosChi\n",
    "\n",
    "    def _cal_backbone_bond_lengths(self, X, seg_id):\n",
    "        # loss of backbone (...N-CA-C(O)-N...) bond length\n",
    "        # N-CA, CA-C, C=O\n",
    "        bl1 = torch.norm(X[:, 1:4] - X[:, :3], dim=-1)  # [N, 3], (N-CA), (CA-C), (C=O)\n",
    "        # C-N\n",
    "        bl2 = torch.norm(X[1:, 0] - X[:-1, 2], dim=-1)  # [N-1]\n",
    "        same_chain_mask = seg_id[1:] == seg_id[:-1]\n",
    "        bl2 = bl2[same_chain_mask]\n",
    "        bl = torch.cat([bl1.flatten(), bl2], dim=0)\n",
    "        return bl\n",
    "\n",
    "    def _cal_angles(self, X, seg_id):\n",
    "        ori_X = X\n",
    "        X = X[:, :3].reshape(-1, 3)  # [N * 3, 3], N, CA, C\n",
    "        U = F.normalize(X[1:] - X[:-1], dim=-1)  # [N * 3 - 1, 3]\n",
    "\n",
    "        # 1. dihedral angles\n",
    "        u_2, u_1, u_0 = U[:-2], U[1:-1], U[2:]   # [N * 3 - 3, 3]\n",
    "        # backbone normals\n",
    "        n_2 = F.normalize(torch.cross(u_2, u_1), dim=-1)\n",
    "        n_1 = F.normalize(torch.cross(u_1, u_0), dim=-1)\n",
    "        # angle between normals\n",
    "        eps = 1e-7\n",
    "        cosD = (n_2 * n_1).sum(-1)  # [(N-1) * 3]\n",
    "        cosD = torch.clamp(cosD, -1 + eps, 1 - eps)\n",
    "        # D = torch.sign((u_2 * n_1).sum(-1)) * torch.acos(cosD)\n",
    "        seg_id_atom = seg_id.repeat(1, 3).flatten()  # [N * 3]\n",
    "        same_chain_mask = sequential_and(\n",
    "            seg_id_atom[:-3] == seg_id_atom[1:-2],\n",
    "            seg_id_atom[1:-2] == seg_id_atom[2:-1],\n",
    "            seg_id_atom[2:-1] == seg_id_atom[3:]\n",
    "        )  # [N * 3 - 3]\n",
    "        # D = D[same_chain_mask]\n",
    "        cosD = cosD[same_chain_mask]\n",
    "\n",
    "        # 2. bond angles (C_{n-1}-N, N-CA), (N-CA, CA-C), (CA-C, C=O), (CA-C, C-N_{n+1}), (O=C, C-Nn)\n",
    "        u_0, u_1 = U[:-1], U[1:]  # [N*3 - 2, 3]\n",
    "        cosA1 = ((-u_0) * u_1).sum(-1)  # [N*3 - 2], (C_{n-1}-N, N-CA), (N-CA, CA-C), (CA-C, C-N_{n+1})\n",
    "        same_chain_mask = sequential_and(\n",
    "            seg_id_atom[:-2] == seg_id_atom[1:-1],\n",
    "            seg_id_atom[1:-1] == seg_id_atom[2:]\n",
    "        )\n",
    "        cosA1 = cosA1[same_chain_mask]  # [N*3 - 2 * num_chain]\n",
    "        u_co = F.normalize(ori_X[:, 3] - ori_X[:, 2], dim=-1)  # [N, 3], C=O\n",
    "        u_cca = -U[1::3]  # [N, 3], C-CA\n",
    "        u_cn = U[2::3] # [N-1, 3], C-N_{n+1}\n",
    "        cosA2 = (u_co * u_cca).sum(-1)  # [N], (C=O, C-CA)\n",
    "        cosA3 = (u_co[:-1] * u_cn).sum(-1)  # [N-1], (C=O, C-N_{n+1})\n",
    "        same_chain_mask = (seg_id[:-1] == seg_id[1:]) # [N-1]\n",
    "        cosA3 = cosA3[same_chain_mask]\n",
    "        cosA = torch.cat([cosA1, cosA2, cosA3], dim=-1)\n",
    "        cosA = torch.clamp(cosA, -1 + eps, 1 - eps)\n",
    "\n",
    "        return cosD, cosA\n",
    "\n",
    "    def coord_loss(self, pred_X, true_X, batch_id, atom_mask, reference=None):\n",
    "        pred_bb, true_bb = pred_X[:, :4], true_X[:, :4]\n",
    "        bb_mask = atom_mask[:, :4]\n",
    "        true_X = true_X.clone()\n",
    "        ops = []\n",
    "\n",
    "        align_obj = pred_bb if reference is None else reference[:, :4]\n",
    "\n",
    "        for i in range(torch.max(batch_id) + 1):\n",
    "            is_cur_graph = batch_id == i\n",
    "            cur_bb_mask = bb_mask[is_cur_graph]\n",
    "            _, R, t = kabsch_torch(\n",
    "                true_bb[is_cur_graph][cur_bb_mask],\n",
    "                align_obj[is_cur_graph][cur_bb_mask],\n",
    "                requires_grad=True)\n",
    "            true_X[is_cur_graph] = torch.matmul(true_X[is_cur_graph], R.T) + t\n",
    "            ops.append((R.detach(), t.detach()))\n",
    "\n",
    "        xloss = F.smooth_l1_loss(\n",
    "            pred_X[atom_mask], true_X[atom_mask],\n",
    "            reduction='sum') / atom_mask.sum()  # atom-level loss\n",
    "        bb_rmsd = torch.sqrt(((pred_X[:, :4] - true_X[:, :4]) ** 2).sum(-1).mean(-1))  # [N]\n",
    "        return xloss, bb_rmsd, ops\n",
    "\n",
    "    def structure_loss(self, pred_X, true_X, S, cmask, batch_id, xloss_mask, aa_feature, full_profile=False, reference=None):\n",
    "        atom_pos = aa_feature._construct_atom_pos(S)[cmask]\n",
    "        seg_id = aa_feature._construct_segment_ids(S)[cmask]\n",
    "        atom_mask = atom_pos != aa_feature.atom_pos_pad_idx\n",
    "        atom_mask = torch.logical_and(atom_mask, xloss_mask[cmask])\n",
    "\n",
    "        pred_X, true_X, batch_id = pred_X[cmask], true_X[cmask], batch_id[cmask]\n",
    "\n",
    "        # loss of absolute coordinates\n",
    "        xloss, bb_rmsd, ops = self.coord_loss(pred_X, true_X, batch_id, atom_mask, reference)\n",
    "\n",
    "        # loss of backbone (...N-CA-C(O)-N...) bond length\n",
    "        true_bl = self._cal_backbone_bond_lengths(true_X, seg_id)\n",
    "        pred_bl = self._cal_backbone_bond_lengths(pred_X, seg_id)\n",
    "        bond_loss = F.smooth_l1_loss(pred_bl, true_bl)\n",
    "\n",
    "        # loss of backbone dihedral angles\n",
    "        if full_profile:\n",
    "            true_cosD, true_cosA = self._cal_angles(true_X, seg_id)\n",
    "            pred_cosD, pred_cosA = self._cal_angles(pred_X, seg_id)\n",
    "            angle_loss = F.smooth_l1_loss(pred_cosD, true_cosD)\n",
    "            bond_angle_loss = F.smooth_l1_loss(pred_cosA, true_cosA)\n",
    "\n",
    "        S = S[cmask]\n",
    "        if self.backbone_only:\n",
    "            sc_bond_loss, sc_chi_loss = 0, 0\n",
    "        else:\n",
    "            # loss of sidechain bonds\n",
    "            true_sc_bl = self._cal_sidechain_bond_lengths(S, true_X, aa_feature)\n",
    "            pred_sc_bl = self._cal_sidechain_bond_lengths(S, pred_X, aa_feature)\n",
    "            sc_bond_loss = F.smooth_l1_loss(pred_sc_bl, true_sc_bl)\n",
    "\n",
    "            # loss of sidechain chis\n",
    "            if full_profile:\n",
    "                true_sc_chi = self._cal_sidechain_chis(S, true_X, aa_feature)\n",
    "                pred_sc_chi = self._cal_sidechain_chis(S, pred_X, aa_feature)\n",
    "                sc_chi_loss = F.smooth_l1_loss(pred_sc_chi, true_sc_chi)\n",
    "\n",
    "        # exerting constraints on bond lengths only is sufficient\n",
    "        violation_loss = bond_loss + sc_bond_loss\n",
    "        loss = xloss + violation_loss\n",
    "\n",
    "        if full_profile:\n",
    "            details = (xloss, bond_loss, bond_angle_loss, angle_loss, sc_bond_loss, sc_chi_loss)\n",
    "        else:\n",
    "            details = (xloss, bond_loss, sc_bond_loss)\n",
    "\n",
    "        return loss, details, bb_rmsd, ops\n",
    "\n",
    "\n",
    "class SeperatedCoordNormalizer(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.mean = torch.tensor(0)\n",
    "        self.std = torch.tensor(10)\n",
    "        self.mean = nn.parameter.Parameter(self.mean, requires_grad=False)\n",
    "        self.std = nn.parameter.Parameter(self.std, requires_grad=False)\n",
    "        self.boa_idx = VOCAB.symbol_to_idx(VOCAB.BOA)\n",
    "\n",
    "    def normalize(self, X):\n",
    "        X = (X - self.mean) / self.std\n",
    "        return X\n",
    "\n",
    "    def unnormalize(self, X):\n",
    "        X = X * self.std + self.mean\n",
    "        return X\n",
    "\n",
    "    def centering(self, X, S, batch_id, aa_feature: AminoAcidFeature):\n",
    "        # centering antigen and antibody separatedly\n",
    "        segment_ids = aa_feature._construct_segment_ids(S)\n",
    "        not_bol = S != aa_feature.bol_idx\n",
    "        tmp_S = S[not_bol]\n",
    "        tmp_X = aa_feature.update_globel_coordinates(X[not_bol], tmp_S)\n",
    "        self.ag_centers = tmp_X[tmp_S == aa_feature.boa_idx][:, 0]\n",
    "        self.ab_centers = tmp_X[tmp_S == aa_feature.boh_idx][:, 0]\n",
    "\n",
    "        is_ag = segment_ids == aa_feature.ag_seg_id\n",
    "        is_ab = torch.logical_not(is_ag)\n",
    "\n",
    "        # compose centers\n",
    "        centers = torch.zeros(X.shape[0], X.shape[-1], dtype=X.dtype, device=X.device)\n",
    "        centers[is_ag] = self.ag_centers[batch_id[is_ag]]\n",
    "        centers[is_ab] = self.ab_centers[batch_id[is_ab]]\n",
    "        X = X - centers.unsqueeze(1)\n",
    "        self.is_ag, self.is_ab = is_ag, is_ab\n",
    "        return X\n",
    "\n",
    "    def uncentering(self, X, batch_id, _type=1):\n",
    "        if _type == 0:\n",
    "            # type 0: [N, 3]\n",
    "            X = X.unsqueeze(1) # then it is type 1\n",
    "        \n",
    "        if _type == 0 or _type == 1:\n",
    "            # type 1: [N, n_channel, 3]\n",
    "            centers = torch.zeros(X.shape[0], X.shape[-1], dtype=X.dtype, device=X.device)\n",
    "            centers[self.is_ag] = self.ag_centers[batch_id[self.is_ag]]\n",
    "            centers[self.is_ab] = self.ab_centers[batch_id[self.is_ab]]\n",
    "            X = X + centers.unsqueeze(1)\n",
    "        elif _type == 2:\n",
    "            # type 2: [2, bs, K, 3], X[0] for antigen, X[1] for antibody\n",
    "            centers = torch.stack([self.ag_centers, self.ab_centers], dim=0)  # [2, bs, 3]\n",
    "            X = X + centers.unsqueeze(-2)\n",
    "        elif _type == 3:\n",
    "            # type 3: [2, Ef, 3], X[0] for antigen, X[1] for antibody\n",
    "            centers = torch.stack([self.ag_centers[batch_id], self.ab_centers[batch_id]], dim=0)\n",
    "            X = X + centers\n",
    "        elif _type == 4:\n",
    "            # type 4: [N, n_channel, 3], but all uncentering to the center of antigen\n",
    "            centers = self.ag_centers[batch_id]\n",
    "            X = X + centers.unsqueeze(1)\n",
    "        else:\n",
    "            raise NotImplementedError(f'uncentering for type {_type} not implemented')\n",
    "\n",
    "        if _type == 0:\n",
    "            X = X.squeeze(1)\n",
    "        return X\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self.ag_centers, self.ab_centers, self.is_ag, self.is_ab = None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-10 01:15:10::INFO::<__main__.Config object at 0x151f2ab7d0d0>\n",
      "2023-10-10 01:15:10::INFO::CDR type: ['H3']\n",
      "2023-10-10 01:15:10::INFO::Paratope: H3\n",
      "2023-10-10 01:15:10::INFO::sequence & structure codesign\n",
      "till here\n",
      "2023-10-10 01:15:10::INFO::No meta-info file found, start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 832/2638 [03:54<11:07,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-10 01:19:05::ERROR::Antigen chain K has something wrong!\n",
      "2023-10-10 01:19:05::ERROR::parse 6ulf pdb failed, skip\n",
      "2023-10-10 01:19:05::ERROR::Antigen chain K has something wrong!\n",
      "2023-10-10 01:19:05::ERROR::parse 6vln pdb failed, skip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1013/2638 [04:30<03:46,  7.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-10 01:19:41::ERROR::Antigen chain G has something wrong!\n",
      "2023-10-10 01:19:41::ERROR::parse 4cc8 pdb failed, skip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 2355/2638 [10:55<01:41,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-10 01:26:06::ERROR::Antigen chain E has something wrong!\n",
      "2023-10-10 01:26:06::ERROR::parse 6qd8 pdb failed, skip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 2461/2638 [11:26<00:21,  8.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-10 01:26:39::ERROR::Antigen chain E has something wrong!\n",
      "2023-10-10 01:26:39::ERROR::parse 6qd7 pdb failed, skip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2638/2638 [12:20<00:00,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-10 01:27:31::INFO::Saving ./all_data/RAbD/train_processed/part_0.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-10 01:28:23::INFO::Loading preprocessed file /home/dagaa/Projects/dyMEAN/all_data/RAbD/train_processed/part_0.pkl, 1/1\n"
     ]
    }
   ],
   "source": [
    "########### load your train / valid set ###########\n",
    "if len(config.gpus) >= 1:\n",
    "    print_log(config)\n",
    "    print_log(f'CDR type: {config.cdr}')\n",
    "    print_log(f'Paratope: {config.paratope}')\n",
    "    print_log('structure only' if config.struct_only else 'sequence & structure codesign')\n",
    "\n",
    "print('till here')\n",
    "train_set = E2EDataset(config.train_set, cdr=config.cdr, paratope=config.paratope)\n",
    "\n",
    "########## set your collate_fn ##########\n",
    "collate_fn = train_set.collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = train_set.__getitem__(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./all_data/RAbD/train.json'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': array([[[194.68936648, -64.90005667, 161.78872256],\n",
       "         [194.68936648, -64.90005667, 161.78872256],\n",
       "         [194.68936648, -64.90005667, 161.78872256],\n",
       "         ...,\n",
       "         [194.68936648, -64.90005667, 161.78872256],\n",
       "         [194.68936648, -64.90005667, 161.78872256],\n",
       "         [194.68936648, -64.90005667, 161.78872256]],\n",
       " \n",
       "        [[187.2230072 , -82.16100311, 154.39599609],\n",
       "         [188.30700684, -81.2990036 , 153.94099426],\n",
       "         [188.4940033 , -81.40899658, 152.42500305],\n",
       "         ...,\n",
       "         [188.30700684, -81.2990036 , 153.94099426],\n",
       "         [188.30700684, -81.2990036 , 153.94099426],\n",
       "         [188.30700684, -81.2990036 , 153.94099426]],\n",
       " \n",
       "        [[185.14500427, -64.29699707, 159.91000366],\n",
       "         [185.47200012, -62.92200089, 159.57000732],\n",
       "         [184.27099609, -62.        , 159.71800232],\n",
       "         ...,\n",
       "         [185.47200012, -62.92200089, 159.57000732],\n",
       "         [185.47200012, -62.92200089, 159.57000732],\n",
       "         [185.47200012, -62.92200089, 159.57000732]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[231.9730072 , -86.21700287, 142.45599365],\n",
       "         [233.22200012, -86.36299896, 141.67599487],\n",
       "         [233.39500427, -87.79599762, 141.16000366],\n",
       "         ...,\n",
       "         [233.22200012, -86.36299896, 141.67599487],\n",
       "         [233.22200012, -86.36299896, 141.67599487],\n",
       "         [233.22200012, -86.36299896, 141.67599487]],\n",
       " \n",
       "        [[234.2250061 , -87.95700073, 140.13600159],\n",
       "         [234.48100281, -89.27899933, 139.53999329],\n",
       "         [235.83099365, -89.89499664, 139.95500183],\n",
       "         ...,\n",
       "         [234.48100281, -89.27899933, 139.53999329],\n",
       "         [234.48100281, -89.27899933, 139.53999329],\n",
       "         [234.48100281, -89.27899933, 139.53999329]],\n",
       " \n",
       "        [[236.65499878, -89.14099884, 140.68899536],\n",
       "         [237.91200256, -89.66200256, 141.23399353],\n",
       "         [237.65899658, -90.71199799, 142.32600403],\n",
       "         ...,\n",
       "         [237.91200256, -89.66200256, 141.23399353],\n",
       "         [237.91200256, -89.66200256, 141.23399353],\n",
       "         [237.91200256, -89.66200256, 141.23399353]]]),\n",
       " 'S': array([22, 10, 13, 15,  5,  2,  3, 10,  2,  0,  5,  2,  3, 10,  5, 15,  4,\n",
       "        11, 19,  5, 16,  4,  0,  8, 15,  2, 12,  7,  6,  3, 15,  6,  6,  5,\n",
       "         5, 10, 12, 19,  7, 10, 15, 19,  9, 13, 12,  0,  6,  8,  5, 23,  2,\n",
       "        13,  3,  2, 11, 16,  0,  0,  0,  2,  2, 13, 19,  0, 15, 16,  3, 15,\n",
       "         3, 16, 18,  1,  1, 16,  0,  5, 17,  5, 16, 10,  7,  0,  4,  9,  6,\n",
       "         2, 15, 13,  1, 19,  0, 12,  0,  3, 11,  6,  2,  1,  2,  4,  6,  7,\n",
       "         8,  0, 16,  4, 12,  7,  7,  1,  8, 16,  2, 12,  0, 15,  5, 17,  4,\n",
       "        16, 15,  8, 10, 16, 12, 10, 17,  3,  7,  3, 13, 14, 10, 16,  3, 15,\n",
       "         1, 11,  8, 17,  1,  2,  7,  7, 18,  1, 15,  8, 15,  1,  1,  1,  0,\n",
       "         3,  9,  7,  7,  7,  0, 14,  8,  2,  6,  0, 13,  0, 17, 17,  2, 17,\n",
       "         2, 16, 16,  1]),\n",
       " 'residue_pos': array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   2,   3,\n",
       "          4,   5,   6,   7,   8,   9,  11,  12,  13,  14,  15,  16,  17,\n",
       "         18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,  29,  30,\n",
       "         35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  45,  46,  47,\n",
       "         48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  62,\n",
       "         63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  74,  75,  76,\n",
       "         77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,\n",
       "         90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102,\n",
       "        103, 104, 105, 106, 107, 108, 109, 110, 111, 111, 111, 112, 112,\n",
       "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124,\n",
       "        125, 126, 127, 128, 129]),\n",
       " 'xloss_mask': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]]),\n",
       " 'cmask': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'smask': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'paratope_mask': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'template': array([[[110.17071005, -33.81381579, -32.1063393 ],\n",
       "         [111.77556251, -33.73790541, -30.23527663],\n",
       "         [111.21288212, -32.85972521, -27.07254233],\n",
       "         ...,\n",
       "         [111.77556251, -33.73790541, -30.23527663],\n",
       "         [111.77556251, -33.73790541, -30.23527663],\n",
       "         [111.77556251, -33.73790541, -30.23527663]],\n",
       " \n",
       "        [[110.94930673, -31.32916218, -30.72705368],\n",
       "         [112.35840757, -31.16359689, -29.04936946],\n",
       "         [111.90436644, -30.29397856, -26.23788704],\n",
       "         ...,\n",
       "         [112.35840757, -31.16359689, -29.04936946],\n",
       "         [112.35840757, -31.16359689, -29.04936946],\n",
       "         [112.35840757, -31.16359689, -29.04936946]],\n",
       " \n",
       "        [[111.72790341, -28.84450857, -29.34776805],\n",
       "         [112.94125264, -28.58928837, -27.86346228],\n",
       "         [112.59585075, -27.7282319 , -25.40323175],\n",
       "         ...,\n",
       "         [112.94125264, -28.58928837, -27.86346228],\n",
       "         [112.94125264, -28.58928837, -27.86346228],\n",
       "         [112.94125264, -28.58928837, -27.86346228]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[120.39450984,  -8.05202588, -17.21819851],\n",
       "         [120.96402728,  -7.75514361, -15.95826796],\n",
       "         [122.24355892,  -7.41565444, -16.00413712],\n",
       "         ...,\n",
       "         [120.96402728,  -7.75514361, -15.95826796],\n",
       "         [120.96402728,  -7.75514361, -15.95826796],\n",
       "         [120.96402728,  -7.75514361, -15.95826796]],\n",
       " \n",
       "        [[125.29934472,  -7.01373187, -17.93531232],\n",
       "         [125.55302374,  -7.54350068, -15.28100158],\n",
       "         [127.63165726,  -7.26289873, -16.72002232],\n",
       "         ...,\n",
       "         [125.55302374,  -7.54350068, -15.28100158],\n",
       "         [125.55302374,  -7.54350068, -15.28100158],\n",
       "         [125.55302374,  -7.54350068, -15.28100158]],\n",
       " \n",
       "        [[130.2041796 ,  -5.97543787, -18.65242614],\n",
       "         [130.14202019,  -7.33185774, -14.60373519],\n",
       "         [133.01975561,  -7.11014303, -17.43590752],\n",
       "         ...,\n",
       "         [130.14202019,  -7.33185774, -14.60373519],\n",
       "         [130.14202019,  -7.33185774, -14.60373519],\n",
       "         [130.14202019,  -7.33185774, -14.60373519]]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X', 'S', 'residue_pos', 'xloss_mask', 'cmask', 'smask', 'paratope_mask', 'template'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174, 14, 3)\n",
      "(174,)\n",
      "(174,)\n",
      "(174, 14)\n",
      "174\n",
      "(124, 14, 3)\n"
     ]
    }
   ],
   "source": [
    "print(example['X'].shape)\n",
    "print(example['S'].shape)\n",
    "print(example['residue_pos'].shape)\n",
    "print(example['xloss_mask'].shape)\n",
    "print(len(example['cmask']))\n",
    "print(example['template'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-10 02:02:10::INFO::<__main__.Config object at 0x151f2ab7d0d0>\n",
      "2023-10-10 02:02:10::INFO::CDR type: ['H3']\n",
      "2023-10-10 02:02:10::INFO::Paratope: H3\n",
      "2023-10-10 02:02:10::INFO::sequence & structure codesign\n",
      "till here\n",
      "2023-10-10 02:02:10::INFO::Loading preprocessed file /home/dagaa/Projects/dyMEAN/all_data/RAbD/train_processed/part_0.pkl, 1/1\n",
      "2023-10-10 02:02:46::INFO::No meta-info file found, start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 48/339 [00:25<02:37,  1.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtill here\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m train_set \u001b[39m=\u001b[39m E2EDataset(config\u001b[39m.\u001b[39mtrain_set, cdr\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mcdr, paratope\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mparatope)\n\u001b[0;32m---> 10\u001b[0m valid_set \u001b[39m=\u001b[39m E2EDataset(config\u001b[39m.\u001b[39;49mvalid_set, cdr\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mcdr, paratope\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mparatope)\n\u001b[1;32m     12\u001b[0m \u001b[39m########## set your collate_fn ##########\u001b[39;00m\n\u001b[1;32m     13\u001b[0m collate_fn \u001b[39m=\u001b[39m train_set\u001b[39m.\u001b[39mcollate_fn\n",
      "Cell \u001b[0;32mIn[9], line 99\u001b[0m, in \u001b[0;36mE2EDataset.__init__\u001b[0;34m(self, file_path, save_dir, cdr, paratope, full_antigen, num_entry_per_file, random)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mif\u001b[39;00m need_process:\n\u001b[1;32m     97\u001b[0m     \u001b[39m# preprocess\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_names, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_num_entries \u001b[39m=\u001b[39m [], []\n\u001b[0;32m---> 99\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpreprocess(file_path, save_dir, num_entry_per_file)\n\u001b[1;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_entry \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_num_entries)\n\u001b[1;32m    102\u001b[0m     metainfo \u001b[39m=\u001b[39m {\n\u001b[1;32m    103\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mnum_entry\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_entry,\n\u001b[1;32m    104\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfile_names\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_names,\n\u001b[1;32m    105\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mfile_num_entries\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_num_entries\n\u001b[1;32m    106\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[9], line 182\u001b[0m, in \u001b[0;36mE2EDataset.preprocess\u001b[0;34m(self, file_path, save_dir, num_entry_per_file)\u001b[0m\n\u001b[1;32m    179\u001b[0m item \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(line)\n\u001b[1;32m    180\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[39m# print('making AgABComplex')\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     cplx \u001b[39m=\u001b[39m AgAbComplex\u001b[39m.\u001b[39;49mfrom_pdb(\n\u001b[1;32m    183\u001b[0m         item[\u001b[39m'\u001b[39;49m\u001b[39mpdb_data_path\u001b[39;49m\u001b[39m'\u001b[39;49m], item[\u001b[39m'\u001b[39;49m\u001b[39mheavy_chain\u001b[39;49m\u001b[39m'\u001b[39;49m], item[\u001b[39m'\u001b[39;49m\u001b[39mlight_chain\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    184\u001b[0m         item[\u001b[39m'\u001b[39;49m\u001b[39mantigen_chains\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    185\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    186\u001b[0m     print_log(e, level\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mERROR\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 626\u001b[0m, in \u001b[0;36mAgAbComplex.from_pdb\u001b[0;34m(cls, pdb_path, heavy_chain, light_chain, antigen_chains, numbering, skip_epitope_cal, skip_validity_check)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    624\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pdb\u001b[39m(\u001b[39mcls\u001b[39m, pdb_path: \u001b[39mstr\u001b[39m, heavy_chain: \u001b[39mstr\u001b[39m, light_chain: \u001b[39mstr\u001b[39m, antigen_chains: List[\u001b[39mstr\u001b[39m],\n\u001b[1;32m    625\u001b[0m              numbering: \u001b[39mstr\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mimgt\u001b[39m\u001b[39m'\u001b[39m, skip_epitope_cal\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, skip_validity_check\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 626\u001b[0m     protein \u001b[39m=\u001b[39m Protein\u001b[39m.\u001b[39;49mfrom_pdb(pdb_path)\n\u001b[1;32m    627\u001b[0m     pdb_id \u001b[39m=\u001b[39m protein\u001b[39m.\u001b[39mget_id()\n\u001b[1;32m    628\u001b[0m     \u001b[39m# print('skipping light chain',protein.get_chain(light_chain))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 510\u001b[0m, in \u001b[0;36mProtein.from_pdb\u001b[0;34m(cls, pdb_path)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    508\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pdb\u001b[39m(\u001b[39mcls\u001b[39m, pdb_path):\n\u001b[1;32m    509\u001b[0m     parser \u001b[39m=\u001b[39m PDBParser(QUIET\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 510\u001b[0m     structure \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39;49mget_structure(\u001b[39m'\u001b[39;49m\u001b[39manonym\u001b[39;49m\u001b[39m'\u001b[39;49m, pdb_path)\n\u001b[1;32m    511\u001b[0m     pdb_id \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mheader[\u001b[39m'\u001b[39m\u001b[39midcode\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mupper()\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m    512\u001b[0m     \u001b[39mif\u001b[39;00m pdb_id \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    513\u001b[0m         \u001b[39m# deduce from file name\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dyMEAN/lib/python3.8/site-packages/Bio/PDB/PDBParser.py:100\u001b[0m, in \u001b[0;36mPDBParser.get_structure\u001b[0;34m(self, id, file)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m lines:\n\u001b[1;32m     99\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mEmpty file.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse(lines)\n\u001b[1;32m    102\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstructure_builder\u001b[39m.\u001b[39mset_header(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheader)\n\u001b[1;32m    103\u001b[0m \u001b[39m# Return the Structure instance\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dyMEAN/lib/python3.8/site-packages/Bio/PDB/PDBParser.py:123\u001b[0m, in \u001b[0;36mPDBParser._parse\u001b[0;34m(self, header_coords_trailer)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheader, coords_trailer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_header(header_coords_trailer)\n\u001b[1;32m    122\u001b[0m \u001b[39m# Parse the atomic data; return the PDB file trailer\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrailer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_coordinates(coords_trailer)\n",
      "File \u001b[0;32m~/anaconda3/envs/dyMEAN/lib/python3.8/site-packages/Bio/PDB/PDBParser.py:299\u001b[0m, in \u001b[0;36mPDBParser._parse_coordinates\u001b[0;34m(self, coords_trailer)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_pqr:\n\u001b[1;32m    297\u001b[0m     \u001b[39m# init atom with pdb fields\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 299\u001b[0m         structure_builder\u001b[39m.\u001b[39;49minit_atom(\n\u001b[1;32m    300\u001b[0m             name,\n\u001b[1;32m    301\u001b[0m             coord,\n\u001b[1;32m    302\u001b[0m             bfactor,\n\u001b[1;32m    303\u001b[0m             occupancy,\n\u001b[1;32m    304\u001b[0m             altloc,\n\u001b[1;32m    305\u001b[0m             fullname,\n\u001b[1;32m    306\u001b[0m             serial_number,\n\u001b[1;32m    307\u001b[0m             element,\n\u001b[1;32m    308\u001b[0m         )\n\u001b[1;32m    309\u001b[0m     \u001b[39mexcept\u001b[39;00m PDBConstructionException \u001b[39mas\u001b[39;00m message:\n\u001b[1;32m    310\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_PDB_exception(message, global_line_counter)\n",
      "File \u001b[0;32m~/anaconda3/envs/dyMEAN/lib/python3.8/site-packages/Bio/PDB/StructureBuilder.py:289\u001b[0m, in \u001b[0;36mStructureBuilder.init_atom\u001b[0;34m(self, name, coord, b_factor, occupancy, altloc, fullname, serial_number, element, pqr_charge, radius, is_pqr)\u001b[0m\n\u001b[1;32m    286\u001b[0m         residue\u001b[39m.\u001b[39mflag_disordered()\n\u001b[1;32m    287\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     \u001b[39m# The atom is not disordered\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m     residue\u001b[39m.\u001b[39;49madd(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49matom)\n",
      "File \u001b[0;32m~/anaconda3/envs/dyMEAN/lib/python3.8/site-packages/Bio/PDB/Residue.py:51\u001b[0m, in \u001b[0;36mResidue.add\u001b[0;34m(self, atom)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_id(atom_id):\n\u001b[1;32m     48\u001b[0m     \u001b[39mraise\u001b[39;00m PDBConstructionException(\n\u001b[1;32m     49\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAtom \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m defined twice in residue \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (atom_id, \u001b[39mself\u001b[39m)\n\u001b[1;32m     50\u001b[0m     )\n\u001b[0;32m---> 51\u001b[0m Entity\u001b[39m.\u001b[39;49madd(\u001b[39mself\u001b[39;49m, atom)\n",
      "File \u001b[0;32m~/anaconda3/envs/dyMEAN/lib/python3.8/site-packages/Bio/PDB/Entity.py:218\u001b[0m, in \u001b[0;36mEntity.add\u001b[0;34m(self, entity)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_id(entity_id):\n\u001b[1;32m    217\u001b[0m     \u001b[39mraise\u001b[39;00m PDBConstructionException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mentity_id\u001b[39m}\u001b[39;00m\u001b[39m defined twice\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 218\u001b[0m entity\u001b[39m.\u001b[39;49mset_parent(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    219\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchild_list\u001b[39m.\u001b[39mappend(entity)\n\u001b[1;32m    220\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchild_dict[entity_id] \u001b[39m=\u001b[39m entity\n",
      "File \u001b[0;32m~/anaconda3/envs/dyMEAN/lib/python3.8/site-packages/Bio/PDB/Atom.py:344\u001b[0m, in \u001b[0;36mAtom.set_parent\u001b[0;34m(self, parent)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Set the parent residue.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \n\u001b[1;32m    339\u001b[0m \u001b[39mArguments:\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39m - parent - Residue object\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \n\u001b[1;32m    342\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparent \u001b[39m=\u001b[39m parent\n\u001b[0;32m--> 344\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_full_id()\n",
      "File \u001b[0;32m~/anaconda3/envs/dyMEAN/lib/python3.8/site-packages/Bio/PDB/Atom.py:386\u001b[0m, in \u001b[0;36mAtom.get_full_id\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the full id of the atom.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \n\u001b[1;32m    381\u001b[0m \u001b[39mThe full id of an atom is a tuple used to uniquely identify\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39mthe atom and consists of the following elements:\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39m(structure id, model id, chain id, residue id, atom name, altloc)\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparent\u001b[39m.\u001b[39;49mget_full_id() \u001b[39m+\u001b[39;49m ((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maltloc),)\n\u001b[1;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maltloc)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "########### load your train / valid set ###########\n",
    "if len(config.gpus) >= 1:\n",
    "    print_log(config)\n",
    "    print_log(f'CDR type: {config.cdr}')\n",
    "    print_log(f'Paratope: {config.paratope}')\n",
    "    print_log('structure only' if config.struct_only else 'sequence & structure codesign')\n",
    "\n",
    "print('till here')\n",
    "train_set = E2EDataset(config.train_set, cdr=config.cdr, paratope=config.paratope)\n",
    "valid_set = E2EDataset(config.valid_set, cdr=config.cdr, paratope=config.paratope)\n",
    "\n",
    "########## set your collate_fn ##########\n",
    "collate_fn = train_set.collate_fn\n",
    "\n",
    "########## define your model/trainer/trainconfig #########\n",
    "config = TrainConfig(**vars(config))\n",
    "\n",
    "if config.model_type == 'dyMEAN':\n",
    "\n",
    "\n",
    "    class dyMEANTrainer(Trainer):\n",
    "\n",
    "        ########## Override start ##########\n",
    "\n",
    "        def __init__(self, model, train_loader, valid_loader, config):\n",
    "            self.global_step = 0\n",
    "            self.epoch = 0\n",
    "            self.max_step = config.max_epoch * config.step_per_epoch\n",
    "            self.log_alpha = log(config.final_lr / config.lr) / self.max_step\n",
    "            super().__init__(model, train_loader, valid_loader, config)\n",
    "\n",
    "        def get_optimizer(self):\n",
    "            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.lr)\n",
    "            return optimizer\n",
    "\n",
    "        def get_scheduler(self, optimizer):\n",
    "            log_alpha = self.log_alpha\n",
    "            lr_lambda = lambda step: exp(log_alpha * (step + 1))  # equal to alpha^{step}\n",
    "            scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "            return {\n",
    "                'scheduler': scheduler,\n",
    "                'frequency': 'batch'\n",
    "            }\n",
    "\n",
    "        def train_step(self, batch, batch_idx):\n",
    "            batch['context_ratio'] = self.get_context_ratio()\n",
    "            return self.share_step(batch, batch_idx, val=False)\n",
    "\n",
    "        def valid_step(self, batch, batch_idx):\n",
    "            batch['context_ratio'] = 0\n",
    "            return self.share_step(batch, batch_idx, val=True)\n",
    "\n",
    "        ########## Override end ##########\n",
    "\n",
    "        def get_context_ratio(self):\n",
    "            step = self.global_step\n",
    "            ratio = 0.5 * (cos(step / self.max_step * pi) + 1) * 0.9  # scale to [0, 0.9]\n",
    "            return ratio\n",
    "\n",
    "        def share_step(self, batch, batch_idx, val=False):\n",
    "            loss, seq_detail, structure_detail, dock_detail, pdev_detail = self.model(**batch)\n",
    "            snll, aar = seq_detail\n",
    "            struct_loss, xloss, bond_loss, sc_bond_loss = structure_detail\n",
    "            dock_loss, interface_loss, ed_loss, r_ed_losses = dock_detail\n",
    "            pdev_loss, prmsd_loss = pdev_detail\n",
    "\n",
    "            log_type = 'Validation' if val else 'Train'\n",
    "\n",
    "            self.log(f'Overall/Loss/{log_type}', loss, batch_idx, val)\n",
    "\n",
    "            self.log(f'Seq/SNLL/{log_type}', snll, batch_idx, val)\n",
    "            self.log(f'Seq/AAR/{log_type}', aar, batch_idx, val)\n",
    "\n",
    "            self.log(f'Struct/StructLoss/{log_type}', struct_loss, batch_idx, val)\n",
    "            self.log(f'Struct/XLoss/{log_type}', xloss, batch_idx, val)\n",
    "            self.log(f'Struct/BondLoss/{log_type}', bond_loss, batch_idx, val)\n",
    "            self.log(f'Struct/SidechainBondLoss/{log_type}', sc_bond_loss, batch_idx, val)\n",
    "\n",
    "            self.log(f'Dock/DockLoss/{log_type}', dock_loss, batch_idx, val)\n",
    "            self.log(f'Dock/SPLoss/{log_type}', interface_loss, batch_idx, val)\n",
    "            self.log(f'Dock/EDLoss/{log_type}', ed_loss, batch_idx, val)\n",
    "            for i, l in enumerate(r_ed_losses):\n",
    "                self.log(f'Dock/edloss{i}/{log_type}', l, batch_idx, val)\n",
    "\n",
    "            if pdev_loss is not None:\n",
    "                self.log(f'PDev/PDevLoss/{log_type}', pdev_loss, batch_idx, val)\n",
    "                self.log(f'PDev/PRMSDLoss/{log_type}', prmsd_loss, batch_idx, val)\n",
    "\n",
    "            if not val:\n",
    "                lr = self.config.lr if self.scheduler is None else self.scheduler.get_last_lr()\n",
    "                lr = lr[0]\n",
    "                self.log('lr', lr, batch_idx, val)\n",
    "                self.log('context_ratio', batch['context_ratio'], batch_idx, val)\n",
    "            return loss\n",
    "    class dyMEANModel(nn.Module):\n",
    "        def __init__(self, embed_size, hidden_size, n_channel, num_classes,\n",
    "                    mask_id=VOCAB.get_mask_idx(), k_neighbors=9, bind_dist_cutoff=6,\n",
    "                    n_layers=3, iter_round=3, dropout=0.1, struct_only=False,\n",
    "                    backbone_only=False, fix_channel_weights=False, pred_edge_dist=True,\n",
    "                    keep_memory=True, cdr_type='H3', paratope='H3', relative_position=False) -> None:\n",
    "            super().__init__()\n",
    "            self.mask_id = mask_id\n",
    "            self.num_classes = num_classes\n",
    "            self.bind_dist_cutoff = bind_dist_cutoff\n",
    "            self.k_neighbors = k_neighbors\n",
    "            self.round = iter_round\n",
    "            self.struct_only = struct_only\n",
    "\n",
    "            # options\n",
    "            self.backbone_only = backbone_only\n",
    "            self.fix_channel_weights = fix_channel_weights\n",
    "            self.pred_edge_dist = pred_edge_dist\n",
    "            self.keep_memory = keep_memory\n",
    "            if self.backbone_only:\n",
    "                n_channel = 4\n",
    "            self.cdr_type = cdr_type\n",
    "            self.paratope = paratope\n",
    "\n",
    "            atom_embed_size = embed_size // 4\n",
    "            self.aa_feature = SeparatedAminoAcidFeature(\n",
    "                embed_size, atom_embed_size,\n",
    "                relative_position=relative_position,\n",
    "                edge_constructor=GMEdgeConstructor,\n",
    "                fix_atom_weights=fix_channel_weights,\n",
    "                backbone_only=backbone_only\n",
    "            )\n",
    "            self.protein_feature = ProteinFeature(backbone_only=backbone_only)\n",
    "            if keep_memory:\n",
    "                self.memory_ffn = nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(hidden_size, hidden_size),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(hidden_size, embed_size)\n",
    "                )\n",
    "            if self.pred_edge_dist:  # use predicted dist for KNN-graph at the interface\n",
    "                if self.keep_memory:  # this ffn acts on the memory\n",
    "                    self.edge_H_ffn = nn.Sequential(\n",
    "                        nn.SiLU(),\n",
    "                        nn.Linear(hidden_size, hidden_size),\n",
    "                        nn.SiLU(),\n",
    "                        nn.Linear(hidden_size, hidden_size)\n",
    "                    )\n",
    "                self.edge_dist_ffn = nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(2 * hidden_size, hidden_size),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(hidden_size, 1)\n",
    "                )\n",
    "                # this GNN encodes the initial hidden states for initial edge distance prediction\n",
    "                self.init_gnn = AMEGNN(\n",
    "                    embed_size, hidden_size, hidden_size, n_channel,\n",
    "                    channel_nf=atom_embed_size, radial_nf=hidden_size,\n",
    "                    in_edge_nf=0, n_layers=n_layers, residual=True,\n",
    "                    dropout=dropout, dense=False)\n",
    "            if not struct_only:\n",
    "                self.ffn_residue = nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(hidden_size, hidden_size),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(hidden_size, self.num_classes)\n",
    "                )\n",
    "            else:\n",
    "                self.prmsd_ffn = nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(hidden_size, hidden_size),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(hidden_size, 1)\n",
    "                )\n",
    "            self.gnn = AMEncoder(\n",
    "                embed_size, hidden_size, hidden_size, n_channel,\n",
    "                channel_nf=atom_embed_size, radial_nf=hidden_size,\n",
    "                in_edge_nf=0, n_layers=n_layers, residual=True,\n",
    "                dropout=dropout, dense=False)\n",
    "            \n",
    "            self.normalizer = SeperatedCoordNormalizer()\n",
    "\n",
    "            # training related cache\n",
    "            self.batch_constants = {}\n",
    "\n",
    "        def init_mask(self, X, S, cmask, smask, template):\n",
    "            if not self.struct_only:\n",
    "                S[smask] = self.mask_id\n",
    "            X[cmask] = template\n",
    "            return X, S\n",
    "\n",
    "        def message_passing(self, X, S, residue_pos, interface_X, paratope_mask, batch_id, t, memory_H=None, smooth_prob=None, smooth_mask=None):\n",
    "            # embeddings\n",
    "            H_0, (ctx_edges, inter_edges), (atom_embeddings, atom_weights) = self.aa_feature(X, S, batch_id, self.k_neighbors, residue_pos, smooth_prob=smooth_prob, smooth_mask=smooth_mask)\n",
    "\n",
    "            if not self.keep_memory:\n",
    "                memory_H = None\n",
    "\n",
    "            if memory_H is not None:\n",
    "                H_0 = H_0 + self.memory_ffn(memory_H)\n",
    "\n",
    "            if self.pred_edge_dist:\n",
    "                if memory_H is not None:\n",
    "                    edge_H = self.edge_H_ffn(memory_H)\n",
    "                else:\n",
    "                    # replace the MLP with gnn for initial edge distance prediction\n",
    "                    edge_H, dumb_X = self.init_gnn(H_0, X, ctx_edges,\n",
    "                                        channel_attr=atom_embeddings,\n",
    "                                        channel_weights=atom_weights)\n",
    "                    X = X + dumb_X * 0  # to cheat the autograd check\n",
    "\n",
    "            # update coordination of the global node\n",
    "            X = self.aa_feature.update_globel_coordinates(X, S)\n",
    "\n",
    "            # prepare local complex\n",
    "            local_mask = self.batch_constants['local_mask']\n",
    "            local_is_ab = self.batch_constants['local_is_ab']\n",
    "            local_batch_id = self.batch_constants['local_batch_id']\n",
    "            local_X = X[local_mask].clone()\n",
    "            # prepare local complex edges\n",
    "            local_ctx_edges = self.batch_constants['local_ctx_edges']  # [2, Ec]\n",
    "            local_inter_edges = self.batch_constants['local_inter_edges']  # [2, Ei]\n",
    "            atom_pos = self.aa_feature._construct_atom_pos(S[local_mask])\n",
    "            offsets, max_n, gni2lni = self.batch_constants['local_edge_infos']\n",
    "            # for context edges, use edges in the native paratope\n",
    "            local_ctx_edges = _knn_edges(\n",
    "                local_X, atom_pos, local_ctx_edges.T,\n",
    "                self.aa_feature.atom_pos_pad_idx, self.k_neighbors,\n",
    "                (offsets, local_batch_id, max_n, gni2lni))\n",
    "            # for interative edges, use edges derived from the predicted distance\n",
    "            local_X[local_is_ab] = interface_X\n",
    "            if self.pred_edge_dist:\n",
    "                local_H = edge_H[local_mask]\n",
    "                src_H, dst_H = local_H[local_inter_edges[0]], local_H[local_inter_edges[1]]\n",
    "                p_edge_dist = self.edge_dist_ffn(torch.cat([src_H, dst_H], dim=-1)) +\\\n",
    "                            self.edge_dist_ffn(torch.cat([dst_H, src_H], dim=-1))  # perm-invariant\n",
    "                p_edge_dist = p_edge_dist.squeeze()\n",
    "            else:\n",
    "                p_edge_dist = None\n",
    "            local_inter_edges = _knn_edges(\n",
    "                local_X, atom_pos, local_inter_edges.T,\n",
    "                self.aa_feature.atom_pos_pad_idx, self.k_neighbors,\n",
    "                (offsets, local_batch_id, max_n, gni2lni), given_dist=p_edge_dist)\n",
    "            local_edges = torch.cat([local_ctx_edges, local_inter_edges], dim=1)\n",
    "\n",
    "            # message passing\n",
    "            H, pred_X, pred_local_X = self.gnn(H_0, X, ctx_edges,\n",
    "                                            local_mask, local_X, local_edges,\n",
    "                                            paratope_mask, local_is_ab,\n",
    "                                            channel_attr=atom_embeddings,\n",
    "                                            channel_weights=atom_weights)\n",
    "            interface_X = pred_local_X[local_is_ab]\n",
    "            pred_logits = None if self.struct_only else self.ffn_residue(H)\n",
    "\n",
    "            return pred_logits, pred_X, interface_X, H, p_edge_dist  # [N, num_classes], [N, n_channel, 3], [Ncdr, n_channel, 3], [N, hidden_size]\n",
    "        \n",
    "        @torch.no_grad()\n",
    "        def init_interface(self, X, S, paratope_mask, batch_id, init_noise=None):\n",
    "            ag_centers = X[S == self.aa_feature.boa_idx][:, 0]  # [bs, 3]\n",
    "            init_local_X = torch.zeros_like(X[paratope_mask])\n",
    "            init_local_X = init_local_X + ag_centers[batch_id[paratope_mask]].unsqueeze(1)\n",
    "            noise = torch.randn_like(init_local_X) if init_noise is None else init_noise\n",
    "            ca_noise = noise[:, 1]\n",
    "            noise = noise / 10  + ca_noise.unsqueeze(1) # scale other atoms\n",
    "            noise[:, 1] = ca_noise\n",
    "            init_local_X = init_local_X + noise\n",
    "            return init_local_X\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def _prepare_batch_constants(self, S, paratope_mask, lengths):\n",
    "            # generate batch id\n",
    "            batch_id = torch.zeros_like(S)  # [N]\n",
    "            batch_id[torch.cumsum(lengths, dim=0)[:-1]] = 1\n",
    "            batch_id.cumsum_(dim=0)  # [N], item idx in the batch\n",
    "            self.batch_constants['batch_id'] = batch_id\n",
    "            self.batch_constants['batch_size'] = torch.max(batch_id) + 1\n",
    "\n",
    "            segment_ids = self.aa_feature._construct_segment_ids(S)\n",
    "            self.batch_constants['segment_ids'] = segment_ids\n",
    "\n",
    "            # interface relatd\n",
    "            is_ag = segment_ids == self.aa_feature.ag_seg_id\n",
    "            not_ag_global = S != self.aa_feature.boa_idx\n",
    "            local_mask = torch.logical_or(\n",
    "                paratope_mask, torch.logical_and(is_ag, not_ag_global)\n",
    "            )\n",
    "            local_segment_ids = segment_ids[local_mask]\n",
    "            local_is_ab = local_segment_ids != self.aa_feature.ag_seg_id\n",
    "            local_batch_id = batch_id[local_mask]\n",
    "            self.batch_constants['is_ag'] = is_ag\n",
    "            self.batch_constants['local_mask'] = local_mask\n",
    "            self.batch_constants['local_is_ab'] = local_is_ab\n",
    "            self.batch_constants['local_batch_id'] = local_batch_id\n",
    "            self.batch_constants['local_segment_ids'] = local_segment_ids\n",
    "            # interface local edges\n",
    "            (row, col), (offsets, max_n, gni2lni) = self.aa_feature.edge_constructor.get_batch_edges(local_batch_id)\n",
    "            row_segment_ids, col_segment_ids = local_segment_ids[row], local_segment_ids[col]\n",
    "            is_ctx = row_segment_ids == col_segment_ids\n",
    "            is_inter = torch.logical_not(is_ctx)\n",
    "\n",
    "            self.batch_constants['local_ctx_edges'] = torch.stack([row[is_ctx], col[is_ctx]])  # [2, Ec]\n",
    "            self.batch_constants['local_inter_edges'] = torch.stack([row[is_inter], col[is_inter]])  # [2, Ei]\n",
    "            self.batch_constants['local_edge_infos'] = (offsets, max_n, gni2lni)\n",
    "\n",
    "            interface_batch_id = batch_id[paratope_mask]\n",
    "            self.batch_constants['interface_batch_id'] = interface_batch_id\n",
    "        \n",
    "        def _clean_batch_constants(self):\n",
    "            self.batch_constants = {}\n",
    "\n",
    "        @torch.no_grad()\n",
    "        def _get_inter_edge_dist(self, X, S):\n",
    "            local_mask = self.batch_constants['local_mask']\n",
    "            atom_pos = self.aa_feature._construct_atom_pos(S[local_mask])\n",
    "            src_dst = self.batch_constants['local_inter_edges'].T\n",
    "            dist = X[local_mask][src_dst]  # [Ef, 2, n_channel, 3]\n",
    "            dist = dist[:, 0].unsqueeze(2) - dist[:, 1].unsqueeze(1)  # [Ef, n_channel, n_channel, 3]\n",
    "            dist = torch.norm(dist, dim=-1)  # [Ef, n_channel, n_channel]\n",
    "            pos_pad = atom_pos[src_dst] == self.aa_feature.atom_pos_pad_idx # [Ef, 2, n_channel]\n",
    "            pos_pad = torch.logical_or(pos_pad[:, 0].unsqueeze(2), pos_pad[:, 1].unsqueeze(1))  # [Ef, n_channel, n_channel]\n",
    "            dist = dist + pos_pad * 1e10  # [Ef, n_channel, n_channel]\n",
    "            dist = torch.min(dist.reshape(dist.shape[0], -1), dim=1)[0]  # [Ef]\n",
    "            return dist\n",
    "            is_binding = dist <= self.bind_dist_cutoff\n",
    "            return is_binding\n",
    "\n",
    "        def _forward(self, X, S, cmask, smask, paratope_mask, residue_pos, template, lengths, init_noise=None):\n",
    "            batch_id = self.batch_constants['batch_id']\n",
    "\n",
    "            # mask sequence and initialize coordinates with template\n",
    "            X, S = self.init_mask(X, S, cmask, smask, template)\n",
    "\n",
    "            # normalize\n",
    "            X = self.normalizer.centering(X, S, batch_id, self.aa_feature)\n",
    "            X = self.normalizer.normalize(X)\n",
    "\n",
    "            # update center\n",
    "            X = self.aa_feature.update_globel_coordinates(X, S)\n",
    "\n",
    "            # prepare initial interface\n",
    "            interface_X = self.init_interface(X, S, paratope_mask, batch_id, init_noise)\n",
    "\n",
    "            # sequence and structure loss\n",
    "            r_pred_S_logits, pred_S_dist, = [], None\n",
    "            r_interface_X = [interface_X.clone()]  # init\n",
    "            r_edge_dist = []\n",
    "            memory_H = None\n",
    "            # message passing\n",
    "            for t in range(self.round):\n",
    "                pred_S_logits, pred_X, interface_X, H, edge_dist = self.message_passing(X, S, residue_pos, interface_X, paratope_mask, batch_id, t, memory_H, pred_S_dist, smask)\n",
    "                memory_H = H\n",
    "                r_interface_X.append(interface_X.clone())\n",
    "                r_pred_S_logits.append((pred_S_logits, smask))\n",
    "                r_edge_dist.append(edge_dist)\n",
    "                # 1. update X\n",
    "                X = X.clone()\n",
    "                X[cmask] = pred_X[cmask]\n",
    "                X = self.aa_feature.update_globel_coordinates(X, S)\n",
    "\n",
    "                if not self.struct_only:\n",
    "                    # 2. update S\n",
    "                    S = S.clone()\n",
    "                    if t == self.round - 1:\n",
    "                        S[smask] = torch.argmax(pred_S_logits[smask], dim=-1)\n",
    "                    else:\n",
    "                        pred_S_dist = torch.softmax(pred_S_logits[smask], dim=-1)\n",
    "\n",
    "            interface_batch_id = self.batch_constants['interface_batch_id']\n",
    "\n",
    "            if self.struct_only:\n",
    "                # predicted rmsd\n",
    "                prmsd = self.prmsd_ffn(H[cmask]).squeeze()  # [N_ab]\n",
    "            else:\n",
    "                prmsd = None\n",
    "\n",
    "            # uncentering and unnormalize\n",
    "            pred_X = self.normalizer.unnormalize(pred_X)\n",
    "            pred_X = self.normalizer.uncentering(pred_X, batch_id)\n",
    "            for i, interface_X in enumerate(r_interface_X):\n",
    "                interface_X = self.normalizer.unnormalize(interface_X)\n",
    "                interface_X = self.normalizer.uncentering(interface_X, interface_batch_id, _type=4)\n",
    "                r_interface_X[i] = interface_X\n",
    "            self.normalizer.clear_cache()\n",
    "\n",
    "            return H, S, r_pred_S_logits, pred_X, r_interface_X,  r_edge_dist, prmsd\n",
    "\n",
    "        def forward(self, X, S, cmask, smask, paratope_mask, residue_pos, template, lengths, xloss_mask, context_ratio=0):\n",
    "            '''\n",
    "            :param X: [N, n_channel, 3], Cartesian coordinates\n",
    "            :param context_ratio: float, rate of context provided in masked sequence, should be [0, 1) and anneal to 0 in training\n",
    "            '''\n",
    "            if self.backbone_only:\n",
    "                X, template = X[:, :4], template[:, :4]  # backbone\n",
    "                xloss_mask = xloss_mask[:, :4]\n",
    "            # clone ground truth coordinates, sequence\n",
    "            true_X, true_S = X.clone(), S.clone()\n",
    "\n",
    "            # prepare constants\n",
    "            self._prepare_batch_constants(S, paratope_mask, lengths)\n",
    "            batch_id = self.batch_constants['batch_id']\n",
    "\n",
    "            # provide some ground truth for annealing sequence training\n",
    "            if context_ratio > 0:\n",
    "                not_ctx_mask = torch.rand_like(smask, dtype=torch.float) >= context_ratio\n",
    "                smask = torch.logical_and(smask, not_ctx_mask)\n",
    "\n",
    "            # get results\n",
    "            H, pred_S, r_pred_S_logits, pred_X, r_interface_X, r_edge_dist, prmsd = self._forward(X, S, cmask, smask, paratope_mask, residue_pos, template, lengths)\n",
    "\n",
    "            # sequence negtive log likelihood\n",
    "            snll, total = 0, 0\n",
    "            if not self.struct_only:\n",
    "                for logits, mask in r_pred_S_logits:\n",
    "                    snll = snll + F.cross_entropy(logits[mask], true_S[mask], reduction='sum')\n",
    "                    total = total + mask.sum()\n",
    "                snll = snll / total\n",
    "\n",
    "            # structure loss\n",
    "            struct_loss, struct_loss_details, bb_rmsd, ops = self.protein_feature.structure_loss(pred_X, true_X, true_S, cmask, batch_id, xloss_mask, self.aa_feature)\n",
    "\n",
    "            # docking loss\n",
    "            gt_interface_X = true_X[paratope_mask]\n",
    "            # 1. interface loss (shadow paratope)\n",
    "            interface_atom_pos = self.aa_feature._construct_atom_pos(true_S[paratope_mask])\n",
    "            interface_atom_mask = interface_atom_pos != self.aa_feature.atom_pos_pad_idx\n",
    "            interface_loss = F.smooth_l1_loss(\n",
    "                r_interface_X[-1][interface_atom_mask],\n",
    "                gt_interface_X[interface_atom_mask])\n",
    "            # 2. edge dist loss\n",
    "            if self.pred_edge_dist:\n",
    "                gt_edge_dist = self._get_inter_edge_dist(self.normalizer.normalize(true_X), true_S)\n",
    "                ed_loss, r_ed_losses = 0, []\n",
    "                for edge_dist in r_edge_dist:\n",
    "                    r_ed_loss = F.smooth_l1_loss(edge_dist, gt_edge_dist)\n",
    "                    ed_loss = ed_loss + r_ed_loss\n",
    "                    r_ed_losses.append(r_ed_loss)\n",
    "            else:\n",
    "                r_ed_losses = [0 for _ in range(self.round)]\n",
    "                ed_loss = 0\n",
    "            dock_loss = interface_loss + ed_loss\n",
    "\n",
    "            if self.struct_only:\n",
    "                # predicted rmsd\n",
    "                prmsd_loss = F.smooth_l1_loss(prmsd, bb_rmsd)\n",
    "                pdev_loss = prmsd_loss\n",
    "            else:\n",
    "                pdev_loss, prmsd_loss = None, None\n",
    "\n",
    "            # comprehensive loss\n",
    "            loss = snll + struct_loss + dock_loss + (0 if pdev_loss is None else pdev_loss)\n",
    "\n",
    "            self._clean_batch_constants()\n",
    "\n",
    "            # AAR\n",
    "            with torch.no_grad():\n",
    "                aa_hit = pred_S[smask] == true_S[smask]\n",
    "                aar = aa_hit.long().sum() / aa_hit.shape[0]\n",
    "\n",
    "            return loss, (snll, aar), (struct_loss, *struct_loss_details), (dock_loss, interface_loss, ed_loss, r_ed_losses), (pdev_loss, prmsd_loss)\n",
    "\n",
    "        def sample(self, X, S, cmask, smask, paratope_mask, residue_pos, template, lengths, init_noise=None, return_hidden=False):\n",
    "            if self.backbone_only:\n",
    "                X, template = X[:, :4], template[:, :4]  # backbone\n",
    "            gen_X, gen_S = X.clone(), S.clone()\n",
    "            \n",
    "            # prepare constants\n",
    "            self._prepare_batch_constants(S, paratope_mask, lengths)\n",
    "\n",
    "            batch_id = self.batch_constants['batch_id']\n",
    "            batch_size = self.batch_constants['batch_size']\n",
    "            segment_ids = self.batch_constants['segment_ids']\n",
    "            interface_batch_id = self.batch_constants['interface_batch_id']\n",
    "            is_ab = segment_ids != self.aa_feature.ag_seg_id\n",
    "            s_batch_id = batch_id[smask]\n",
    "\n",
    "            best_metric = torch.ones(batch_size, dtype=torch.float, device=X.device) * 1e10\n",
    "            interface_cmask = paratope_mask[cmask]\n",
    "\n",
    "            n_tries = 10 if self.struct_only else 1\n",
    "            for i in range(n_tries):\n",
    "            \n",
    "                # generate\n",
    "                H, pred_S, r_pred_S_logits, pred_X, r_interface_X, _, prmsd = self._forward(X, S, cmask, smask, paratope_mask, residue_pos, template, lengths, init_noise)\n",
    "\n",
    "                # PPL or PRMSD\n",
    "                if not self.struct_only:\n",
    "                    S_logits = r_pred_S_logits[-1][0][smask]\n",
    "                    S_probs = torch.max(torch.softmax(S_logits, dim=-1), dim=-1)[0]\n",
    "                    nlls = -torch.log(S_probs)\n",
    "                    metric = scatter_mean(nlls, s_batch_id)  # [batch_size]\n",
    "                else:\n",
    "                    metric = scatter_mean(prmsd[interface_cmask], interface_batch_id)  # [batch_size]\n",
    "\n",
    "                update = metric < best_metric\n",
    "                cupdate = cmask & update[batch_id]\n",
    "                supdate = smask & update[batch_id]\n",
    "                # update metric history\n",
    "                best_metric[update] = metric[update]\n",
    "\n",
    "                # 1. set generated part\n",
    "                gen_X[cupdate] = pred_X[cupdate]\n",
    "                if not self.struct_only:\n",
    "                    gen_S[supdate] = pred_S[supdate]\n",
    "            \n",
    "                interface_X = r_interface_X[-1]\n",
    "                # 2. align by cdr\n",
    "                for i in range(batch_size):\n",
    "                    if not update[i]:\n",
    "                        continue\n",
    "                    # 1. align CDRH3\n",
    "                    is_cur_graph = batch_id == i\n",
    "                    cdrh3_cur_graph = torch.logical_and(is_cur_graph, paratope_mask)\n",
    "                    ori_cdr = gen_X[cdrh3_cur_graph][:, :4]  # backbone\n",
    "                    pred_cdr = interface_X[interface_batch_id == i][:, :4]\n",
    "                    _, R, t = kabsch_torch(ori_cdr.reshape(-1, 3), pred_cdr.reshape(-1, 3))\n",
    "\n",
    "                    # 2. tranform antibody\n",
    "                    is_cur_ab = is_cur_graph & is_ab\n",
    "                    ab_X = torch.matmul(gen_X[is_cur_ab], R.T) + t\n",
    "                    gen_X[is_cur_ab] = ab_X\n",
    "\n",
    "            self._clean_batch_constants()\n",
    "\n",
    "            if return_hidden:\n",
    "                return gen_X, gen_S, metric, H\n",
    "            return gen_X, gen_S, metric\n",
    "    model = dyMEANModel(config.embed_dim, config.hidden_size, VOCAB.MAX_ATOM_NUMBER,\n",
    "                VOCAB.get_num_amino_acid_type(), VOCAB.get_mask_idx(),\n",
    "                config.k_neighbors, bind_dist_cutoff=config.bind_dist_cutoff,\n",
    "                n_layers=config.n_layers, struct_only=config.struct_only,\n",
    "                iter_round=config.iter_round,\n",
    "                backbone_only=config.backbone_only,\n",
    "                fix_channel_weights=config.fix_channel_weights,\n",
    "                pred_edge_dist=not config.no_pred_edge_dist,\n",
    "                keep_memory=not config.no_memory,\n",
    "                cdr_type=config.cdr, paratope=config.paratope)\n",
    "    \n",
    "elif config.model_type == 'dyMEANOpt':\n",
    "    # from trainer import dyMEANOptTrainer\n",
    "\n",
    "\n",
    "    class dyMEANOptTrainer(Trainer):\n",
    "\n",
    "        ########## Override start ##########\n",
    "\n",
    "        def __init__(self, model, train_loader, valid_loader, config):\n",
    "            self.global_step = 0\n",
    "            self.epoch = 0\n",
    "            self.max_step = config.max_epoch * config.step_per_epoch\n",
    "            self.log_alpha = log(config.final_lr / config.lr) / self.max_step\n",
    "            self.seq_warmup = config.seq_warmup\n",
    "            super().__init__(model, train_loader, valid_loader, config)\n",
    "\n",
    "        def get_optimizer(self):\n",
    "            optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.lr)\n",
    "            return optimizer\n",
    "\n",
    "        def get_scheduler(self, optimizer):\n",
    "            log_alpha = self.log_alpha\n",
    "            lr_lambda = lambda step: exp(log_alpha * (step + 1))  # equal to alpha^{step}\n",
    "            scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "            return {\n",
    "                'scheduler': scheduler,\n",
    "                'frequency': 'batch'\n",
    "            }\n",
    "\n",
    "        def train_step(self, batch, batch_idx):\n",
    "            # batch['seq_alpha'] = min((self.epoch + 1) / (self.seq_warmup + 1), 1) # linear\n",
    "            batch['seq_alpha'] = 1.0 - 1.0 * self.epoch / self.config.max_epoch\n",
    "            return self.share_step(batch, batch_idx, val=False)\n",
    "\n",
    "        def valid_step(self, batch, batch_idx):\n",
    "            batch['seq_alpha'] = 1\n",
    "            return self.share_step(batch, batch_idx, val=True)\n",
    "\n",
    "        ########## Override end ##########\n",
    "\n",
    "        def get_context_ratio(self):\n",
    "            ratio = random() * 0.9\n",
    "            return ratio\n",
    "\n",
    "        def share_step(self, batch, batch_idx, val=False):\n",
    "            del batch['paratope_mask']\n",
    "            del batch['template']\n",
    "            batch['context_ratio'] = self.get_context_ratio()\n",
    "            loss, seq_detail, structure_detail, pdev_detail = self.model(**batch)\n",
    "            snll, aar = seq_detail\n",
    "            struct_loss, xloss, bond_loss, sc_bond_loss = structure_detail\n",
    "            pdev_loss, prmsd_loss = pdev_detail\n",
    "\n",
    "            log_type = 'Validation' if val else 'Train'\n",
    "\n",
    "            self.log(f'Overall/Loss/{log_type}', loss, batch_idx, val)\n",
    "\n",
    "            self.log(f'Seq/SNLL/{log_type}', snll, batch_idx, val)\n",
    "            self.log(f'Seq/AAR/{log_type}', aar, batch_idx, val)\n",
    "\n",
    "            self.log(f'Struct/StructLoss/{log_type}', struct_loss, batch_idx, val)\n",
    "            self.log(f'Struct/XLoss/{log_type}', xloss, batch_idx, val)\n",
    "            self.log(f'Struct/BondLoss/{log_type}', bond_loss, batch_idx, val)\n",
    "            self.log(f'Struct/SidechainBondLoss/{log_type}', sc_bond_loss, batch_idx, val)\n",
    "\n",
    "            if pdev_loss is not None:\n",
    "                self.log(f'PDev/PDevLoss/{log_type}', pdev_loss, batch_idx, val)\n",
    "                self.log(f'PDev/PRMSDLoss/{log_type}', prmsd_loss, batch_idx, val)\n",
    "\n",
    "            if not val:\n",
    "                lr = self.config.lr if self.scheduler is None else self.scheduler.get_last_lr()\n",
    "                lr = lr[0]\n",
    "                self.log('lr', lr, batch_idx, val)\n",
    "                self.log('context_ratio', batch['context_ratio'], batch_idx, val)\n",
    "                self.log('seq_alpha', batch['seq_alpha'], batch_idx, val)\n",
    "            return loss\n",
    "    \n",
    "    '''\n",
    "    Masked 1D & 3D language model\n",
    "    Add noise to ground truth 3D coordination\n",
    "    Add mask to 1D sequence\n",
    "    '''\n",
    "    class dyMEANOptModel(nn.Module):\n",
    "        def __init__(self, embed_size, hidden_size, n_channel, num_classes,\n",
    "                    mask_id=VOCAB.get_mask_idx(), k_neighbors=9, bind_dist_cutoff=6,\n",
    "                    n_layers=3, iter_round=3, dropout=0.1, struct_only=False,\n",
    "                    fix_atom_weights=False, cdr_type='H3', relative_position=False) -> None:\n",
    "            super().__init__()\n",
    "            self.mask_id = mask_id\n",
    "            self.num_classes = num_classes\n",
    "            self.bind_dist_cutoff = bind_dist_cutoff\n",
    "            self.k_neighbors = k_neighbors\n",
    "            self.round = iter_round\n",
    "            self.cdr_type = cdr_type  # only to indicate the usage of the model\n",
    "\n",
    "            atom_embed_size = embed_size // 4\n",
    "            self.aa_feature = SeparatedAminoAcidFeature(\n",
    "                embed_size, atom_embed_size,\n",
    "                relative_position=relative_position,\n",
    "                edge_constructor=EdgeConstructor,\n",
    "                fix_atom_weights=fix_atom_weights)\n",
    "            self.protein_feature = ProteinFeature()\n",
    "            \n",
    "            self.memory_ffn = nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(hidden_size, hidden_size),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(hidden_size, embed_size)\n",
    "            )\n",
    "            self.struct_only = struct_only\n",
    "            if not struct_only:\n",
    "                self.ffn_residue = nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(hidden_size, hidden_size),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(hidden_size, self.num_classes)\n",
    "                )\n",
    "            else:\n",
    "                self.prmsd_ffn = nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(hidden_size, hidden_size),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(hidden_size, 1)\n",
    "                )\n",
    "            self.gnn = AMEGNN(\n",
    "                embed_size, hidden_size, hidden_size, n_channel,\n",
    "                channel_nf=atom_embed_size, radial_nf=hidden_size,\n",
    "                in_edge_nf=0, n_layers=n_layers, residual=True,\n",
    "                dropout=dropout, dense=False)\n",
    "            \n",
    "            # training related cache\n",
    "            self.start_seq_training = False\n",
    "            self.batch_constants = {}\n",
    "\n",
    "        def init_mask(self, X, S, cmask, smask, init_noise):\n",
    "            if not self.struct_only:\n",
    "                S[smask] = self.mask_id\n",
    "            coords = X[cmask]\n",
    "            noise = torch.randn_like(coords) if init_noise is None else init_noise\n",
    "            X = X.clone()\n",
    "            X[cmask] = coords + noise\n",
    "            return X, S\n",
    "\n",
    "        def message_passing(self, X, S, residue_pos, batch_id, t, memory_H=None, smooth_prob=None, smooth_mask=None):\n",
    "            # embeddings\n",
    "            H_0, (ctx_edges, inter_edges), (atom_embeddings, atom_weights) = self.aa_feature(X, S, batch_id, self.k_neighbors, residue_pos, smooth_prob=smooth_prob, smooth_mask=smooth_mask)\n",
    "            inter_edges = self._get_binding_edges(X, S, inter_edges)\n",
    "            edges = torch.cat([ctx_edges, inter_edges], dim=1)\n",
    "\n",
    "            if memory_H is not None:\n",
    "                H_0 = H_0 + self.memory_ffn(memory_H)\n",
    "\n",
    "            # update coordination of the global node\n",
    "            X = self.aa_feature.update_globel_coordinates(X, S)\n",
    "\n",
    "            H, pred_X = self.gnn(H_0, X, edges,\n",
    "                                channel_attr=atom_embeddings,\n",
    "                                channel_weights=atom_weights)\n",
    "\n",
    "\n",
    "            pred_logits = None if self.struct_only else self.ffn_residue(H)\n",
    "\n",
    "            return pred_logits, pred_X, H # [N, num_classes], [N, n_channel, 3], [N, hidden_size]\n",
    "        \n",
    "        @torch.no_grad()\n",
    "        def _prepare_batch_constants(self, S, lengths):\n",
    "            # generate batch id\n",
    "            batch_id = torch.zeros_like(S)  # [N]\n",
    "            batch_id[torch.cumsum(lengths, dim=0)[:-1]] = 1\n",
    "            batch_id.cumsum_(dim=0)  # [N], item idx in the batch\n",
    "            self.batch_constants['batch_id'] = batch_id\n",
    "            self.batch_constants['batch_size'] = torch.max(batch_id) + 1\n",
    "\n",
    "            segment_ids = self.aa_feature._construct_segment_ids(S)\n",
    "            self.batch_constants['segment_ids'] = segment_ids\n",
    "\n",
    "            # interface relatd\n",
    "            is_ag = segment_ids == self.aa_feature.ag_seg_id\n",
    "            self.batch_constants['is_ag'] = is_ag\n",
    "        \n",
    "        @torch.no_grad()\n",
    "        def _get_binding_edges(self, X, S, inter_edges):\n",
    "            atom_pos = self.aa_feature._construct_atom_pos(S)\n",
    "            src_dst = inter_edges.T\n",
    "            dist = X[src_dst]  # [Ef, 2, n_channel, 3]\n",
    "            dist = dist[:, 0].unsqueeze(2) - dist[:, 1].unsqueeze(1)  # [Ef, n_channel, n_channel, 3]\n",
    "            dist = torch.norm(dist, dim=-1)  # [Ef, n_channel, n_channel]\n",
    "            pos_pad = atom_pos[src_dst] == self.aa_feature.atom_pos_pad_idx # [Ef, 2, n_channel]\n",
    "            pos_pad = torch.logical_or(pos_pad[:, 0].unsqueeze(2), pos_pad[:, 1].unsqueeze(1))  # [Ef, n_channel, n_channel]\n",
    "            dist = dist + pos_pad * 1e10  # [Ef, n_channel, n_channel]\n",
    "            dist = torch.min(dist.reshape(dist.shape[0], -1), dim=1)[0]  # [Ef]\n",
    "            is_binding = dist <= self.bind_dist_cutoff\n",
    "            return src_dst[is_binding].T\n",
    "\n",
    "        def _clean_batch_constants(self):\n",
    "            self.batch_constants = {}\n",
    "\n",
    "        def _forward(self, X, S, cmask, smask, residue_pos, init_noise=None):\n",
    "            batch_id = self.batch_constants['batch_id']\n",
    "\n",
    "            # mask sequence and add noise to ground truth coordinates\n",
    "            X, S = self.init_mask(X, S, cmask, smask, init_noise)\n",
    "\n",
    "            # update center\n",
    "            X = self.aa_feature.update_globel_coordinates(X, S)\n",
    "\n",
    "            # sequence and structure loss\n",
    "            r_pred_S_logits, pred_S_dist = [], None\n",
    "            memory_H = None\n",
    "            # message passing\n",
    "            for t in range(self.round):\n",
    "                pred_S_logits, pred_X, H = self.message_passing(X, S, residue_pos, batch_id, t, memory_H, pred_S_dist, smask)\n",
    "                r_pred_S_logits.append((pred_S_logits, smask))\n",
    "                memory_H = H\n",
    "                # 1. update X\n",
    "                X = X.clone()\n",
    "                X[cmask] = pred_X[cmask]\n",
    "                X = self.aa_feature.update_globel_coordinates(X, S)\n",
    "\n",
    "                if not self.struct_only:\n",
    "                    # 2. update S\n",
    "                    S = S.clone()\n",
    "                    if t == self.round - 1:\n",
    "                        S[smask] = torch.argmax(pred_S_logits[smask], dim=-1)\n",
    "                    else:\n",
    "                        pred_S_dist = torch.softmax(pred_S_logits[smask], dim=-1)\n",
    "\n",
    "            if self.struct_only:\n",
    "                # predicted rmsd\n",
    "                prmsd = self.prmsd_ffn(H[cmask]).squeeze()  # [N_ab]\n",
    "            else:\n",
    "                prmsd = None\n",
    "\n",
    "            return H, S, r_pred_S_logits, pred_X, prmsd\n",
    "\n",
    "        def forward(self, X, S, cmask, smask, residue_pos, lengths, xloss_mask, context_ratio=0, seq_alpha=1):\n",
    "            '''\n",
    "            :param bind_ag: [N_bind], node idx of binding residues in antigen\n",
    "            :param bind_ab: [N_bind], node idx of binding residues in antibody\n",
    "            :param bind_ag_X: [N_bind, 3], coordinations of the midpoint of binding pairs relative to ag\n",
    "            :param bind_ab_X: [N_bind, 3], coordinations of the midpoint of binding pairs relative to ab\n",
    "            :param context_ratio: float, rate of context provided in masked sequence, should be [0, 1) and anneal to 0 in training\n",
    "            :param seq_alpha: float, weight of SNLL, linearly increase from 0 to 1 at warmup phase\n",
    "            '''\n",
    "            # clone ground truth coordinates, sequence\n",
    "            true_X, true_S = X.clone(), S.clone()\n",
    "\n",
    "            # prepare constants\n",
    "            self._prepare_batch_constants(S, lengths)\n",
    "            batch_id = self.batch_constants['batch_id']\n",
    "\n",
    "            # provide some ground truth for annealing sequence training\n",
    "            if context_ratio > 0:\n",
    "                not_ctx_mask = torch.rand_like(smask, dtype=torch.float) >= context_ratio\n",
    "                smask = torch.logical_and(smask, not_ctx_mask)\n",
    "\n",
    "            # get results\n",
    "            H, pred_S, r_pred_S_logits, pred_X, prmsd = self._forward(X, S, cmask, smask, residue_pos)\n",
    "\n",
    "            # sequence negtive log likelihood\n",
    "            snll, total = 0, 0\n",
    "            if not self.struct_only:\n",
    "                for logits, mask in r_pred_S_logits:\n",
    "                    snll = snll + F.cross_entropy(logits[mask], true_S[mask], reduction='sum')\n",
    "                    total = total + mask.sum()\n",
    "                snll = snll / total\n",
    "\n",
    "            # coordination loss\n",
    "            struct_loss, struct_loss_details, bb_rmsd, _ = self.protein_feature.structure_loss(pred_X, true_X, true_S, cmask, batch_id, xloss_mask, self.aa_feature)\n",
    "\n",
    "            if self.struct_only:\n",
    "                # predicted rmsd\n",
    "                prmsd_loss = F.smooth_l1_loss(prmsd, bb_rmsd)\n",
    "                pdev_loss = prmsd_loss# + prmsd_i_loss\n",
    "            else:\n",
    "                pdev_loss, prmsd_loss = None, None\n",
    "\n",
    "            # comprehensive loss\n",
    "            loss = seq_alpha * snll + struct_loss + (0 if pdev_loss is None else pdev_loss)\n",
    "\n",
    "            self._clean_batch_constants()\n",
    "\n",
    "            # AAR\n",
    "            with torch.no_grad():\n",
    "                aa_hit = pred_S[smask] == true_S[smask]\n",
    "                aar = aa_hit.long().sum() / aa_hit.shape[0]\n",
    "\n",
    "            return loss, (snll, aar), (struct_loss, *struct_loss_details), (pdev_loss, prmsd_loss)\n",
    "\n",
    "        def sample(self, X, S, cmask, smask, residue_pos, lengths, return_hidden=False, init_noise=None):\n",
    "            gen_X, gen_S = X.clone(), S.clone()\n",
    "            \n",
    "            # prepare constants\n",
    "            self._prepare_batch_constants(S, lengths)\n",
    "\n",
    "            batch_id = self.batch_constants['batch_id']\n",
    "            batch_size = self.batch_constants['batch_size']\n",
    "            s_batch_id = batch_id[smask]\n",
    "\n",
    "            # generate\n",
    "            H, pred_S, r_pred_S_logits, pred_X, _ = self._forward(X, S, cmask, smask, residue_pos, init_noise)\n",
    "\n",
    "            # PPL\n",
    "            if not self.struct_only:\n",
    "                S_logits = r_pred_S_logits[-1][0][smask]\n",
    "                S_dists = torch.softmax(S_logits, dim=-1)\n",
    "                pred_S[smask] = torch.multinomial(S_dists, num_samples=1).squeeze()\n",
    "                S_probs = S_dists[torch.arange(s_batch_id.shape[0], device=S_dists.device), pred_S[smask]]\n",
    "                nlls = -torch.log(S_probs)\n",
    "                ppl = scatter_mean(nlls, s_batch_id)  # [batch_size]\n",
    "            else:\n",
    "                ppl = torch.zeros(batch_size, device=pred_S.device)\n",
    "\n",
    "            # 1. set generated part\n",
    "            gen_X[cmask] = pred_X[cmask]\n",
    "            if not self.struct_only:\n",
    "                gen_S[smask] = pred_S[smask]\n",
    "            \n",
    "            self._clean_batch_constants()\n",
    "\n",
    "            if return_hidden:\n",
    "                return gen_X, gen_S, ppl, H\n",
    "            return gen_X, gen_S, ppl\n",
    "\n",
    "        def optimize_sample(self, X, S, cmask, smask, residue_pos, lengths, predictor, opt_steps=10, init_noise=None):\n",
    "            self._prepare_batch_constants(S, lengths)\n",
    "            batch_id = self.batch_constants['batch_id']\n",
    "            batch_size = self.batch_constants['batch_size']\n",
    "            # noise_batch_id = batch_id[smask].unsqueeze(1).repeat(1, X.shape[1] * X.shape[2]).flatten()\n",
    "            noise_batch_id = batch_id[cmask].unsqueeze(1).repeat(1, X.shape[1] * X.shape[2]).flatten()\n",
    "\n",
    "            final_X, final_S = X.clone(), S.clone()\n",
    "            best_metric = torch.ones(batch_size, dtype=torch.float, device=X.device) * 1e10\n",
    "\n",
    "            all_noise = torch.randn_like(X, requires_grad=False)\n",
    "            # init_noise = torch.randn_like(X[smask], requires_grad=True)\n",
    "            init_noise = torch.randn_like(X[cmask], requires_grad=True)\n",
    "            optimizer = torch.optim.Adam([init_noise], lr=1.0)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            for i in range(opt_steps):\n",
    "                all_noise = all_noise.detach()\n",
    "                X, S, cmask, smask, residue_pos, lengths = X.clone(), S.clone(), cmask.clone(), smask.clone(), residue_pos.clone(), lengths.clone()\n",
    "                # all_noise[smask] = init_noise\n",
    "                all_noise[cmask] = init_noise\n",
    "                gen_X, gen_S, _, H = self.sample(X, S, cmask, smask, residue_pos, lengths, return_hidden=True, init_noise=all_noise[cmask])\n",
    "                h = scatter_mean(H, batch_id, dim=0)\n",
    "                pmetric = predictor.inference(h)\n",
    "\n",
    "                # use KL to regularize noise\n",
    "                mean = scatter_mean(init_noise.flatten(), noise_batch_id)  # [bs]\n",
    "                std = scatter_std(init_noise.flatten(), noise_batch_id)\n",
    "                # std, mean = torch.std_mean(init_noise.flatten())\n",
    "                kl = -0.5 * (1 + 2 * torch.log(std) - std ** 2 - mean ** 2)\n",
    "\n",
    "                (pmetric + kl).sum().backward()\n",
    "                pmetric = pmetric.detach()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    update = pmetric < best_metric\n",
    "                    cupdate = cmask & update[batch_id]\n",
    "                    supdate = smask & update[batch_id]\n",
    "                    # update pmetric best history\n",
    "                    best_metric[update] = pmetric[update]\n",
    "\n",
    "                    final_X[cupdate] = gen_X[cupdate].detach()\n",
    "                    if not self.struct_only:\n",
    "                        final_S[supdate] = gen_S[supdate].detach()\n",
    "                \n",
    "            return final_X, final_S, best_metric\n",
    "    model = dyMEANOptModel(config.embed_dim, config.hidden_size, VOCAB.MAX_ATOM_NUMBER,\n",
    "                VOCAB.get_num_amino_acid_type(), VOCAB.get_mask_idx(),\n",
    "                config.k_neighbors, bind_dist_cutoff=config.bind_dist_cutoff,\n",
    "                n_layers=config.n_layers, struct_only=config.struct_only,\n",
    "                fix_atom_weights=config.fix_channel_weights, cdr_type=config.cdr)\n",
    "\n",
    "else:\n",
    "    raise NotImplemented(f'model {config.model_type} not implemented')\n",
    "\n",
    "step_per_epoch = (len(train_set) + config.batch_size - 1) // config.batch_size\n",
    "config.add_parameter(step_per_epoch=step_per_epoch)\n",
    "\n",
    "if len(config.gpus) > 1:\n",
    "    config.local_rank = int(-1)\n",
    "    torch.cuda.set_device(config.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl', world_size=len(config.gpus))\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(train_set, shuffle=config.shuffle)\n",
    "    config.batch_size = int(config.batch_size / len(config.gpus))\n",
    "    if config.local_rank == 0:\n",
    "        print_log(f'Batch size on a single GPU: {config.batch_size}')\n",
    "else:\n",
    "    config.local_rank = -1\n",
    "    train_sampler = None\n",
    "config.local_rank = config.local_rank\n",
    "\n",
    "if config.local_rank == 0 or config.local_rank == -1:\n",
    "    print_log(f'step per epoch: {step_per_epoch}')\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=config.batch_size,\n",
    "                            num_workers=config.num_workers,\n",
    "                            shuffle=(config.shuffle and train_sampler is None),\n",
    "                            sampler=train_sampler,\n",
    "                            collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_set, batch_size=config.batch_size,\n",
    "                            num_workers=config.num_workers,\n",
    "                            collate_fn=collate_fn)\n",
    "\n",
    "if config.model_type == 'dyMEAN':\n",
    "    trainer = dyMEANTrainer(model, train_loader, valid_loader, config)\n",
    "elif config.model_type == 'dyMEANOpt':\n",
    "    from trainer import dyMEANOptTrainer\n",
    "    trainer = dyMEANOptTrainer(model, train_loader, valid_loader, config)\n",
    "else:\n",
    "    raise NotImplemented(f'model {config.model_type} not implemented')\n",
    "\n",
    "trainer.train(config.gpus, config.local_rank)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !GPU=0,1 bash scripts/train/train.sh scripts/train/configs/single_cdr_design.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyMEAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
